{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/matery/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/matery.css","path":"css/matery.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/my.css","path":"css/my.css","modified":0,"renderable":1},{"_id":"themes/matery/source/js/matery.js","path":"js/matery.js","modified":0,"renderable":1},{"_id":"themes/matery/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeBlockFuction.js","path":"libs/codeBlock/codeBlockFuction.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeCopy.js","path":"libs/codeBlock/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeLang.js","path":"libs/codeBlock/codeLang.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeShrink.js","path":"libs/codeBlock/codeShrink.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/instantpage/instantpage.js","path":"libs/instantpage/instantpage.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/reward/alipay.png","path":"medias/reward/alipay.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jquery/jquery.min.js","path":"libs/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.eot","path":"libs/awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.ttf","path":"libs/awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff","path":"libs/awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff2","path":"libs/awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/css/all.css","path":"libs/awesome/css/all.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff","path":"libs/awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff2","path":"libs/awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff","path":"libs/awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff2","path":"libs/awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.ttf","path":"libs/awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.svg","path":"libs/awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.eot","path":"libs/awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.eot","path":"libs/awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.ttf","path":"libs/awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.svg","path":"libs/awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.svg","path":"libs/awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"6f821e9ab75e29fdffecc2457e108d79834be3ab","modified":1585383098169},{"_id":"themes/matery/.gitignore","hash":"727607929a51db7ea10968f547c26041eee9cfff","modified":1584836812000},{"_id":"themes/matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1584836812000},{"_id":"themes/matery/README.md","hash":"14e087a7d13de010f4912fcdc3d36bc0091b70c1","modified":1584836812000},{"_id":"themes/matery/README_CN.md","hash":"90547af29a466cda933e77b81a9b9cbd842df0b9","modified":1584836812000},{"_id":"themes/matery/_config.yml","hash":"db744f5ff85416c5076ea39eb65b063b65f1b0e8","modified":1597912904658},{"_id":"source/_data/friends.json","hash":"4d9cb56daa9e398a706c7ccd8da9e57adfa12079","modified":1585392698040},{"_id":"source/_data/musics.json","hash":"32bc061f34721b4ff55f880de1d0ec5787acd2f9","modified":1585383098174},{"_id":"source/_posts/HBase-shell-基本操作.md","hash":"a6282ccb501fc22c736bfa3bffe7d83cecc54af7","modified":1597912904640},{"_id":"source/_posts/Hadoop-HA-搭建.md","hash":"4ca784aa6479a2ca5329e793df9cd9950df1a9f8","modified":1597912904641},{"_id":"source/_posts/RNN-架构梳理.md","hash":"86e58da6b81f4b0b3d25a2b09736c1b26f90f39c","modified":1597912904641},{"_id":"source/_posts/Todo-List.md","hash":"52279b2f5fc7b76f2793dc1b28c2992ab49a7b98","modified":1597912904643},{"_id":"source/_posts/Zookeeper-Hadoop-HBase搭建.md","hash":"b0c70735c3fb387fe03505ba167b538f25b3cff2","modified":1597912904645},{"_id":"source/_posts/mathjax-公式渲染修复.md","hash":"5f697c28d4e59fb38a10a356a47f1b7b2ba584fa","modified":1597912904645},{"_id":"source/_posts/ubuntu1604-搭建Hadoop集群.md","hash":"9be9142fc982b39c4bad724ea55a9fae0a5b2488","modified":1597912904645},{"_id":"source/_posts/ubuntu1604-设置静态IP.md","hash":"a2255f22f205e44eb93f0a763afcd40043c921e5","modified":1597912904646},{"_id":"source/_posts/【求甚解】怎样评价一个模型的泛化能力.md","hash":"3d4c5a73d4081e7f7e6ddef86250953c303c37bd","modified":1597912904648},{"_id":"source/_posts/【文献翻译】基于深度学习的文本分类：全面回顾.md","hash":"87ac46cc6d3d00f03f7d268a8a27a23e3964a372","modified":1597912904647},{"_id":"source/_posts/个人可持续性发展.md","hash":"aa14f78211c9a96861c22eaa44ee4eec81a18cf9","modified":1597912904649},{"_id":"source/_posts/了不起的盖茨比.md","hash":"5df4a6150f2fa4b09b0b17f5ed47fdd3fb9fe2f5","modified":1597912904650},{"_id":"source/_posts/价值网与云服务平台技术.md","hash":"93a67f4f5072f3f35b2b37bf32fb253457b40e06","modified":1597912904651},{"_id":"source/_posts/异构图神经网络.md","hash":"cbbcdf4519395b75956d80d30261a28f13ae8b81","modified":1597912904651},{"_id":"source/_posts/我与小可爱.md","hash":"68da4f6d1c32e3f69800d9d701f98d3f15ddb262","modified":1597912904652},{"_id":"source/_posts/潜在语义分析.md","hash":"1318d8f93a7d81ccf48139789b11cfc80c919463","modified":1597912904653},{"_id":"source/_posts/短文本分类.md","hash":"9c4285e6ac520c4d9b616682a05c2fd0c48e497c","modified":1597912904654},{"_id":"source/_posts/美颜技术的普及对社会审美是好事.md","hash":"fabf48dc5086a211693ea17f2fcf55bd5f0d2720","modified":1597912904656},{"_id":"source/_posts/短文本分类架构梳理.md","hash":"b072d41d1c0e737de2f9b9a82cd625370fdba291","modified":1597912904655},{"_id":"source/_posts/胶囊网络.md","hash":"40c39b5b2ea074762d37e45320357ce37c40621b","modified":1597912904656},{"_id":"source/about/index.md","hash":"352877517b4653ded0a096d98a5b253dcfbe9d86","modified":1585386579651},{"_id":"source/archives/index.md","hash":"a62b7d9b8a8bdf966ec5c823e71581d2b185156e","modified":1585383098201},{"_id":"source/categories/index.md","hash":"f0c10666a2c373409bac5ce68ba343b63c7281c2","modified":1585399373564},{"_id":"source/friends/index.md","hash":"d338f65677213fb4f0726b53582f3cb7fe313631","modified":1585399486241},{"_id":"source/contact/index.md","hash":"933b044611a7554a79da2d5cf25a3c356d51137f","modified":1585383098208},{"_id":"source/tags/index.md","hash":"98697833897097a5c65521ea02baa1c6f85948d8","modified":1585383098214},{"_id":"themes/matery/languages/default.yml","hash":"17aca10aa3a627e84071f4c411f2b76c3161d076","modified":1588078991665},{"_id":"themes/matery/languages/zh-CN.yml","hash":"315de4f99b95ec08ae79394c92022abc634c47ec","modified":1588079317984},{"_id":"themes/matery/layout/about.ejs","hash":"ee639d0310867976b3e5fb9f92c215a17a433703","modified":1585480753036},{"_id":"themes/matery/layout/archive.ejs","hash":"7fe7b9028b0da9c84715c3583b6b4172c2342ac8","modified":1585485749841},{"_id":"themes/matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1584836812000},{"_id":"themes/matery/layout/category.ejs","hash":"720d02e5fc37d154b60590bb7f64a2a4651c02db","modified":1585485809559},{"_id":"themes/matery/layout/contact.ejs","hash":"c3396cc5b1cbb102f500554f76946f5b45ee6d54","modified":1584836812000},{"_id":"themes/matery/layout/friends.ejs","hash":"4cb216b2a650ad5d2942047a65d0883a188c2abb","modified":1584836812000},{"_id":"themes/matery/layout/index.ejs","hash":"5a15bc8312e3ba265692a604e7ec5db0a9e3b779","modified":1585485842529},{"_id":"themes/matery/layout/layout.ejs","hash":"cda1921ec208dfe4f3445e93ab11421657b3d564","modified":1585489264629},{"_id":"themes/matery/layout/post.ejs","hash":"14695375ba83ef0d8a8940891243a32906a20800","modified":1584836812000},{"_id":"themes/matery/layout/tag.ejs","hash":"0c0194cf006fab2dccf4f788075e51cd06637df4","modified":1585485874107},{"_id":"themes/matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1584836812000},{"_id":"themes/matery/layout/_partial/back-top.ejs","hash":"47ee36a042bb6d52bbe1d0f329637e8ffcf1d0aa","modified":1584836812000},{"_id":"themes/matery/layout/_partial/baidu-analytics.ejs","hash":"3bbcdb474ca1dcad514bdc4b7763e17c55df04fd","modified":1584836812000},{"_id":"themes/matery/layout/_partial/baidu-push.ejs","hash":"2cebcc5ea3614d7f76ec36670e68050cbe611202","modified":1584836812000},{"_id":"themes/matery/layout/_partial/bg-cover-content.ejs","hash":"7fbddbf26f64510b290b052a70deb3cfee57cac0","modified":1585479193876},{"_id":"themes/matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1584836812000},{"_id":"themes/matery/layout/_partial/disqus.ejs","hash":"a0f53d1a9b579d52e52ccad8c6e330bf3b89547e","modified":1584836812000},{"_id":"themes/matery/layout/_partial/footer.ejs","hash":"76226ad09fa9cb5a0cd09b8549322935d50d97cd","modified":1584836812000},{"_id":"themes/matery/layout/_partial/gitalk.ejs","hash":"ce7e330165e0db604e0a1845bc460ef35772c656","modified":1584836812000},{"_id":"themes/matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1584836812000},{"_id":"themes/matery/layout/_partial/gitment.ejs","hash":"2628a7a0238f4b35a26cdea36b0e0bf05431ea80","modified":1584836812000},{"_id":"themes/matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1584836812000},{"_id":"themes/matery/layout/_partial/head.ejs","hash":"bfef8b961956e6956677b7b14e4c93adb6642613","modified":1585480551121},{"_id":"themes/matery/layout/_partial/header.ejs","hash":"e253c813b3ee5ed924700a95133741802e58adc5","modified":1585480623555},{"_id":"themes/matery/layout/_partial/index-cover.ejs","hash":"ed47a9800a5ac0b3b703630373175f89ae8435fe","modified":1585485961206},{"_id":"themes/matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1584836812000},{"_id":"themes/matery/layout/_partial/mobile-nav.ejs","hash":"dec2fed861d429063662c2bc90f2708e58fbbad9","modified":1585480658109},{"_id":"themes/matery/layout/_partial/navigation.ejs","hash":"5ff6fdfe973619120a9eda4505bbff4545e39ff0","modified":1584836812000},{"_id":"themes/matery/layout/_partial/paging.ejs","hash":"e2df12cf92a82b1a7a7add2eac1db1d954bc5511","modified":1584836812000},{"_id":"themes/matery/layout/_partial/post-cover.ejs","hash":"68738493f40e22ff82891e3aecaa2746c8470cd0","modified":1585479872766},{"_id":"themes/matery/layout/_partial/post-detail-toc.ejs","hash":"e64819596a61293f9880ee16feaa3c1677d228b8","modified":1584836812000},{"_id":"themes/matery/layout/_partial/post-detail.ejs","hash":"2d20e5dcf172c1e6b5853d8c24b0670fafe48dbf","modified":1584836812000},{"_id":"themes/matery/layout/_partial/post-statis.ejs","hash":"04889f9031743c6b081d02fa4027b0dbfcc45ecf","modified":1584836812000},{"_id":"themes/matery/layout/_partial/prev-next.ejs","hash":"35b6b4a0200c10be6ae9d9558367718290476f84","modified":1585487076540},{"_id":"themes/matery/layout/_partial/reprint-statement.ejs","hash":"01f5eef82bbcb9d432631dbb78dd51d4d4b3b8b5","modified":1584836812000},{"_id":"themes/matery/layout/_partial/reward.ejs","hash":"39b570b9446a7897063fdd6d538ad476fd84f17f","modified":1584836812000},{"_id":"themes/matery/layout/_partial/search.ejs","hash":"b72ae1de86cc6828b7ff1570d02b784167ef0fff","modified":1584836812000},{"_id":"themes/matery/layout/_partial/share.ejs","hash":"36fb0d22d50a9d348fc72ea0fb6c071f2c25b95b","modified":1584836812000},{"_id":"themes/matery/layout/_partial/social-link.ejs","hash":"6f871bd3a70f720e4e451f1f4f625cbc6d8994a4","modified":1584836812000},{"_id":"themes/matery/layout/_partial/valine.ejs","hash":"25bb67cf1c7dc297088f0bdd9a30a05231aeb5d0","modified":1584836812000},{"_id":"themes/matery/layout/_widget/category-cloud.ejs","hash":"424ef5db791264a79c1f3338e7c43a2f445cb2ab","modified":1584836812000},{"_id":"themes/matery/layout/_widget/category-radar.ejs","hash":"383a4501d42df2dadb254f2ae6facc1886605497","modified":1584836812000},{"_id":"themes/matery/layout/_widget/dream.ejs","hash":"ba83115ce66f4328601e567aa30f50d1410b9bfa","modified":1584836812000},{"_id":"themes/matery/layout/_widget/music.ejs","hash":"7618b3ed63d714ba67281a93870fe947aa11fa14","modified":1585479427546},{"_id":"themes/matery/layout/_widget/my-projects.ejs","hash":"dbd8df5146bd6e873535e24f09dd7cf04e17a4e4","modified":1584836812000},{"_id":"themes/matery/layout/_widget/my-gallery.ejs","hash":"de2e0abc085b721318f35c0b5d4891230be36529","modified":1585489436578},{"_id":"themes/matery/layout/_widget/my-skills.ejs","hash":"89a0092df72d23093128f2fbbdc8ca7f83ebcfd9","modified":1584836812000},{"_id":"themes/matery/layout/_widget/post-calendar.ejs","hash":"3fa9ceb2a28929b14002d59e2d96cc4bac39eb7b","modified":1584836812000},{"_id":"themes/matery/layout/_widget/post-charts.ejs","hash":"bff6033f89daa925e2d44c28b1dd4d21fb773dd8","modified":1584836812000},{"_id":"themes/matery/layout/_widget/recommend.ejs","hash":"7412060c10f4493a17b5c02cfc343fac106ab927","modified":1585486101479},{"_id":"themes/matery/layout/_widget/tag-cloud.ejs","hash":"fc42b72cddc231f7485cdc1fd6852b66be6add26","modified":1584836812000},{"_id":"themes/matery/layout/_widget/tag-wordcloud.ejs","hash":"70fc118bcc8705f47580e3c190bb486d94982032","modified":1584836812000},{"_id":"themes/matery/layout/_widget/video.ejs","hash":"d752c9f54300a9d762433ac4a00de3cc7e79d584","modified":1584836812000},{"_id":"themes/matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1584836812000},{"_id":"themes/matery/source/css/matery.css","hash":"9af007b47df7be7713a74ce670336b5b60d770d5","modified":1584836812000},{"_id":"themes/matery/source/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1584836812000},{"_id":"themes/matery/source/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1584836812000},{"_id":"themes/matery/source/js/matery.js","hash":"07ed4f743a497d7850b3fdda2a5d9beccc5a8fb5","modified":1584836812000},{"_id":"themes/matery/source/js/search.js","hash":"499e11786efbb04815b54a1de317cc8606a37555","modified":1584836812000},{"_id":"themes/matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1584836812000},{"_id":"themes/matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1584836812000},{"_id":"themes/matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1584836812000},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1584836812000},{"_id":"themes/matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1584836812000},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1584836812000},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1584836812000},{"_id":"themes/matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1584836812000},{"_id":"themes/matery/source/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1584836812000},{"_id":"themes/matery/source/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1584836812000},{"_id":"themes/matery/source/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1584836812000},{"_id":"themes/matery/source/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1584836812000},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1584836812000},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1584836812000},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1584836812000},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1584836812000},{"_id":"themes/matery/source/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1584836812000},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1584836812000},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1584836812000},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1584836812000},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1584836812000},{"_id":"themes/matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1584836812000},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1584836812000},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","hash":"15601837bf8557c2fd111e4450ed4c8495fd11a0","modified":1584836812000},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1584836812000},{"_id":"themes/matery/source/medias/reward/alipay.png","hash":"52065fd3ea5d7ccad3e3e555cbd92ca3a9f46caf","modified":1567586144276},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1584836812000},{"_id":"themes/matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1584836812000},{"_id":"themes/matery/source/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1584836812000},{"_id":"themes/matery/source/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1584836812000},{"_id":"themes/matery/source/medias/reward/wechat.png","hash":"24983b1b51c78d0ee6963fce32754d6042e7ac09","modified":1567586204639},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1584836812000},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","hash":"9a732790adc004b22022cc60fd5f77ec4c8e3e5a","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","hash":"fbfdbe06aebf7d0c00da175a4810cf888d128f11","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","hash":"1142b47de219dddfba2e712cd3189dec0c8b7bee","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","hash":"39150b45ec5fc03155b7ebeaa44f1829281788e2","modified":1584836812000},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1584836812000},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","hash":"30351cf15f5f2325275d7e0754afdef011f4b830","modified":1584836812000},{"_id":"themes/matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1584836812000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1584836812000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","hash":"f0a1b849868a6bf351ff98dc3924a4e7254eb88b","modified":1584836812000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1584836812000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1584836812000},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1584836812000},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1584836812000},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1584836812000},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.svg","hash":"229afff648cbd17de80176e0feb969c7f514be7e","modified":1584836812000},{"_id":"themes/matery/source/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1584836812000},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.svg","hash":"25612c76ded31c497effe46454d8d2bb36fb99d6","modified":1584836812000},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.svg","hash":"2c026711e4dd6b6d805cc19c0e4a572e6239a05b","modified":1584836812000},{"_id":"public/atom.xml","hash":"51405277ae4e6d7c3a12eed4b797eaec1f73cb27","modified":1597914464314},{"_id":"public/about/index.html","hash":"64b813aac83e52395426ae76187b9ea2e25a3c38","modified":1597914464314},{"_id":"public/archives/index.html","hash":"1e97bb800ca8a1d3614d2e0602e34d5362550c3a","modified":1597914464314},{"_id":"public/categories/index.html","hash":"d74c2bc65a3765860870c4d0fca40766851bb040","modified":1597914464314},{"_id":"public/friends/index.html","hash":"cea737b3f7eefcb0fa7086f433bacf2411e18314","modified":1597914464314},{"_id":"public/contact/index.html","hash":"7c5985a6b8efc8626f60adbc437655eba67d61c4","modified":1597914464314},{"_id":"public/tags/index.html","hash":"f5ce3413bc9b1d77cbb903347cd14e2f021da3e6","modified":1597914464314},{"_id":"public/2020/08/20/mei-yan-ji-zhu-de-pu-ji-dui-she-hui-shen-mei-shi-hao-shi/index.html","hash":"2e68edb6319586cfeaecdbb25b86717d30a49e6d","modified":1597914464314},{"_id":"public/2020/07/07/qian-zai-yu-yi-fen-xi/index.html","hash":"90c484a0c2c1fe5a3c61360d40a4803f71c740fc","modified":1597914464314},{"_id":"public/2020/06/20/jie-zhi-wang-yu-yun-fu-wu-ping-tai-ji-zhu/index.html","hash":"67e0da472977a057c5e806509935ec54dc93798a","modified":1597914464314},{"_id":"public/2020/06/18/duan-wen-ben-fen-lei/index.html","hash":"95ad26979ffebfc039358a3b46610914191c2b68","modified":1597914464314},{"_id":"public/2020/06/17/wo-yu-xiao-ke-ai/index.html","hash":"220c0d0b2da615eb5d703cb6ed4b5f0733722b81","modified":1597914464314},{"_id":"public/2020/06/07/yi-gou-tu-shen-jing-wang-luo/index.html","hash":"fc40b3b33e0aecd27edd4f2e16a0c3fea5bacba7","modified":1597914464314},{"_id":"public/2020/06/06/duan-wen-ben-fen-lei-jia-gou-shu-li/index.html","hash":"5cf1b8e75ad5140dc9efb90ff771476dc13cb7c6","modified":1597914464314},{"_id":"public/2020/05/19/mathjax-gong-shi-xuan-ran-xiu-fu/index.html","hash":"f0af993ac62a443ae4889e7416e0ef766f40f6c8","modified":1597914464314},{"_id":"public/2020/05/19/rnn-jia-gou-shu-li/index.html","hash":"ff09c202eef5892807a2020230d651f16f23c998","modified":1597914464314},{"_id":"public/2020/05/09/hbase-shell-ji-ben-cao-zuo/index.html","hash":"9c06e2411db7866ee1a51e031f8d416f034b64fa","modified":1597914464314},{"_id":"public/2020/05/07/zookeeper-hadoop-hbase-da-jian/index.html","hash":"d0fd4692127911e16d57b24ab34182952946fd7e","modified":1597914464314},{"_id":"public/2020/05/06/hadoop-ha-da-jian/index.html","hash":"2ee0b4b88e53f12c009472a15e7208bfefd5509d","modified":1597914464314},{"_id":"public/2020/05/03/qiu-shen-jie-zen-yang-ping-jie-yi-ge-mo-xing-de-fan-hua-neng-li/index.html","hash":"d1a62ead346ca487fad918aa6ec5e93aea33e0e4","modified":1597914464314},{"_id":"public/2020/05/01/xiao-nang-wang-luo/index.html","hash":"48f9f43214a54b22944b1843e8f72b153562f4f4","modified":1597914464314},{"_id":"public/2020/04/30/ubuntu1604-da-jian-hadoop-ji-qun/index.html","hash":"b4386eae1538d5e7285e850adce5c2e80a9f1359","modified":1597914464314},{"_id":"public/2020/04/30/ubuntu1604-she-zhi-jing-tai-ip/index.html","hash":"89b0f3a3514b1ae49a377b9133b1bdab50cdb16b","modified":1597914464314},{"_id":"public/2020/04/30/todo-list/index.html","hash":"0f5d13bb51624dc421aea85d0f05487afc635d8b","modified":1597914464314},{"_id":"public/2020/04/27/wen-xian-fan-yi-ji-yu-shen-du-xue-xi-de-wen-ben-fen-lei-quan-mian-hui-gu/index.html","hash":"8841c93ff6f3afa8aad854f40883884909d02aaf","modified":1597914464314},{"_id":"public/2020/04/21/ge-ren-ke-chi-xu-xing-fa-zhan/index.html","hash":"8b9d532e036cfc048fc0ba036c6acce98a8653f8","modified":1597914464314},{"_id":"public/2020/04/11/liao-bu-qi-de-gai-ci-bi/index.html","hash":"756b96caa4bc40c4b04a59298f6e0bbe89b7225a","modified":1597914464314},{"_id":"public/archives/page/2/index.html","hash":"fb6d38365715247a5689d204ee7709fbd21e1414","modified":1597914464314},{"_id":"public/archives/2020/index.html","hash":"f3af429f23857bc1ec9d830fa39a31c3058ef874","modified":1597914464314},{"_id":"public/archives/2020/page/2/index.html","hash":"6036594d85b6c672bcb641824e72f5b2120812ab","modified":1597914464314},{"_id":"public/archives/2020/04/index.html","hash":"ff7a38ba61f99d75c2300c36715522ea22a3177f","modified":1597914464314},{"_id":"public/archives/2020/05/index.html","hash":"a20963bcea29cc855b06273ea2c50f02f44275e2","modified":1597914464314},{"_id":"public/archives/2020/06/index.html","hash":"f13db85d31abf7fd27fb8bf71915a6e4e331e5b0","modified":1597914464314},{"_id":"public/archives/2020/07/index.html","hash":"f7ec501d5e73be412fc429f1f5f1ff025f7b08d4","modified":1597914464314},{"_id":"public/archives/2020/08/index.html","hash":"5f32c8687db3e433c2d324f3951e63c45db6d075","modified":1597914464314},{"_id":"public/categories/大数据/index.html","hash":"ad550db76e3100304ac76b9413a6571b6a3d5b6a","modified":1597914464314},{"_id":"public/categories/RNN/index.html","hash":"db8359555307d7729abe280bc38c576e0dc9ec08","modified":1597914464314},{"_id":"public/categories/待完成/index.html","hash":"d073c7b45d83e71c0da23ae0ad2f23bbd388dc0b","modified":1597914464314},{"_id":"public/categories/hexo/index.html","hash":"6117c7539c41eda4c2ead397bae01c50ff9ea88f","modified":1597914464314},{"_id":"public/categories/Linux/index.html","hash":"4a964be2f250dd5519f098a75f7d4ba255d13561","modified":1597914464314},{"_id":"public/categories/求甚解/index.html","hash":"91b615e181d7788e60ac118ea9f1ada1dcd7b35e","modified":1597914464314},{"_id":"public/categories/文献翻译/index.html","hash":"74aad5f3a500bb6249e9ebd8a19f98962b0c2ee9","modified":1597914464314},{"_id":"public/categories/杂/index.html","hash":"cd39c444da0fd3a634fcf622fffb28b401132fce","modified":1597914464314},{"_id":"public/categories/影评/index.html","hash":"7580e10b970559490f44b668866a26e4de167ea0","modified":1597914464314},{"_id":"public/categories/图网络/index.html","hash":"30cfafcd9afb9dffd53ff5bb48d25ea46b76d9d3","modified":1597914464314},{"_id":"public/categories/课程/index.html","hash":"b2f0c636a3bc541105418332d6947f01411f0db4","modified":1597914464314},{"_id":"public/categories/小可爱/index.html","hash":"f712bc32e5ef40026a96c556a5bcaa9ca062978b","modified":1597914464314},{"_id":"public/categories/机器学习/index.html","hash":"cf01a93cdc75a7e8f84e98beaffca5bd453c4daa","modified":1597914464314},{"_id":"public/categories/短文本/index.html","hash":"c1c24e15836f75ea4d002d4bf5344f071863811f","modified":1597914464314},{"_id":"public/categories/短文本理解/index.html","hash":"b28e05f2726b2aa522e5b171bc52c93b8527a66f","modified":1597914464314},{"_id":"public/categories/深度学习/index.html","hash":"0c2f5b0eb2451e960a4c6aa036a8c34958838f10","modified":1597914464314},{"_id":"public/index.html","hash":"59e277d36509b49d6fb26ffd7a4f81e13e8cf46d","modified":1597914464314},{"_id":"public/page/2/index.html","hash":"f4fe7638105623e407c75b34ec03f885e0fab94c","modified":1597914464314},{"_id":"public/tags/hbsse/index.html","hash":"86abafd3071af1fee90e6defa603e8ef29731418","modified":1597914464314},{"_id":"public/tags/大数据/index.html","hash":"e55a84b83f34b48dcc609d83012906f6facc6b2d","modified":1597914464314},{"_id":"public/tags/高可用/index.html","hash":"37ddbbdaf3ed3bc1159ffd88fe897849615de111","modified":1597914464314},{"_id":"public/tags/HA/index.html","hash":"31f07127d566ef36dbe1c21988f716318ffa05d3","modified":1597914464314},{"_id":"public/tags/hadoop/index.html","hash":"05c5003c9059edc51b4b57ac21d863da77a2b941","modified":1597914464314},{"_id":"public/tags/RNN/index.html","hash":"24cba0a506f88ad5ef03e925666a9ebecd104687","modified":1597914464314},{"_id":"public/tags/架构梳理/index.html","hash":"9c4b6c5d4ecaf705ca2c2f50b0c1591c736dd9d2","modified":1597914464314},{"_id":"public/tags/文本分类/index.html","hash":"a4a4089febaa49fe1af1b84332da919c3e033529","modified":1597914464314},{"_id":"public/tags/深度学习/index.html","hash":"82fdc5f9a6e384cd8e7de1a6d595cd1197a4a23b","modified":1597914464314},{"_id":"public/tags/todo-list/index.html","hash":"8a7a93c36cc9cdd7f7269d432ffa1ce4e9ad0c63","modified":1597914464314},{"_id":"public/tags/zookeeper/index.html","hash":"41b1e3e460d29d7b425c7ae7a73dff8aa5d6fec1","modified":1597914464314},{"_id":"public/tags/hbase/index.html","hash":"3d1f46b4f9913a296ba096ddebdfbb5dce8dad83","modified":1597914464314},{"_id":"public/tags/mathjax渲染/index.html","hash":"8aaf3d6b31bf476ed5a085ac12751715cc399051","modified":1597914464314},{"_id":"public/tags/hexo/index.html","hash":"e4791be97edb6fbc03a6d7604f5bf9ccdf5f146a","modified":1597914464314},{"_id":"public/tags/集群/index.html","hash":"49e8cd409ad97df259d4bb4c160b1d640cf3920a","modified":1597914464314},{"_id":"public/tags/IP/index.html","hash":"af5124bfe092c950611d97b1648b6b8b794c8fde","modified":1597914464314},{"_id":"public/tags/静态IP/index.html","hash":"5851dee76cd0147f66065a0deef526407f9efc8a","modified":1597914464314},{"_id":"public/tags/Ubuntu/index.html","hash":"db181f769efe3c7806a5e3eabc381c4455eaf04d","modified":1597914464314},{"_id":"public/tags/求甚解/index.html","hash":"f15c4c1f92c4e53923ff34d7fe8198c08e5207aa","modified":1597914464314},{"_id":"public/tags/综述/index.html","hash":"cb52e1098c3dfb4a814e03aa7c323e767d600e87","modified":1597914464314},{"_id":"public/tags/文献翻译/index.html","hash":"a0122fc508c341fec7ee6da4ff0d72d36c3e6c52","modified":1597914464314},{"_id":"public/tags/可持续性发展/index.html","hash":"55d0038823403bc4f5e72e5a77ef4aa0e83c763e","modified":1597914464314},{"_id":"public/tags/影评/index.html","hash":"2333b936fb09f564837577a112ee7ac3ccfab01d","modified":1597914464314},{"_id":"public/tags/异构图/index.html","hash":"3b877f67a59d12a3fd47401fce3dcf4e1bed9d8f","modified":1597914464314},{"_id":"public/tags/图网络/index.html","hash":"3ffd729a29f8cc2edde39f5615fafcc6f19fe7c2","modified":1597914464314},{"_id":"public/tags/课程/index.html","hash":"4db6b4ab7de50735ec603ab92e0287de3b07fd74","modified":1597914464314},{"_id":"public/tags/价值网与云服务平台技术/index.html","hash":"ea7c63dee16fa80f7031f64f57944a351bb55af1","modified":1597914464314},{"_id":"public/tags/小可爱/index.html","hash":"80664ade3b8ff59685e2cb295e8e1684891a590f","modified":1597914464314},{"_id":"public/tags/潜在语义分析/index.html","hash":"06c69d9212dd5e4471171b83e50132924cf76aaa","modified":1597914464314},{"_id":"public/tags/LSA/index.html","hash":"4a9cee898f5721b17c3c4833727b020fb601e786","modified":1597914464314},{"_id":"public/tags/短文本/index.html","hash":"7aafa47f980bff634dcc5d66a6c4a95756683ce3","modified":1597914464314},{"_id":"public/tags/分类/index.html","hash":"f97f95900c8a6e5afb0c506ca5ba6397c728ea0a","modified":1597914464314},{"_id":"public/tags/NLP/index.html","hash":"7b8e190bc54d7fc6dde86b521de5869652f487cd","modified":1597914464314},{"_id":"public/tags/胶囊网络/index.html","hash":"ee20beeb3bc415a21bb800e1918f9619682d0d99","modified":1597914464314},{"_id":"public/tags/Capsule-Networks/index.html","hash":"47dc56821fdf4a0fdd6872689adcab3e9098dfce","modified":1597914464314},{"_id":"public/CNAME","hash":"6f821e9ab75e29fdffecc2457e108d79834be3ab","modified":1597914464314},{"_id":"public/medias/reward/alipay.png","hash":"52065fd3ea5d7ccad3e3e555cbd92ca3a9f46caf","modified":1597914464314},{"_id":"public/medias/reward/wechat.png","hash":"24983b1b51c78d0ee6963fce32754d6042e7ac09","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1597914464314},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1597914464314},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9a732790adc004b22022cc60fd5f77ec4c8e3e5a","modified":1597914464314},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1597914464314},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1597914464314},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1597914464314},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"fbfdbe06aebf7d0c00da175a4810cf888d128f11","modified":1597914464314},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"1142b47de219dddfba2e712cd3189dec0c8b7bee","modified":1597914464314},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"39150b45ec5fc03155b7ebeaa44f1829281788e2","modified":1597914464314},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1597914464314},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"f0a1b849868a6bf351ff98dc3924a4e7254eb88b","modified":1597914464314},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1597914464314},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1597914464314},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1597914464314},{"_id":"public/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1597914464314},{"_id":"public/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1597914464314},{"_id":"public/js/matery.js","hash":"07ed4f743a497d7850b3fdda2a5d9beccc5a8fb5","modified":1597914464314},{"_id":"public/js/search.js","hash":"499e11786efbb04815b54a1de317cc8606a37555","modified":1597914464314},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1597914464314},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1597914464314},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1597914464314},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1597914464314},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1597914464314},{"_id":"public/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1597914464314},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1597914464314},{"_id":"public/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1597914464314},{"_id":"public/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1597914464314},{"_id":"public/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1597914464314},{"_id":"public/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1597914464314},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1597914464314},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1597914464314},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1597914464314},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1597914464314},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1597914464314},{"_id":"public/libs/tocbot/tocbot.css","hash":"15601837bf8557c2fd111e4450ed4c8495fd11a0","modified":1597914464314},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1597914464314},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1597914464314},{"_id":"public/css/matery.css","hash":"9af007b47df7be7713a74ce670336b5b60d770d5","modified":1597914464314},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1597914464314},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1597914464314},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1597914464314},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1597914464314},{"_id":"public/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1597914464314},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1597914464314},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1597914464314},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1597914464314},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1597914464314},{"_id":"public/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1597914464314},{"_id":"public/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1597914464314},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1597914464314},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1597914464314},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1597914464314},{"_id":"public/libs/materialize/materialize.min.css","hash":"30351cf15f5f2325275d7e0754afdef011f4b830","modified":1597914464314},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1597914464314},{"_id":"public/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1597914464314},{"_id":"public/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1597914464314},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1597914464314},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1597914464314},{"_id":"public/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-regular-400.svg","hash":"229afff648cbd17de80176e0feb969c7f514be7e","modified":1597914464314},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-brands-400.svg","hash":"25612c76ded31c497effe46454d8d2bb36fb99d6","modified":1597914464314},{"_id":"public/libs/awesome/webfonts/fa-solid-900.svg","hash":"2c026711e4dd6b6d805cc19c0e4a572e6239a05b","modified":1597914464314}],"Category":[{"name":"大数据","_id":"cke2l0m930004ewse0yzb4qvf"},{"name":"RNN","_id":"cke2l0mab000hewse8ggsgbx6"},{"name":"待完成","_id":"cke2l0mai000qewse4i0n6rex"},{"name":"hexo","_id":"cke2l0map000wewseg16n6iw3"},{"name":"Linux","_id":"cke2l0maw0011ewseeuabdidi"},{"name":"求甚解","_id":"cke2l0mb80017ewse9ljse8h1"},{"name":"文献翻译","_id":"cke2l0mbh001eewse0h5xen10"},{"name":"杂","_id":"cke2l0mbp001lewsebov892xq"},{"name":"影评","_id":"cke2l0mbr001oewse0h6egk25"},{"name":"图网络","_id":"cke2l0mbt001rewsecxnsfwcn"},{"name":"课程","_id":"cke2l0mbw001wewse8w73atm4"},{"name":"小可爱","_id":"cke2l0mc00022ewsebe85g7oj"},{"name":"机器学习","_id":"cke2l0mc20026ewse0s8u663j"},{"name":"短文本","_id":"cke2l0mc30029ewsefi7ad7ug"},{"name":"短文本理解","_id":"cke2l0mc5002cewsefdv7f9et"},{"name":"深度学习","_id":"cke2l0mc6002hewse7ggsf34c"}],"Data":[{"_id":"musics","data":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]},{"_id":"friends","data":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}],"Page":[{"title":"about","date":"2019-09-03T08:41:10.000Z","type":"about","layout":"about","_content":"\n\n# 在读研究生\n* <b>硕士 软件工程专业</b>\n西南交通大学\n预计2022年6月毕业\n\n\n# 联系方式\n* <b>Telegram</b>\nhttps://t.me/zhishuang\n* <b>电子邮箱</b>\nzhishuang.rao@gmail.com","source":"about/index.md","raw":"---\ntitle: about\ndate: 2019-09-03 16:41:10\ntype: \"about\"\nlayout: \"about\"\n---\n\n\n# 在读研究生\n* <b>硕士 软件工程专业</b>\n西南交通大学\n预计2022年6月毕业\n\n\n# 联系方式\n* <b>Telegram</b>\nhttps://t.me/zhishuang\n* <b>电子邮箱</b>\nzhishuang.rao@gmail.com","updated":"2020-03-28T09:09:39.651Z","path":"about/index.html","comments":1,"_id":"cke2l0m8w0001ewse8rkb1xrl","content":"<h1 id=\"在读研究生\"><a href=\"#在读研究生\" class=\"headerlink\" title=\"在读研究生\"></a>在读研究生</h1><ul>\n<li><b>硕士 软件工程专业</b><br>西南交通大学<br>预计2022年6月毕业</li>\n</ul>\n<h1 id=\"联系方式\"><a href=\"#联系方式\" class=\"headerlink\" title=\"联系方式\"></a>联系方式</h1><ul>\n<li><b>Telegram</b><br><a href=\"https://t.me/zhishuang\" target=\"_blank\" rel=\"noopener\">https://t.me/zhishuang</a></li>\n<li><b>电子邮箱</b><br>zhishuang.rao@gmail.com</li>\n</ul>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h1 id=\"在读研究生\"><a href=\"#在读研究生\" class=\"headerlink\" title=\"在读研究生\"></a>在读研究生</h1><ul>\n<li><b>硕士 软件工程专业</b><br>西南交通大学<br>预计2022年6月毕业</li>\n</ul>\n<h1 id=\"联系方式\"><a href=\"#联系方式\" class=\"headerlink\" title=\"联系方式\"></a>联系方式</h1><ul>\n<li><b>Telegram</b><br><a href=\"https://t.me/zhishuang\" target=\"_blank\" rel=\"noopener\">https://t.me/zhishuang</a></li>\n<li><b>电子邮箱</b><br>zhishuang.rao@gmail.com</li>\n</ul>\n"},{"title":"archives","date":"2019-07-19T08:39:20.000Z","type":"archives","layout":"archives","_content":"","source":"archives/index.md","raw":"---\ntitle: archives\ndate: 2019-07-19 16:39:20\ntype: \"archives\"\nlayout: \"archives\"\n---","updated":"2020-03-28T08:11:38.201Z","path":"archives/index.html","comments":1,"_id":"cke2l0m900003ewse7rc02nn7","content":"<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":""},{"title":"categories","date":"2020-03-28T12:42:14.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-03-28 20:42:14\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2020-03-28T12:42:53.564Z","path":"categories/index.html","comments":1,"_id":"cke2l0m9i0007ewse1w283wo9","content":"<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":""},{"title":"friends","date":"2019-07-19T08:42:10.000Z","type":"friends","layout":"friends","_content":"\n**感谢所有支持和打赏过我的人，希望今后也能和大家一起学习进步。**","source":"friends/index.md","raw":"---\ntitle: friends\ndate: 2019-07-19 16:42:10\ntype: \"friends\"\nlayout: \"friends\"\n---\n\n**感谢所有支持和打赏过我的人，希望今后也能和大家一起学习进步。**","updated":"2020-03-28T12:44:46.241Z","path":"friends/index.html","comments":1,"_id":"cke2l0m9q0009ewse35fa9orv","content":"<p><strong>感谢所有支持和打赏过我的人，希望今后也能和大家一起学习进步。</strong></p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p><strong>感谢所有支持和打赏过我的人，希望今后也能和大家一起学习进步。</strong></p>\n"},{"title":"contact","date":"2019-09-03T09:17:02.000Z","type":"contact","layout":"contact","_content":"\n大家有任何问题，都可以在下面给我留言。\n或者加我QQ：1032979481\n我很忙啦，如果不是很麻烦的问题就直接在评论区留言啦。\n**当然不介意小改改加我哦~~**","source":"contact/index.md","raw":"---\ntitle: contact\ndate: 2019-09-03 17:17:02\ntype: \"contact\"\nlayout: \"contact\"\n---\n\n大家有任何问题，都可以在下面给我留言。\n或者加我QQ：1032979481\n我很忙啦，如果不是很麻烦的问题就直接在评论区留言啦。\n**当然不介意小改改加我哦~~**","updated":"2020-03-28T08:11:38.208Z","path":"contact/index.html","comments":1,"_id":"cke2l0m9z000dewsegrl19wwp","content":"<p>大家有任何问题，都可以在下面给我留言。<br>或者加我QQ：1032979481<br>我很忙啦，如果不是很麻烦的问题就直接在评论区留言啦。<br><strong>当然不介意小改改加我哦~~</strong></p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>大家有任何问题，都可以在下面给我留言。<br>或者加我QQ：1032979481<br>我很忙啦，如果不是很麻烦的问题就直接在评论区留言啦。<br><strong>当然不介意小改改加我哦~~</strong></p>\n"},{"title":"tags","date":"2019-07-19T08:40:27.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-07-19 16:40:27\ntype: \"tags\"\nlayout: \"tags\"\n---","updated":"2020-03-28T08:11:38.214Z","path":"tags/index.html","comments":1,"_id":"cke2l0ma7000fewse6h8rdq6v","content":"<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":""}],"Post":[{"title":"HBase shell 基本操作","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-05-09T06:46:32.000Z","password":null,"summary":null,"img":null,"keywords":"hbase 大数据","_content":"\n* 创建表：\n\t* `create 'student','info'`\n\n* 插入数据到表：\n\t* `put 'student','1001','info:sex','male’`\n\t* `put 'student','1001','info:age','18'`\n\n* 扫描查看表数据\n\t* `scan 'student'`\n\t* `scan 'student',{STARTROW => '1001', STOPROW => '1001'}`\n\t* `scan 'student',{STARTROW => '1001'}`\n\n* 更新指定字段的数据\n\t* `put 'student','1001','info:name','Nick'`\n\t* `put 'student','1001','info:age','100'`\n\n* 查看“指定行”或“指定列族:列”的数据\n\t* `get 'student','1001'`\n\t* `get 'student','1001','info:name'`\n\n* 删除数据\n\t* 删除某 rowkey 的全部数据：`deleteall 'student','1001'`\n\t* 删除某 rowkey 的某一列数据：`delete 'student','1001','info:sex'`\n\n* 清空表数据\n\t* `truncate 'student'`\n\n* 删除表\n\t* `disable 'student'`\n\t* `drop 'student'`","source":"_posts/HBase-shell-基本操作.md","raw":"---\ntitle: HBase shell 基本操作\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-05-09 14:46:32\npassword:\nsummary:\ncategories: 大数据\nimg:\nkeywords: hbase 大数据\ntags: \n\t- hbsse \n\t- 大数据\n---\n\n* 创建表：\n\t* `create 'student','info'`\n\n* 插入数据到表：\n\t* `put 'student','1001','info:sex','male’`\n\t* `put 'student','1001','info:age','18'`\n\n* 扫描查看表数据\n\t* `scan 'student'`\n\t* `scan 'student',{STARTROW => '1001', STOPROW => '1001'}`\n\t* `scan 'student',{STARTROW => '1001'}`\n\n* 更新指定字段的数据\n\t* `put 'student','1001','info:name','Nick'`\n\t* `put 'student','1001','info:age','100'`\n\n* 查看“指定行”或“指定列族:列”的数据\n\t* `get 'student','1001'`\n\t* `get 'student','1001','info:name'`\n\n* 删除数据\n\t* 删除某 rowkey 的全部数据：`deleteall 'student','1001'`\n\t* 删除某 rowkey 的某一列数据：`delete 'student','1001','info:sex'`\n\n* 清空表数据\n\t* `truncate 'student'`\n\n* 删除表\n\t* `disable 'student'`\n\t* `drop 'student'`","slug":"HBase-shell-基本操作","published":1,"updated":"2020-08-20T08:41:44.640Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0m8j0000ewsea6el4vln","content":"<ul>\n<li><p>创建表：</p>\n<ul>\n<li><code>create 'student','info'</code></li>\n</ul>\n</li>\n<li><p>插入数据到表：</p>\n<ul>\n<li><code>put 'student','1001','info:sex','male’</code></li>\n<li><code>put 'student','1001','info:age','18'</code></li>\n</ul>\n</li>\n<li><p>扫描查看表数据</p>\n<ul>\n<li><code>scan 'student'</code></li>\n<li><code>scan 'student',{STARTROW =&gt; '1001', STOPROW =&gt; '1001'}</code></li>\n<li><code>scan 'student',{STARTROW =&gt; '1001'}</code></li>\n</ul>\n</li>\n<li><p>更新指定字段的数据</p>\n<ul>\n<li><code>put 'student','1001','info:name','Nick'</code></li>\n<li><code>put 'student','1001','info:age','100'</code></li>\n</ul>\n</li>\n<li><p>查看“指定行”或“指定列族:列”的数据</p>\n<ul>\n<li><code>get 'student','1001'</code></li>\n<li><code>get 'student','1001','info:name'</code></li>\n</ul>\n</li>\n<li><p>删除数据</p>\n<ul>\n<li>删除某 rowkey 的全部数据：<code>deleteall 'student','1001'</code></li>\n<li>删除某 rowkey 的某一列数据：<code>delete 'student','1001','info:sex'</code></li>\n</ul>\n</li>\n<li><p>清空表数据</p>\n<ul>\n<li><code>truncate 'student'</code></li>\n</ul>\n</li>\n<li><p>删除表</p>\n<ul>\n<li><code>disable 'student'</code></li>\n<li><code>drop 'student'</code></li>\n</ul>\n</li>\n</ul>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<ul>\n<li><p>创建表：</p>\n<ul>\n<li><code>create &#39;student&#39;,&#39;info&#39;</code></li>\n</ul>\n</li>\n<li><p>插入数据到表：</p>\n<ul>\n<li><code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:sex&#39;,&#39;male’</code></li>\n<li><code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;18&#39;</code></li>\n</ul>\n</li>\n<li><p>扫描查看表数据</p>\n<ul>\n<li><code>scan &#39;student&#39;</code></li>\n<li><code>scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;, STOPROW =&gt; &#39;1001&#39;}</code></li>\n<li><code>scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;}</code></li>\n</ul>\n</li>\n<li><p>更新指定字段的数据</p>\n<ul>\n<li><code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Nick&#39;</code></li>\n<li><code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;100&#39;</code></li>\n</ul>\n</li>\n<li><p>查看“指定行”或“指定列族:列”的数据</p>\n<ul>\n<li><code>get &#39;student&#39;,&#39;1001&#39;</code></li>\n<li><code>get &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;</code></li>\n</ul>\n</li>\n<li><p>删除数据</p>\n<ul>\n<li>删除某 rowkey 的全部数据：<code>deleteall &#39;student&#39;,&#39;1001&#39;</code></li>\n<li>删除某 rowkey 的某一列数据：<code>delete &#39;student&#39;,&#39;1001&#39;,&#39;info:sex&#39;</code></li>\n</ul>\n</li>\n<li><p>清空表数据</p>\n<ul>\n<li><code>truncate &#39;student&#39;</code></li>\n</ul>\n</li>\n<li><p>删除表</p>\n<ul>\n<li><code>disable &#39;student&#39;</code></li>\n<li><code>drop &#39;student&#39;</code></li>\n</ul>\n</li>\n</ul>\n"},{"title":"Hadoop HA 搭建","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-05-06T12:30:35.000Z","password":null,"summary":null,"img":null,"keywords":"高可用 HA hadoop 大数据","_content":"\n所谓HA（High Available）是Hadoop2.0中引入来解决Hadoop1.0中单点故障问题的一种机制。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。\n\n## HDFS-HA集群配置\n\n### 规划集群\n\n| master            | slave1                  | slave2                   |\n| ----------------- | ----------------------- | ------------------------ |\n| NameNode (active) | NameNode (standby)      |                          |\n| JournalNode       | JournalNode             | JournalNode              |\n| DataNode          | DataNode                | DataNode                 |\n| ZK                | ZK                      | ZK                       |\n|                   | ResourceManager(active) | ResourceManager(standby) |\n| NodeManager       | NodeManager             | NodeManager              |\n\n### 配置Zookeeper集群\n\n在master、slave1和slave2三个节点上部署Zookeeper\n\n### 安装zookeeper\n\n解压zookeeper，并重命名为zookeeper\n\n```bash\ntar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/\ncd /usr/local\nmv apache-zookeeper-3.5.7-bin/ zookeeper\n```\n\n### 配置zookeeper\n\n* 在`/usr/local/zookeeper/`这个目录下创建zkData\n\n```bash\ncd /usr/local/zookeeper\nmkdir zkData\n```\n\n* 重命名`/usr/local/zookeeper/conf`这个目录下的zoo_sample.cfg为zoo.cfg\n\n```bash\ncd /usr/local/zookeeper/conf\nmv zoo_sample.cfg zoo.cfg\n```\n\n* 执行`sudo gedit zoo.cfg`打开配置文件，配置如下：\n\n```bash\n# 修改原有dataDir值如下\ndataDir=/usr/local/zookeeper/zkData\n#######################cluster##########################\nserver.1=master:2888:3888\nserver.2=slave1:2888:3888\nserver.3=slave2:2888:3888\n```\n\nServer.A=B:C:D。\n\nA是一个数字，表示这个是第几号服务器；\n\nB是这个服务器的IP地址；\n\nC是这个服务器与集群中的Leader服务器交换信息的端口；\n\nD是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。\n\n集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。\n\n* 在/usr/local/zookeeper/zkData目录下创建并打开文件myid\n\n```bash\ngedit myid\n```\n\n在myid文件中增加数据：1\n\n* 拷贝配置好的zookeeper到其他机器上,并分别修改myid文件中内容为2、3\n\n```bash\nscp -r /usr/local/zookeeper/ slave1:/usr/local/\nscp -r /usr/local/zookeeper/ slave2:/usr/local/\n```\n\n* 配置zookeeper环境变量\n\n执行`sudo gedit /etc/profile`打开配置文件，加入下列内容\n\n```bash\nexport ZOOKEEPER_HOME=/usr/local/zookeeper\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n```\n\n执行`source /etc/profile`命令让配置文件生效\n\n* 拷贝`/etc/profile`文件到其他机器\n\n```bash\nscp /etc/profile slave1:/etc\n```\n\n\n\n### 启动zookeeper\n\n分别启动zookeeper,在三台机器上执行`zkServer.sh start`命令启动zookeeper,再执行`jps`命令可以看到有`QuorumPeerMain`\n\n## 配置HDFS-HA集群\n\n* 配置core-site.xml：\n\n```xml\n<configuration>\n\t<!-- 把两个NameNode的地址组装成一个集群mycluster -->\n\t<property>\n\t\t<name>fs.defaultFS</name>\n        <value>hdfs://mycluster</value>\n\t</property>\n\n\t<!-- 指定hadoop运行时产生文件的存储目录 -->\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/usr/local/hadoop/tmp</value>\n\t</property>\n\t<!-- 在网页界面访问数据使用的用户名。默认值是一个不真实存在的用户，此用户权限很小 -->\n\t<property>\n        <name>hadoop.http.staticuser.user</name>\n        <value>ubuntu</value>\n\t</property> \n\t<!--Ha功能，需要一组zk地址，用逗号分隔。被ZKFailoverController使用于自动失效备援failover-->\n\t<property>\n   \t\t<name>ha.zookeeper.quorum</name>\n  \t    <value>master:2181,slave1:2181,slave2:2181</value>\n \t</property>\n</configuration>\n```\n\n* 配置hdfs-site.xml\n\n```xml\n<configuration>\n\t<!-- 冗余度 -->\n\t<property>\n  \t\t<name>dfs.replication</name>\n  \t\t<value>3</value>\n\t</property>\n\t<!-- 暂不配置 \n\t<property>\n  \t\t<name>dfs.namenode.secondary.http-address</name>\n  \t\t<value>slave1:9869</value>\n\t</property>\n\t-->\n\t<!-- 完全分布式集群名称 -->\n\t<property>\n\t\t<name>dfs.nameservices</name>\n\t\t<value>mycluster</value>\n\t</property>\n\n\t<!-- 集群中NameNode节点都有哪些 -->\n\t<property>\n\t\t<name>dfs.ha.namenodes.mycluster</name>\n\t\t<value>nn1,nn2</value>\n\t</property>\n\n\t<!-- nn1的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.mycluster.nn1</name>\n\t\t<value>master:8020</value>\n\t</property>\n\n\t<!-- nn2的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.mycluster.nn2</name>\n\t\t<value>slave1:8020</value>\n\t</property>\n\n\t<!-- nn1的http通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.mycluster.nn1</name>\n\t\t<value>master:9870</value>\n\t</property>\n\n\t<!-- nn2的http通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.mycluster.nn2</name>\n\t\t<value>slave1:9870</value>\n\t</property>\n\n\t<!-- 指定NameNode元数据在JournalNode上的存放位置 -->\n\t<property>\n\t\t<name>dfs.namenode.shared.edits.dir</name>\n\t<value>qjournal://master:8485;slave1:8485;slave2:8485/mycluster</value>\n\t</property>\n\n\t<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.methods</name>\n\t\t<value>sshfence</value>\n\t</property>\n\n\t<!-- 使用隔离机制时需要ssh无秘钥登录-->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.private-key-files</name>\n\t\t<value>/home/ubuntu/.ssh/id_rsa</value>\n\t</property>\n\n\t<!-- 声明journalnode服务器存储目录-->\n\t<property>\n\t\t<name>dfs.journalnode.edits.dir</name>\n\t\t<value>/usr/local/hadoop/tmp/jn</value>\n\t</property>\n\n\t<!-- 关闭权限检查-->\n\t<property>\n\t\t<name>dfs.permissions.enable</name>\n\t\t<value>false</value>\n\t</property>\n\t\n\t<!-- 是否开启自动故障转移-->\n\t<property>\n   \t\t<name>dfs.ha.automatic-failover.enabled</name>\n   \t\t<value>true</value>\n\t</property>\n\t\n\t<!--访问代理类：client，mycluster，active配置失败自动切换实现方式-->\n\t<property>\n  \t\t<name>dfs.client.failover.proxy.provider.mycluster</name>\n\t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\t</property>\n</configuration>\n```\n\n* 配置mapred-site.xml\n\n```xml\n<configuration>\n   <property>\n       <name>mapreduce.framework.name</name>\n       <value>yarn</value>\n   </property>\n</configuration>\n```\n\n配置yarn-site.xml\n\n```xml\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n\n    <!--启用resourcemanager ha-->\n    <property>\n        <name>yarn.resourcemanager.ha.enabled</name>\n        <value>true</value>\n    </property>\n \n    <!--声明两台resourcemanager的地址-->\n    <property>\n        <name>yarn.resourcemanager.cluster-id</name>\n        <value>cluster-yarn1</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.ha.rm-ids</name>\n        <value>rm1,rm2</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.hostname.rm1</name>\n        <value>slave1</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.hostname.rm2</name>\n        <value>slave2</value>\n    </property>\n \n    <!--指定zookeeper集群的地址--> \n    <property>\n        <name>yarn.resourcemanager.zk-address</name>\n        <value>master:2181,slave1:2181,slave2:2181</value>\n    </property>\n\n    <!--启用自动恢复--> \n    <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n \n    <!--指定resourcemanager的状态信息存储在zookeeper集群--> \n    <property>\n\t<name>yarn.resourcemanager.store.class</name>\n<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n    </property>\n\n</configuration>\n```\n\n* 拷贝配置好的Hadoop文件到其他机器上\n\n```bash\nscp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n```\n\n## 启动\n\n### 启动zookeeper集群\n\n分别在master，slave1，slave2上执行`zkServer.sh start`命名启动zk\n\n### 启动journalnode\n\n分别在master，slave1，slave2上执行`hdfs --daemon start journalnode`命令启动JN\n\n运行jps命令检验，master，slave1，slave2上多了JournalNode进程\n\n### 格式化namenode，并启动\n\n在master上执行命令`hdfs namenode -format`格式化namenode\n\n在master上执行命令`hdfs --daemon start namenode`命令格式化namenode\n\n### 副节点同步主节点格式化\n\n在slave1上执行命令`hdfs namenode -bootstrapStandby`命令同步主节点格式化\n\n### 格式化ZKFC\n\n在master上执行命令`hdfs zkfc -formatZK`格式化ZKFC，第一次启动时需要格式化\n\n### 启动HDFS\n\n在master上执行命令`start-dfs.sh`启动HDFS\n\n### 启动YARN\n\n在slave1上执行`start-yarn.sh`命令启动YARN，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动\n\n## 测试集群工作状态的一些指令 \n\n* `hdfs dfsadmin -report` 查看hdfs的各节点状态信息\n\n* `hdfs haadmin -getServiceState nn1` 获取一个namenode节点的HA状态\n* `hdfs haadmin -transitionToActive nn1`将nn1切换为Active\n* `hdfs haadmin -transitionToStandby nn1`将nn1切换为Standby\n* `hdfs haadmin -failover nn1 nn2`主备切换\n* `yarn rmadmin -getServiceState rm1`获取一个resourcemanager节点的HA状态\n* `hdfs --daemon start namenode` 单独启动一个namenode进程\n* `hdfs --daemon start zkfc` 单独启动一个zkfc进程\n\n## 测试集群高可用\n\n执行`hadoop fs -put /etc/profile /profile`上传一个文件到hdfs\n\n执行`hadoop fs -ls /`可查看hdfs下的文件 ：\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507183228.png)\n\n执行`hdfs haadmin -getServiceState nn1`和``hdfs haadmin -getServiceState nn2`命令查master和slave1上NameNode的状态\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191108.png)\n\n在master上执行`jps`命令查看NameNode进程ID，再执行`kill -p <pid of NameNode>`关掉Active状态的NameNode，再执行`jps`会发现没有NameNode了,最后执行`hdfs haadmin -getServiceState nn2`会发现slave1的NameNode已经是active状态了\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191904.png)\n\n执行`hadoop fs -ls /`会发现仍然能访问到hdfs的文件\n\n在master上执行`hdfs --daemon start namenode`重新启动NameNode，再执行`hdfs haadmin -getServiceState nn1`会发现master上NameNode的状态变为了standby\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507192833.png)\n\n执行`hdfs haadmin -failover nn2 nn1`会主备切换，nn1变为active，nn2变为standby.\n\n## 群起脚本\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   \techo \"======================== start zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh start\"\n\tdone\n\techo \"======================== start hdfs ========================== \"\n  \tssh master \"source /etc/profile;start-dfs.sh\"\n   \techo \"======================== start yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;start-yarn.sh\"\n;;\n\"stop\")\n\techo \"======================== stop yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;stop-yarn.sh\"\n   \techo \"======================== stop hdfs ========================== \"\n  \tssh master \"source /etc/profile;stop-dfs.sh\"\n  \techo \"======================== stop zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh stop\"\n\tdone\n;;\n*)\n  \techo \"Input Args Error!!!!!\"\n;;\nesac\n```\n\n","source":"_posts/Hadoop-HA-搭建.md","raw":"---\ntitle: Hadoop HA 搭建\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-05-06 20:30:35\npassword:\nsummary: \ncategories: 大数据\nimg:\nkeywords: 高可用 HA hadoop 大数据\ntags:\n\t- 高可用\n\t- HA\n\t- hadoop\n\t- 大数据\n---\n\n所谓HA（High Available）是Hadoop2.0中引入来解决Hadoop1.0中单点故障问题的一种机制。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。\n\n## HDFS-HA集群配置\n\n### 规划集群\n\n| master            | slave1                  | slave2                   |\n| ----------------- | ----------------------- | ------------------------ |\n| NameNode (active) | NameNode (standby)      |                          |\n| JournalNode       | JournalNode             | JournalNode              |\n| DataNode          | DataNode                | DataNode                 |\n| ZK                | ZK                      | ZK                       |\n|                   | ResourceManager(active) | ResourceManager(standby) |\n| NodeManager       | NodeManager             | NodeManager              |\n\n### 配置Zookeeper集群\n\n在master、slave1和slave2三个节点上部署Zookeeper\n\n### 安装zookeeper\n\n解压zookeeper，并重命名为zookeeper\n\n```bash\ntar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/\ncd /usr/local\nmv apache-zookeeper-3.5.7-bin/ zookeeper\n```\n\n### 配置zookeeper\n\n* 在`/usr/local/zookeeper/`这个目录下创建zkData\n\n```bash\ncd /usr/local/zookeeper\nmkdir zkData\n```\n\n* 重命名`/usr/local/zookeeper/conf`这个目录下的zoo_sample.cfg为zoo.cfg\n\n```bash\ncd /usr/local/zookeeper/conf\nmv zoo_sample.cfg zoo.cfg\n```\n\n* 执行`sudo gedit zoo.cfg`打开配置文件，配置如下：\n\n```bash\n# 修改原有dataDir值如下\ndataDir=/usr/local/zookeeper/zkData\n#######################cluster##########################\nserver.1=master:2888:3888\nserver.2=slave1:2888:3888\nserver.3=slave2:2888:3888\n```\n\nServer.A=B:C:D。\n\nA是一个数字，表示这个是第几号服务器；\n\nB是这个服务器的IP地址；\n\nC是这个服务器与集群中的Leader服务器交换信息的端口；\n\nD是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。\n\n集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。\n\n* 在/usr/local/zookeeper/zkData目录下创建并打开文件myid\n\n```bash\ngedit myid\n```\n\n在myid文件中增加数据：1\n\n* 拷贝配置好的zookeeper到其他机器上,并分别修改myid文件中内容为2、3\n\n```bash\nscp -r /usr/local/zookeeper/ slave1:/usr/local/\nscp -r /usr/local/zookeeper/ slave2:/usr/local/\n```\n\n* 配置zookeeper环境变量\n\n执行`sudo gedit /etc/profile`打开配置文件，加入下列内容\n\n```bash\nexport ZOOKEEPER_HOME=/usr/local/zookeeper\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n```\n\n执行`source /etc/profile`命令让配置文件生效\n\n* 拷贝`/etc/profile`文件到其他机器\n\n```bash\nscp /etc/profile slave1:/etc\n```\n\n\n\n### 启动zookeeper\n\n分别启动zookeeper,在三台机器上执行`zkServer.sh start`命令启动zookeeper,再执行`jps`命令可以看到有`QuorumPeerMain`\n\n## 配置HDFS-HA集群\n\n* 配置core-site.xml：\n\n```xml\n<configuration>\n\t<!-- 把两个NameNode的地址组装成一个集群mycluster -->\n\t<property>\n\t\t<name>fs.defaultFS</name>\n        <value>hdfs://mycluster</value>\n\t</property>\n\n\t<!-- 指定hadoop运行时产生文件的存储目录 -->\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/usr/local/hadoop/tmp</value>\n\t</property>\n\t<!-- 在网页界面访问数据使用的用户名。默认值是一个不真实存在的用户，此用户权限很小 -->\n\t<property>\n        <name>hadoop.http.staticuser.user</name>\n        <value>ubuntu</value>\n\t</property> \n\t<!--Ha功能，需要一组zk地址，用逗号分隔。被ZKFailoverController使用于自动失效备援failover-->\n\t<property>\n   \t\t<name>ha.zookeeper.quorum</name>\n  \t    <value>master:2181,slave1:2181,slave2:2181</value>\n \t</property>\n</configuration>\n```\n\n* 配置hdfs-site.xml\n\n```xml\n<configuration>\n\t<!-- 冗余度 -->\n\t<property>\n  \t\t<name>dfs.replication</name>\n  \t\t<value>3</value>\n\t</property>\n\t<!-- 暂不配置 \n\t<property>\n  \t\t<name>dfs.namenode.secondary.http-address</name>\n  \t\t<value>slave1:9869</value>\n\t</property>\n\t-->\n\t<!-- 完全分布式集群名称 -->\n\t<property>\n\t\t<name>dfs.nameservices</name>\n\t\t<value>mycluster</value>\n\t</property>\n\n\t<!-- 集群中NameNode节点都有哪些 -->\n\t<property>\n\t\t<name>dfs.ha.namenodes.mycluster</name>\n\t\t<value>nn1,nn2</value>\n\t</property>\n\n\t<!-- nn1的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.mycluster.nn1</name>\n\t\t<value>master:8020</value>\n\t</property>\n\n\t<!-- nn2的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.mycluster.nn2</name>\n\t\t<value>slave1:8020</value>\n\t</property>\n\n\t<!-- nn1的http通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.mycluster.nn1</name>\n\t\t<value>master:9870</value>\n\t</property>\n\n\t<!-- nn2的http通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.mycluster.nn2</name>\n\t\t<value>slave1:9870</value>\n\t</property>\n\n\t<!-- 指定NameNode元数据在JournalNode上的存放位置 -->\n\t<property>\n\t\t<name>dfs.namenode.shared.edits.dir</name>\n\t<value>qjournal://master:8485;slave1:8485;slave2:8485/mycluster</value>\n\t</property>\n\n\t<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.methods</name>\n\t\t<value>sshfence</value>\n\t</property>\n\n\t<!-- 使用隔离机制时需要ssh无秘钥登录-->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.private-key-files</name>\n\t\t<value>/home/ubuntu/.ssh/id_rsa</value>\n\t</property>\n\n\t<!-- 声明journalnode服务器存储目录-->\n\t<property>\n\t\t<name>dfs.journalnode.edits.dir</name>\n\t\t<value>/usr/local/hadoop/tmp/jn</value>\n\t</property>\n\n\t<!-- 关闭权限检查-->\n\t<property>\n\t\t<name>dfs.permissions.enable</name>\n\t\t<value>false</value>\n\t</property>\n\t\n\t<!-- 是否开启自动故障转移-->\n\t<property>\n   \t\t<name>dfs.ha.automatic-failover.enabled</name>\n   \t\t<value>true</value>\n\t</property>\n\t\n\t<!--访问代理类：client，mycluster，active配置失败自动切换实现方式-->\n\t<property>\n  \t\t<name>dfs.client.failover.proxy.provider.mycluster</name>\n\t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\t</property>\n</configuration>\n```\n\n* 配置mapred-site.xml\n\n```xml\n<configuration>\n   <property>\n       <name>mapreduce.framework.name</name>\n       <value>yarn</value>\n   </property>\n</configuration>\n```\n\n配置yarn-site.xml\n\n```xml\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n\n    <!--启用resourcemanager ha-->\n    <property>\n        <name>yarn.resourcemanager.ha.enabled</name>\n        <value>true</value>\n    </property>\n \n    <!--声明两台resourcemanager的地址-->\n    <property>\n        <name>yarn.resourcemanager.cluster-id</name>\n        <value>cluster-yarn1</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.ha.rm-ids</name>\n        <value>rm1,rm2</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.hostname.rm1</name>\n        <value>slave1</value>\n    </property>\n\n    <property>\n        <name>yarn.resourcemanager.hostname.rm2</name>\n        <value>slave2</value>\n    </property>\n \n    <!--指定zookeeper集群的地址--> \n    <property>\n        <name>yarn.resourcemanager.zk-address</name>\n        <value>master:2181,slave1:2181,slave2:2181</value>\n    </property>\n\n    <!--启用自动恢复--> \n    <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n \n    <!--指定resourcemanager的状态信息存储在zookeeper集群--> \n    <property>\n\t<name>yarn.resourcemanager.store.class</name>\n<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n    </property>\n\n</configuration>\n```\n\n* 拷贝配置好的Hadoop文件到其他机器上\n\n```bash\nscp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n```\n\n## 启动\n\n### 启动zookeeper集群\n\n分别在master，slave1，slave2上执行`zkServer.sh start`命名启动zk\n\n### 启动journalnode\n\n分别在master，slave1，slave2上执行`hdfs --daemon start journalnode`命令启动JN\n\n运行jps命令检验，master，slave1，slave2上多了JournalNode进程\n\n### 格式化namenode，并启动\n\n在master上执行命令`hdfs namenode -format`格式化namenode\n\n在master上执行命令`hdfs --daemon start namenode`命令格式化namenode\n\n### 副节点同步主节点格式化\n\n在slave1上执行命令`hdfs namenode -bootstrapStandby`命令同步主节点格式化\n\n### 格式化ZKFC\n\n在master上执行命令`hdfs zkfc -formatZK`格式化ZKFC，第一次启动时需要格式化\n\n### 启动HDFS\n\n在master上执行命令`start-dfs.sh`启动HDFS\n\n### 启动YARN\n\n在slave1上执行`start-yarn.sh`命令启动YARN，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动\n\n## 测试集群工作状态的一些指令 \n\n* `hdfs dfsadmin -report` 查看hdfs的各节点状态信息\n\n* `hdfs haadmin -getServiceState nn1` 获取一个namenode节点的HA状态\n* `hdfs haadmin -transitionToActive nn1`将nn1切换为Active\n* `hdfs haadmin -transitionToStandby nn1`将nn1切换为Standby\n* `hdfs haadmin -failover nn1 nn2`主备切换\n* `yarn rmadmin -getServiceState rm1`获取一个resourcemanager节点的HA状态\n* `hdfs --daemon start namenode` 单独启动一个namenode进程\n* `hdfs --daemon start zkfc` 单独启动一个zkfc进程\n\n## 测试集群高可用\n\n执行`hadoop fs -put /etc/profile /profile`上传一个文件到hdfs\n\n执行`hadoop fs -ls /`可查看hdfs下的文件 ：\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507183228.png)\n\n执行`hdfs haadmin -getServiceState nn1`和``hdfs haadmin -getServiceState nn2`命令查master和slave1上NameNode的状态\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191108.png)\n\n在master上执行`jps`命令查看NameNode进程ID，再执行`kill -p <pid of NameNode>`关掉Active状态的NameNode，再执行`jps`会发现没有NameNode了,最后执行`hdfs haadmin -getServiceState nn2`会发现slave1的NameNode已经是active状态了\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191904.png)\n\n执行`hadoop fs -ls /`会发现仍然能访问到hdfs的文件\n\n在master上执行`hdfs --daemon start namenode`重新启动NameNode，再执行`hdfs haadmin -getServiceState nn1`会发现master上NameNode的状态变为了standby\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507192833.png)\n\n执行`hdfs haadmin -failover nn2 nn1`会主备切换，nn1变为active，nn2变为standby.\n\n## 群起脚本\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   \techo \"======================== start zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh start\"\n\tdone\n\techo \"======================== start hdfs ========================== \"\n  \tssh master \"source /etc/profile;start-dfs.sh\"\n   \techo \"======================== start yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;start-yarn.sh\"\n;;\n\"stop\")\n\techo \"======================== stop yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;stop-yarn.sh\"\n   \techo \"======================== stop hdfs ========================== \"\n  \tssh master \"source /etc/profile;stop-dfs.sh\"\n  \techo \"======================== stop zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh stop\"\n\tdone\n;;\n*)\n  \techo \"Input Args Error!!!!!\"\n;;\nesac\n```\n\n","slug":"Hadoop-HA-搭建","published":1,"updated":"2020-08-20T08:41:44.641Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0m8y0002ewse815g51qf","content":"<p>所谓HA（High Available）是Hadoop2.0中引入来解决Hadoop1.0中单点故障问题的一种机制。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。</p>\n<h2 id=\"HDFS-HA集群配置\"><a href=\"#HDFS-HA集群配置\" class=\"headerlink\" title=\"HDFS-HA集群配置\"></a>HDFS-HA集群配置</h2><h3 id=\"规划集群\"><a href=\"#规划集群\" class=\"headerlink\" title=\"规划集群\"></a>规划集群</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>master</th>\n<th>slave1</th>\n<th>slave2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NameNode (active)</td>\n<td>NameNode (standby)</td>\n<td></td>\n</tr>\n<tr>\n<td>JournalNode</td>\n<td>JournalNode</td>\n<td>JournalNode</td>\n</tr>\n<tr>\n<td>DataNode</td>\n<td>DataNode</td>\n<td>DataNode</td>\n</tr>\n<tr>\n<td>ZK</td>\n<td>ZK</td>\n<td>ZK</td>\n</tr>\n<tr>\n<td></td>\n<td>ResourceManager(active)</td>\n<td>ResourceManager(standby)</td>\n</tr>\n<tr>\n<td>NodeManager</td>\n<td>NodeManager</td>\n<td>NodeManager</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"配置Zookeeper集群\"><a href=\"#配置Zookeeper集群\" class=\"headerlink\" title=\"配置Zookeeper集群\"></a>配置Zookeeper集群</h3><p>在master、slave1和slave2三个节点上部署Zookeeper</p>\n<h3 id=\"安装zookeeper\"><a href=\"#安装zookeeper\" class=\"headerlink\" title=\"安装zookeeper\"></a>安装zookeeper</h3><p>解压zookeeper，并重命名为zookeeper</p>\n<pre><code class=\"lang-bash\">tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/\ncd /usr/local\nmv apache-zookeeper-3.5.7-bin/ zookeeper\n</code></pre>\n<h3 id=\"配置zookeeper\"><a href=\"#配置zookeeper\" class=\"headerlink\" title=\"配置zookeeper\"></a>配置zookeeper</h3><ul>\n<li>在<code>/usr/local/zookeeper/</code>这个目录下创建zkData</li>\n</ul>\n<pre><code class=\"lang-bash\">cd /usr/local/zookeeper\nmkdir zkData\n</code></pre>\n<ul>\n<li>重命名<code>/usr/local/zookeeper/conf</code>这个目录下的zoo_sample.cfg为zoo.cfg</li>\n</ul>\n<pre><code class=\"lang-bash\">cd /usr/local/zookeeper/conf\nmv zoo_sample.cfg zoo.cfg\n</code></pre>\n<ul>\n<li>执行<code>sudo gedit zoo.cfg</code>打开配置文件，配置如下：</li>\n</ul>\n<pre><code class=\"lang-bash\"># 修改原有dataDir值如下\ndataDir=/usr/local/zookeeper/zkData\n#######################cluster##########################\nserver.1=master:2888:3888\nserver.2=slave1:2888:3888\nserver.3=slave2:2888:3888\n</code></pre>\n<p>Server.A=B:C:D。</p>\n<p>A是一个数字，表示这个是第几号服务器；</p>\n<p>B是这个服务器的IP地址；</p>\n<p>C是这个服务器与集群中的Leader服务器交换信息的端口；</p>\n<p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>\n<p>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p>\n<ul>\n<li>在/usr/local/zookeeper/zkData目录下创建并打开文件myid</li>\n</ul>\n<pre><code class=\"lang-bash\">gedit myid\n</code></pre>\n<p>在myid文件中增加数据：1</p>\n<ul>\n<li>拷贝配置好的zookeeper到其他机器上,并分别修改myid文件中内容为2、3</li>\n</ul>\n<pre><code class=\"lang-bash\">scp -r /usr/local/zookeeper/ slave1:/usr/local/\nscp -r /usr/local/zookeeper/ slave2:/usr/local/\n</code></pre>\n<ul>\n<li>配置zookeeper环境变量</li>\n</ul>\n<p>执行<code>sudo gedit /etc/profile</code>打开配置文件，加入下列内容</p>\n<pre><code class=\"lang-bash\">export ZOOKEEPER_HOME=/usr/local/zookeeper\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n</code></pre>\n<p>执行<code>source /etc/profile</code>命令让配置文件生效</p>\n<ul>\n<li>拷贝<code>/etc/profile</code>文件到其他机器</li>\n</ul>\n<pre><code class=\"lang-bash\">scp /etc/profile slave1:/etc\n</code></pre>\n<h3 id=\"启动zookeeper\"><a href=\"#启动zookeeper\" class=\"headerlink\" title=\"启动zookeeper\"></a>启动zookeeper</h3><p>分别启动zookeeper,在三台机器上执行<code>zkServer.sh start</code>命令启动zookeeper,再执行<code>jps</code>命令可以看到有<code>QuorumPeerMain</code></p>\n<h2 id=\"配置HDFS-HA集群\"><a href=\"#配置HDFS-HA集群\" class=\"headerlink\" title=\"配置HDFS-HA集群\"></a>配置HDFS-HA集群</h2><ul>\n<li>配置core-site.xml：</li>\n</ul>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;!-- 把两个NameNode的地址组装成一个集群mycluster --&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://mycluster&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!-- 在网页界面访问数据使用的用户名。默认值是一个不真实存在的用户，此用户权限很小 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n        &lt;value&gt;ubuntu&lt;/value&gt;\n    &lt;/property&gt; \n    &lt;!--Ha功能，需要一组zk地址，用逗号分隔。被ZKFailoverController使用于自动失效备援failover--&gt;\n    &lt;property&gt;\n           &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\n          &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt;\n     &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>配置hdfs-site.xml</li>\n</ul>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;!-- 冗余度 --&gt;\n    &lt;property&gt;\n          &lt;name&gt;dfs.replication&lt;/name&gt;\n          &lt;value&gt;3&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!-- 暂不配置 \n    &lt;property&gt;\n          &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n          &lt;value&gt;slave1:9869&lt;/value&gt;\n    &lt;/property&gt;\n    --&gt;\n    &lt;!-- 完全分布式集群名称 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.nameservices&lt;/name&gt;\n        &lt;value&gt;mycluster&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 集群中NameNode节点都有哪些 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;\n        &lt;value&gt;nn1,nn2&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn1的RPC通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;\n        &lt;value&gt;master:8020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn2的RPC通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;\n        &lt;value&gt;slave1:8020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn1的http通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;\n        &lt;value&gt;master:9870&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn2的http通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;\n        &lt;value&gt;slave1:9870&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\n    &lt;value&gt;qjournal://master:8485;slave1:8485;slave2:8485/mycluster&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\n        &lt;value&gt;sshfence&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\n        &lt;value&gt;/home/ubuntu/.ssh/id_rsa&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 声明journalnode服务器存储目录--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/hadoop/tmp/jn&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 关闭权限检查--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.permissions.enable&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 是否开启自动故障转移--&gt;\n    &lt;property&gt;\n           &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\n           &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;\n    &lt;property&gt;\n          &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>配置mapred-site.xml</li>\n</ul>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n   &lt;property&gt;\n       &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n       &lt;value&gt;yarn&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>配置yarn-site.xml</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--启用resourcemanager ha--&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--声明两台resourcemanager的地址--&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;\n        &lt;value&gt;cluster-yarn1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;\n        &lt;value&gt;rm1,rm2&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;\n        &lt;value&gt;slave1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;\n        &lt;value&gt;slave2&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--指定zookeeper集群的地址--&gt; \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;\n        &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--启用自动恢复--&gt; \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; \n    &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;\n&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;\n    &lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>拷贝配置好的Hadoop文件到其他机器上</li>\n</ul>\n<pre><code class=\"lang-bash\">scp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n</code></pre>\n<h2 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h2><h3 id=\"启动zookeeper集群\"><a href=\"#启动zookeeper集群\" class=\"headerlink\" title=\"启动zookeeper集群\"></a>启动zookeeper集群</h3><p>分别在master，slave1，slave2上执行<code>zkServer.sh start</code>命名启动zk</p>\n<h3 id=\"启动journalnode\"><a href=\"#启动journalnode\" class=\"headerlink\" title=\"启动journalnode\"></a>启动journalnode</h3><p>分别在master，slave1，slave2上执行<code>hdfs --daemon start journalnode</code>命令启动JN</p>\n<p>运行jps命令检验，master，slave1，slave2上多了JournalNode进程</p>\n<h3 id=\"格式化namenode，并启动\"><a href=\"#格式化namenode，并启动\" class=\"headerlink\" title=\"格式化namenode，并启动\"></a>格式化namenode，并启动</h3><p>在master上执行命令<code>hdfs namenode -format</code>格式化namenode</p>\n<p>在master上执行命令<code>hdfs --daemon start namenode</code>命令格式化namenode</p>\n<h3 id=\"副节点同步主节点格式化\"><a href=\"#副节点同步主节点格式化\" class=\"headerlink\" title=\"副节点同步主节点格式化\"></a>副节点同步主节点格式化</h3><p>在slave1上执行命令<code>hdfs namenode -bootstrapStandby</code>命令同步主节点格式化</p>\n<h3 id=\"格式化ZKFC\"><a href=\"#格式化ZKFC\" class=\"headerlink\" title=\"格式化ZKFC\"></a>格式化ZKFC</h3><p>在master上执行命令<code>hdfs zkfc -formatZK</code>格式化ZKFC，第一次启动时需要格式化</p>\n<h3 id=\"启动HDFS\"><a href=\"#启动HDFS\" class=\"headerlink\" title=\"启动HDFS\"></a>启动HDFS</h3><p>在master上执行命令<code>start-dfs.sh</code>启动HDFS</p>\n<h3 id=\"启动YARN\"><a href=\"#启动YARN\" class=\"headerlink\" title=\"启动YARN\"></a>启动YARN</h3><p>在slave1上执行<code>start-yarn.sh</code>命令启动YARN，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动</p>\n<h2 id=\"测试集群工作状态的一些指令\"><a href=\"#测试集群工作状态的一些指令\" class=\"headerlink\" title=\"测试集群工作状态的一些指令\"></a>测试集群工作状态的一些指令</h2><ul>\n<li><p><code>hdfs dfsadmin -report</code> 查看hdfs的各节点状态信息</p>\n</li>\n<li><p><code>hdfs haadmin -getServiceState nn1</code> 获取一个namenode节点的HA状态</p>\n</li>\n<li><code>hdfs haadmin -transitionToActive nn1</code>将nn1切换为Active</li>\n<li><code>hdfs haadmin -transitionToStandby nn1</code>将nn1切换为Standby</li>\n<li><code>hdfs haadmin -failover nn1 nn2</code>主备切换</li>\n<li><code>yarn rmadmin -getServiceState rm1</code>获取一个resourcemanager节点的HA状态</li>\n<li><code>hdfs --daemon start namenode</code> 单独启动一个namenode进程</li>\n<li><code>hdfs --daemon start zkfc</code> 单独启动一个zkfc进程</li>\n</ul>\n<h2 id=\"测试集群高可用\"><a href=\"#测试集群高可用\" class=\"headerlink\" title=\"测试集群高可用\"></a>测试集群高可用</h2><p>执行<code>hadoop fs -put /etc/profile /profile</code>上传一个文件到hdfs</p>\n<p>执行<code>hadoop fs -ls /</code>可查看hdfs下的文件 ：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507183228.png\" alt=\"\"></p>\n<p>执行<code>hdfs haadmin -getServiceState nn1</code>和<code>`hdfs haadmin -getServiceState nn2</code>命令查master和slave1上NameNode的状态</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191108.png\" alt=\"\"></p>\n<p>在master上执行<code>jps</code>命令查看NameNode进程ID，再执行<code>kill -p &lt;pid of NameNode&gt;</code>关掉Active状态的NameNode，再执行<code>jps</code>会发现没有NameNode了,最后执行<code>hdfs haadmin -getServiceState nn2</code>会发现slave1的NameNode已经是active状态了</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191904.png\" alt=\"\"></p>\n<p>执行<code>hadoop fs -ls /</code>会发现仍然能访问到hdfs的文件</p>\n<p>在master上执行<code>hdfs --daemon start namenode</code>重新启动NameNode，再执行<code>hdfs haadmin -getServiceState nn1</code>会发现master上NameNode的状态变为了standby</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507192833.png\" alt=\"\"></p>\n<p>执行<code>hdfs haadmin -failover nn2 nn1</code>会主备切换，nn1变为active，nn2变为standby.</p>\n<h2 id=\"群起脚本\"><a href=\"#群起脚本\" class=\"headerlink\" title=\"群起脚本\"></a>群起脚本</h2><pre><code class=\"lang-bash\">#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n       echo \"======================== start zookeeper ========================== \"\n    for i in master slave1 slave2\n    do\n           echo \"========== $i zookeeper ==========\"\n           ssh $i \"source /etc/profile;zkServer.sh start\"\n    done\n    echo \"======================== start hdfs ========================== \"\n      ssh master \"source /etc/profile;start-dfs.sh\"\n       echo \"======================== start yarn ========================== \"\n       ssh slave1 \"source /etc/profile;start-yarn.sh\"\n;;\n\"stop\")\n    echo \"======================== stop yarn ========================== \"\n       ssh slave1 \"source /etc/profile;stop-yarn.sh\"\n       echo \"======================== stop hdfs ========================== \"\n      ssh master \"source /etc/profile;stop-dfs.sh\"\n      echo \"======================== stop zookeeper ========================== \"\n    for i in master slave1 slave2\n    do\n           echo \"========== $i zookeeper ==========\"\n           ssh $i \"source /etc/profile;zkServer.sh stop\"\n    done\n;;\n*)\n      echo \"Input Args Error!!!!!\"\n;;\nesac\n</code></pre>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>所谓HA（High Available）是Hadoop2.0中引入来解决Hadoop1.0中单点故障问题的一种机制。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。</p>\n<h2 id=\"HDFS-HA集群配置\"><a href=\"#HDFS-HA集群配置\" class=\"headerlink\" title=\"HDFS-HA集群配置\"></a>HDFS-HA集群配置</h2><h3 id=\"规划集群\"><a href=\"#规划集群\" class=\"headerlink\" title=\"规划集群\"></a>规划集群</h3><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>master</th>\n<th>slave1</th>\n<th>slave2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NameNode (active)</td>\n<td>NameNode (standby)</td>\n<td></td>\n</tr>\n<tr>\n<td>JournalNode</td>\n<td>JournalNode</td>\n<td>JournalNode</td>\n</tr>\n<tr>\n<td>DataNode</td>\n<td>DataNode</td>\n<td>DataNode</td>\n</tr>\n<tr>\n<td>ZK</td>\n<td>ZK</td>\n<td>ZK</td>\n</tr>\n<tr>\n<td></td>\n<td>ResourceManager(active)</td>\n<td>ResourceManager(standby)</td>\n</tr>\n<tr>\n<td>NodeManager</td>\n<td>NodeManager</td>\n<td>NodeManager</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"配置Zookeeper集群\"><a href=\"#配置Zookeeper集群\" class=\"headerlink\" title=\"配置Zookeeper集群\"></a>配置Zookeeper集群</h3><p>在master、slave1和slave2三个节点上部署Zookeeper</p>\n<h3 id=\"安装zookeeper\"><a href=\"#安装zookeeper\" class=\"headerlink\" title=\"安装zookeeper\"></a>安装zookeeper</h3><p>解压zookeeper，并重命名为zookeeper</p>\n<pre><code class=\"lang-bash\">tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/\ncd /usr/local\nmv apache-zookeeper-3.5.7-bin/ zookeeper\n</code></pre>\n<h3 id=\"配置zookeeper\"><a href=\"#配置zookeeper\" class=\"headerlink\" title=\"配置zookeeper\"></a>配置zookeeper</h3><ul>\n<li>在<code>/usr/local/zookeeper/</code>这个目录下创建zkData</li>\n</ul>\n<pre><code class=\"lang-bash\">cd /usr/local/zookeeper\nmkdir zkData\n</code></pre>\n<ul>\n<li>重命名<code>/usr/local/zookeeper/conf</code>这个目录下的zoo_sample.cfg为zoo.cfg</li>\n</ul>\n<pre><code class=\"lang-bash\">cd /usr/local/zookeeper/conf\nmv zoo_sample.cfg zoo.cfg\n</code></pre>\n<ul>\n<li>执行<code>sudo gedit zoo.cfg</code>打开配置文件，配置如下：</li>\n</ul>\n<pre><code class=\"lang-bash\"># 修改原有dataDir值如下\ndataDir=/usr/local/zookeeper/zkData\n#######################cluster##########################\nserver.1=master:2888:3888\nserver.2=slave1:2888:3888\nserver.3=slave2:2888:3888\n</code></pre>\n<p>Server.A=B:C:D。</p>\n<p>A是一个数字，表示这个是第几号服务器；</p>\n<p>B是这个服务器的IP地址；</p>\n<p>C是这个服务器与集群中的Leader服务器交换信息的端口；</p>\n<p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>\n<p>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p>\n<ul>\n<li>在/usr/local/zookeeper/zkData目录下创建并打开文件myid</li>\n</ul>\n<pre><code class=\"lang-bash\">gedit myid\n</code></pre>\n<p>在myid文件中增加数据：1</p>\n<ul>\n<li>拷贝配置好的zookeeper到其他机器上,并分别修改myid文件中内容为2、3</li>\n</ul>\n<pre><code class=\"lang-bash\">scp -r /usr/local/zookeeper/ slave1:/usr/local/\nscp -r /usr/local/zookeeper/ slave2:/usr/local/\n</code></pre>\n<ul>\n<li>配置zookeeper环境变量</li>\n</ul>\n<p>执行<code>sudo gedit /etc/profile</code>打开配置文件，加入下列内容</p>\n<pre><code class=\"lang-bash\">export ZOOKEEPER_HOME=/usr/local/zookeeper\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n</code></pre>\n<p>执行<code>source /etc/profile</code>命令让配置文件生效</p>\n<ul>\n<li>拷贝<code>/etc/profile</code>文件到其他机器</li>\n</ul>\n<pre><code class=\"lang-bash\">scp /etc/profile slave1:/etc\n</code></pre>\n<h3 id=\"启动zookeeper\"><a href=\"#启动zookeeper\" class=\"headerlink\" title=\"启动zookeeper\"></a>启动zookeeper</h3><p>分别启动zookeeper,在三台机器上执行<code>zkServer.sh start</code>命令启动zookeeper,再执行<code>jps</code>命令可以看到有<code>QuorumPeerMain</code></p>\n<h2 id=\"配置HDFS-HA集群\"><a href=\"#配置HDFS-HA集群\" class=\"headerlink\" title=\"配置HDFS-HA集群\"></a>配置HDFS-HA集群</h2><ul>\n<li>配置core-site.xml：</li>\n</ul>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;!-- 把两个NameNode的地址组装成一个集群mycluster --&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://mycluster&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!-- 在网页界面访问数据使用的用户名。默认值是一个不真实存在的用户，此用户权限很小 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n        &lt;value&gt;ubuntu&lt;/value&gt;\n    &lt;/property&gt; \n    &lt;!--Ha功能，需要一组zk地址，用逗号分隔。被ZKFailoverController使用于自动失效备援failover--&gt;\n    &lt;property&gt;\n           &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\n          &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt;\n     &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>配置hdfs-site.xml</li>\n</ul>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;!-- 冗余度 --&gt;\n    &lt;property&gt;\n          &lt;name&gt;dfs.replication&lt;/name&gt;\n          &lt;value&gt;3&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!-- 暂不配置 \n    &lt;property&gt;\n          &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n          &lt;value&gt;slave1:9869&lt;/value&gt;\n    &lt;/property&gt;\n    --&gt;\n    &lt;!-- 完全分布式集群名称 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.nameservices&lt;/name&gt;\n        &lt;value&gt;mycluster&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 集群中NameNode节点都有哪些 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;\n        &lt;value&gt;nn1,nn2&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn1的RPC通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;\n        &lt;value&gt;master:8020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn2的RPC通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;\n        &lt;value&gt;slave1:8020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn1的http通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;\n        &lt;value&gt;master:9870&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- nn2的http通信地址 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;\n        &lt;value&gt;slave1:9870&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\n    &lt;value&gt;qjournal://master:8485;slave1:8485;slave2:8485/mycluster&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\n        &lt;value&gt;sshfence&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\n        &lt;value&gt;/home/ubuntu/.ssh/id_rsa&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 声明journalnode服务器存储目录--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/hadoop/tmp/jn&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 关闭权限检查--&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.permissions.enable&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 是否开启自动故障转移--&gt;\n    &lt;property&gt;\n           &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\n           &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;\n    &lt;property&gt;\n          &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>配置mapred-site.xml</li>\n</ul>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n   &lt;property&gt;\n       &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n       &lt;value&gt;yarn&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>配置yarn-site.xml</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--启用resourcemanager ha--&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--声明两台resourcemanager的地址--&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;\n        &lt;value&gt;cluster-yarn1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;\n        &lt;value&gt;rm1,rm2&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;\n        &lt;value&gt;slave1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;\n        &lt;value&gt;slave2&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--指定zookeeper集群的地址--&gt; \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;\n        &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--启用自动恢复--&gt; \n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; \n    &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;\n&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;\n    &lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>拷贝配置好的Hadoop文件到其他机器上</li>\n</ul>\n<pre><code class=\"lang-bash\">scp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n</code></pre>\n<h2 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h2><h3 id=\"启动zookeeper集群\"><a href=\"#启动zookeeper集群\" class=\"headerlink\" title=\"启动zookeeper集群\"></a>启动zookeeper集群</h3><p>分别在master，slave1，slave2上执行<code>zkServer.sh start</code>命名启动zk</p>\n<h3 id=\"启动journalnode\"><a href=\"#启动journalnode\" class=\"headerlink\" title=\"启动journalnode\"></a>启动journalnode</h3><p>分别在master，slave1，slave2上执行<code>hdfs --daemon start journalnode</code>命令启动JN</p>\n<p>运行jps命令检验，master，slave1，slave2上多了JournalNode进程</p>\n<h3 id=\"格式化namenode，并启动\"><a href=\"#格式化namenode，并启动\" class=\"headerlink\" title=\"格式化namenode，并启动\"></a>格式化namenode，并启动</h3><p>在master上执行命令<code>hdfs namenode -format</code>格式化namenode</p>\n<p>在master上执行命令<code>hdfs --daemon start namenode</code>命令格式化namenode</p>\n<h3 id=\"副节点同步主节点格式化\"><a href=\"#副节点同步主节点格式化\" class=\"headerlink\" title=\"副节点同步主节点格式化\"></a>副节点同步主节点格式化</h3><p>在slave1上执行命令<code>hdfs namenode -bootstrapStandby</code>命令同步主节点格式化</p>\n<h3 id=\"格式化ZKFC\"><a href=\"#格式化ZKFC\" class=\"headerlink\" title=\"格式化ZKFC\"></a>格式化ZKFC</h3><p>在master上执行命令<code>hdfs zkfc -formatZK</code>格式化ZKFC，第一次启动时需要格式化</p>\n<h3 id=\"启动HDFS\"><a href=\"#启动HDFS\" class=\"headerlink\" title=\"启动HDFS\"></a>启动HDFS</h3><p>在master上执行命令<code>start-dfs.sh</code>启动HDFS</p>\n<h3 id=\"启动YARN\"><a href=\"#启动YARN\" class=\"headerlink\" title=\"启动YARN\"></a>启动YARN</h3><p>在slave1上执行<code>start-yarn.sh</code>命令启动YARN，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动</p>\n<h2 id=\"测试集群工作状态的一些指令\"><a href=\"#测试集群工作状态的一些指令\" class=\"headerlink\" title=\"测试集群工作状态的一些指令\"></a>测试集群工作状态的一些指令</h2><ul>\n<li><p><code>hdfs dfsadmin -report</code> 查看hdfs的各节点状态信息</p>\n</li>\n<li><p><code>hdfs haadmin -getServiceState nn1</code> 获取一个namenode节点的HA状态</p>\n</li>\n<li><code>hdfs haadmin -transitionToActive nn1</code>将nn1切换为Active</li>\n<li><code>hdfs haadmin -transitionToStandby nn1</code>将nn1切换为Standby</li>\n<li><code>hdfs haadmin -failover nn1 nn2</code>主备切换</li>\n<li><code>yarn rmadmin -getServiceState rm1</code>获取一个resourcemanager节点的HA状态</li>\n<li><code>hdfs --daemon start namenode</code> 单独启动一个namenode进程</li>\n<li><code>hdfs --daemon start zkfc</code> 单独启动一个zkfc进程</li>\n</ul>\n<h2 id=\"测试集群高可用\"><a href=\"#测试集群高可用\" class=\"headerlink\" title=\"测试集群高可用\"></a>测试集群高可用</h2><p>执行<code>hadoop fs -put /etc/profile /profile</code>上传一个文件到hdfs</p>\n<p>执行<code>hadoop fs -ls /</code>可查看hdfs下的文件 ：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507183228.png\" alt=\"\"></p>\n<p>执行<code>hdfs haadmin -getServiceState nn1</code>和<code>`hdfs haadmin -getServiceState nn2</code>命令查master和slave1上NameNode的状态</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191108.png\" alt=\"\"></p>\n<p>在master上执行<code>jps</code>命令查看NameNode进程ID，再执行<code>kill -p &lt;pid of NameNode&gt;</code>关掉Active状态的NameNode，再执行<code>jps</code>会发现没有NameNode了,最后执行<code>hdfs haadmin -getServiceState nn2</code>会发现slave1的NameNode已经是active状态了</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507191904.png\" alt=\"\"></p>\n<p>执行<code>hadoop fs -ls /</code>会发现仍然能访问到hdfs的文件</p>\n<p>在master上执行<code>hdfs --daemon start namenode</code>重新启动NameNode，再执行<code>hdfs haadmin -getServiceState nn1</code>会发现master上NameNode的状态变为了standby</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200507192833.png\" alt=\"\"></p>\n<p>执行<code>hdfs haadmin -failover nn2 nn1</code>会主备切换，nn1变为active，nn2变为standby.</p>\n<h2 id=\"群起脚本\"><a href=\"#群起脚本\" class=\"headerlink\" title=\"群起脚本\"></a>群起脚本</h2><pre><code class=\"lang-bash\">#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo &quot;No Args Input Error!!!!!&quot;\n   exit\nfi\ncase $1 in \n&quot;start&quot;)\n       echo &quot;======================== start zookeeper ========================== &quot;\n    for i in master slave1 slave2\n    do\n           echo &quot;========== $i zookeeper ==========&quot;\n           ssh $i &quot;source /etc/profile;zkServer.sh start&quot;\n    done\n    echo &quot;======================== start hdfs ========================== &quot;\n      ssh master &quot;source /etc/profile;start-dfs.sh&quot;\n       echo &quot;======================== start yarn ========================== &quot;\n       ssh slave1 &quot;source /etc/profile;start-yarn.sh&quot;\n;;\n&quot;stop&quot;)\n    echo &quot;======================== stop yarn ========================== &quot;\n       ssh slave1 &quot;source /etc/profile;stop-yarn.sh&quot;\n       echo &quot;======================== stop hdfs ========================== &quot;\n      ssh master &quot;source /etc/profile;stop-dfs.sh&quot;\n      echo &quot;======================== stop zookeeper ========================== &quot;\n    for i in master slave1 slave2\n    do\n           echo &quot;========== $i zookeeper ==========&quot;\n           ssh $i &quot;source /etc/profile;zkServer.sh stop&quot;\n    done\n;;\n*)\n      echo &quot;Input Args Error!!!!!&quot;\n;;\nesac\n</code></pre>\n"},{"title":"RNN 架构梳理","top":false,"cover":false,"toc":true,"mathjax":true,"reprintPolicy":"cc_by","date":"2020-05-19T05:02:57.000Z","author":null,"password":null,"img":null,"summary":"梳理基于RNN的文本分类网络架构","keywords":"RNN 架构梳理 文本分类 深度学习","_content":"## RNN架构梳理\n\n* vanilla RNN\n* LSTM\n* GRU\n* Tree-LSTM\n* chain-structured LSTM（Long short-term memory over recursive structures）\n* Graph-LSTM\n* Long short-term memory-networks for machine reading\n* MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）\n* TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）","source":"_posts/RNN-架构梳理.md","raw":"---\ntitle: RNN 架构梳理\ntop: false\ncover: false\ntoc: true\nmathjax: true\nreprintPolicy: cc_by\ndate: 2020-05-19 13:02:57\nauthor:\npassword:\nimg:\nsummary: 梳理基于RNN的文本分类网络架构\ncategories: RNN\nkeywords: RNN 架构梳理 文本分类 深度学习\ntags:\n\t- RNN\n\t- 架构梳理\n\t- 文本分类\n\t- 深度学习\n---\n## RNN架构梳理\n\n* vanilla RNN\n* LSTM\n* GRU\n* Tree-LSTM\n* chain-structured LSTM（Long short-term memory over recursive structures）\n* Graph-LSTM\n* Long short-term memory-networks for machine reading\n* MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）\n* TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）","slug":"RNN-架构梳理","published":1,"updated":"2020-08-20T08:41:44.641Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0m990006ewse38npgmlw","content":"<h2 id=\"RNN架构梳理\"><a href=\"#RNN架构梳理\" class=\"headerlink\" title=\"RNN架构梳理\"></a>RNN架构梳理</h2><ul>\n<li>vanilla RNN</li>\n<li>LSTM</li>\n<li>GRU</li>\n<li>Tree-LSTM</li>\n<li>chain-structured LSTM（Long short-term memory over recursive structures）</li>\n<li>Graph-LSTM</li>\n<li>Long short-term memory-networks for machine reading</li>\n<li>MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）</li>\n<li>TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）</li>\n</ul>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h2 id=\"RNN架构梳理\"><a href=\"#RNN架构梳理\" class=\"headerlink\" title=\"RNN架构梳理\"></a>RNN架构梳理</h2><ul>\n<li>vanilla RNN</li>\n<li>LSTM</li>\n<li>GRU</li>\n<li>Tree-LSTM</li>\n<li>chain-structured LSTM（Long short-term memory over recursive structures）</li>\n<li>Graph-LSTM</li>\n<li>Long short-term memory-networks for machine reading</li>\n<li>MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）</li>\n<li>TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）</li>\n</ul>\n"},{"title":"Todo-List","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-04-30T06:46:32.000Z","password":null,"summary":null,"img":null,"keywords":"todo list","_content":"\n* 前馈网络架构梳理\n\t* MLP（Multi-Layer Perceptrons）\n\t* DAN（Deep unordered composition rivals syntactic methods for text classification）\n\t* fastText（Fasttext. zip: Compressing text classification models）\n\t* doc2vec（Distributed representations of sentences and documents）\n* RNN架构梳理\n\t* vanilla RNN\n\t* LSTM\n\t* GRU\n\t* Tree-LSTM\n\t* chain-structured LSTM（Long short-term memory over recursive structures）\n\t* Graph-LSTM\n\t* Long short-term memory-networks for machine reading\n\t* MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）\n\t* TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）\n* CNN架构梳理\n\t* TextCNN（Convolutional neural networks for sentence classification）\n\t* Character-level CNNs（Character-level convolutional networks for text classification）\n\t* VDCNN（Very deep convolutional networks for text classification）\n\t* TreeCNN（Natural language inference by tree-based convolution and heuristic matching）\n\t* GCN\n* 胶囊网络\n\t* Investigating capsule networks with dynamic routing for text classification\n\t* Investigating the transferring capability of capsule networks for text classification\n\t* Towards scalable and reliable capsule networks for challenging NLP applications\n\t* Text classification using capsules\n\t* Stacked Capsule Autoencoders（升级版胶囊网络）\n* 基于注意力机制的模型\n\t* \n\n* 自监督学习 解耦表示 半监督学习","source":"_posts/Todo-List.md","raw":"---\ntitle: Todo-List\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-04-30 14:46:32\npassword:\nsummary:\ncategories: 待完成\nimg:\nkeywords: todo list\ntags: \n\t- todo list\n---\n\n* 前馈网络架构梳理\n\t* MLP（Multi-Layer Perceptrons）\n\t* DAN（Deep unordered composition rivals syntactic methods for text classification）\n\t* fastText（Fasttext. zip: Compressing text classification models）\n\t* doc2vec（Distributed representations of sentences and documents）\n* RNN架构梳理\n\t* vanilla RNN\n\t* LSTM\n\t* GRU\n\t* Tree-LSTM\n\t* chain-structured LSTM（Long short-term memory over recursive structures）\n\t* Graph-LSTM\n\t* Long short-term memory-networks for machine reading\n\t* MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）\n\t* TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）\n* CNN架构梳理\n\t* TextCNN（Convolutional neural networks for sentence classification）\n\t* Character-level CNNs（Character-level convolutional networks for text classification）\n\t* VDCNN（Very deep convolutional networks for text classification）\n\t* TreeCNN（Natural language inference by tree-based convolution and heuristic matching）\n\t* GCN\n* 胶囊网络\n\t* Investigating capsule networks with dynamic routing for text classification\n\t* Investigating the transferring capability of capsule networks for text classification\n\t* Towards scalable and reliable capsule networks for challenging NLP applications\n\t* Text classification using capsules\n\t* Stacked Capsule Autoencoders（升级版胶囊网络）\n* 基于注意力机制的模型\n\t* \n\n* 自监督学习 解耦表示 半监督学习","slug":"Todo-List","published":1,"updated":"2020-08-20T08:41:44.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0m9o0008ewseg0f501qt","content":"<ul>\n<li>前馈网络架构梳理<ul>\n<li>MLP（Multi-Layer Perceptrons）</li>\n<li>DAN（Deep unordered composition rivals syntactic methods for text classification）</li>\n<li>fastText（Fasttext. zip: Compressing text classification models）</li>\n<li>doc2vec（Distributed representations of sentences and documents）</li>\n</ul>\n</li>\n<li>RNN架构梳理<ul>\n<li>vanilla RNN</li>\n<li>LSTM</li>\n<li>GRU</li>\n<li>Tree-LSTM</li>\n<li>chain-structured LSTM（Long short-term memory over recursive structures）</li>\n<li>Graph-LSTM</li>\n<li>Long short-term memory-networks for machine reading</li>\n<li>MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）</li>\n<li>TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）</li>\n</ul>\n</li>\n<li>CNN架构梳理<ul>\n<li>TextCNN（Convolutional neural networks for sentence classification）</li>\n<li>Character-level CNNs（Character-level convolutional networks for text classification）</li>\n<li>VDCNN（Very deep convolutional networks for text classification）</li>\n<li>TreeCNN（Natural language inference by tree-based convolution and heuristic matching）</li>\n<li>GCN</li>\n</ul>\n</li>\n<li>胶囊网络<ul>\n<li>Investigating capsule networks with dynamic routing for text classification</li>\n<li>Investigating the transferring capability of capsule networks for text classification</li>\n<li>Towards scalable and reliable capsule networks for challenging NLP applications</li>\n<li>Text classification using capsules</li>\n<li>Stacked Capsule Autoencoders（升级版胶囊网络）</li>\n</ul>\n</li>\n<li><p>基于注意力机制的模型</p>\n<ul>\n<li></li>\n</ul>\n</li>\n<li><p>自监督学习 解耦表示 半监督学习</p>\n</li>\n</ul>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<ul>\n<li>前馈网络架构梳理<ul>\n<li>MLP（Multi-Layer Perceptrons）</li>\n<li>DAN（Deep unordered composition rivals syntactic methods for text classification）</li>\n<li>fastText（Fasttext. zip: Compressing text classification models）</li>\n<li>doc2vec（Distributed representations of sentences and documents）</li>\n</ul>\n</li>\n<li>RNN架构梳理<ul>\n<li>vanilla RNN</li>\n<li>LSTM</li>\n<li>GRU</li>\n<li>Tree-LSTM</li>\n<li>chain-structured LSTM（Long short-term memory over recursive structures）</li>\n<li>Graph-LSTM</li>\n<li>Long short-term memory-networks for machine reading</li>\n<li>MT-LSTM（Multi-timescale long short-term memory neural network for modelling sentences and documents）</li>\n<li>TopicRNN（Topicrnn: A recurrent neural network with long-range semantic dependency）</li>\n</ul>\n</li>\n<li>CNN架构梳理<ul>\n<li>TextCNN（Convolutional neural networks for sentence classification）</li>\n<li>Character-level CNNs（Character-level convolutional networks for text classification）</li>\n<li>VDCNN（Very deep convolutional networks for text classification）</li>\n<li>TreeCNN（Natural language inference by tree-based convolution and heuristic matching）</li>\n<li>GCN</li>\n</ul>\n</li>\n<li>胶囊网络<ul>\n<li>Investigating capsule networks with dynamic routing for text classification</li>\n<li>Investigating the transferring capability of capsule networks for text classification</li>\n<li>Towards scalable and reliable capsule networks for challenging NLP applications</li>\n<li>Text classification using capsules</li>\n<li>Stacked Capsule Autoencoders（升级版胶囊网络）</li>\n</ul>\n</li>\n<li><p>基于注意力机制的模型</p>\n<ul>\n<li></li>\n</ul>\n</li>\n<li><p>自监督学习 解耦表示 半监督学习</p>\n</li>\n</ul>\n"},{"title":"Zookeeper+Hadoop+HBase搭建","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-05-07T13:54:26.000Z","password":null,"summary":null,"img":null,"keywords":"hadoop zookeeper hbase 大数据","_content":"\n[Hadoop搭建](https://zhishuang.tk/2020/04/30/ubuntu1604-da-jian-hadoop-ji-qun/)\n\n[Zookeeper+Hadoopda搭建（Hadoop HA）](https://zhishuang.tk/2020/05/06/hadoop-ha-da-jian/)\n\n这儿实现在Hadoop HA的基础上搭建Hbase\n\n## 安装Hbase\n\n将安装文件`hbase-2.2.4-bin.tar.gz`到`/usr/local`并重命名为`hbase`\n\n```bash\ntar -zxvf hbase-2.2.4-bin.tar.gz -C /usr/local\ncd /usr/local\nmv -r hbase-2.2.4 hbase\n```\n\n## 配置Hbase\n\n**环境变量**，执行`sudo gedit /etc/profile`打开配置文件，添加如下内容\n\n```bash\n#set hbase env\nexport HBASE_HOME=/usr/local/hbase\nexport PATH=$HBASE_HOME/bin:$PATH\n```\n\n记得所有主机都要配置，执行`source /etc/profile`使配置生效\n\n**hbase-env.sh**\n\n```bash\nexport JAVA_HOME=/usr/java/jdk1.8.0_251\nexport HADOOP_HOME=/usr/local/hadoop\nexport HBASE_HOME=/usr/local/hbase\n#关闭自身zookeeper，采用外部的zookeeper\nexport HBASE_MANAGES_ZK=false\n```\n\n**hbase-site.xml**\n\n```xml\n<configuration>\n\t<!-- hadoop集群名称 -->\n    <property>\n        <name>hbase.rootdir</name>\n        <value>hdfs://mycluster/hbase</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.quorum</name>\n        <value>master,slave1,slave2</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.property.clientPort</name>\n        <value>2181</value>\n    </property>\n    <!--  是否是完全分布式 -->\n    <property>\n        <name>hbase.cluster.distributed</name>\n        <value>true</value>\n    </property>\n    <!--  完全分布式式必须为false  -->\n    <property>\n        <name>hbase.unsafe.stream.capability.enforce</name>\n        <value>false</value>\n    </property>\n    <!--  指定缓存文件存储的路径 -->\n    <property>\n        <name>hbase.tmp.dir</name>\n        <value>/usr/local/hadoop/tmp</value>\n    </property>\n    <!--  指定Zookeeper数据存储的路径  -->\n    <property>\n    \t<name>hbase.zookeeper.property.dataDir</name>\n    \t<value>/usr/local/zookeeper/zkData</value>\n    </property>\n</configuration>\n```\n\n**regionservers**\n\n```bash\nmaster\nslave1\nslave2\n```\n\n**配置Hmaster高可用**\n\n为了保证HBase集群的高可靠性，HBase支持多Backup Master 设置。当Active Master挂掉后，Backup Master可以自动接管整个HBase的集群。该配置极其简单：在 $HBASE_HOME/conf/目录下新增文件配置backup-masters，在其内添加要用做Backup Master的节点hostname。\n\t\t执行`gedit /usr/local/hbase/conf/backup-masters`新建并打开文件，添加`slave2`\n\t\t没设置backup-masters之前启动hbase， 只有一台有启动了HMaster进程，设置之后，重新启动整个集群，我们会发现，在backup-masters清单上的主机，都启动了HMaster进程\n\n**分发hbase给其他主机**\n\n```bash\nscp -r /usr/local/hbase/ slave1:/usr/local/\nscp -r /usr/local/hbase/ slave1:/usr/local/\n```\n\n## 时间同步\n\n执行`sudo apt-get install ntp`安装ntp\n\n配置ntp，执行`gedit /etc/ntp.conf`打开配置文件\n\n* master端配置\n\n```bash\n# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\npool 0.ubuntu.pool.ntp.org iburst\npool 1.ubuntu.pool.ntp.org iburst\npool 2.ubuntu.pool.ntp.org iburst\npool 3.ubuntu.pool.ntp.org iburst\n\n# Use Ubuntu's ntp server as a fallback.\npool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\nrestrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n```\n\n* slave端配置\n\n```bash\n# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\n# pool 0.ubuntu.pool.ntp.org iburst\n# pool 1.ubuntu.pool.ntp.org iburst\n# pool 2.ubuntu.pool.ntp.org iburst\n# pool 3.ubuntu.pool.ntp.org iburst\nserver 192.168.79.129\n# Use Ubuntu's ntp server as a fallback.\n# pool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\n# restrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n```\n\n\n\n查看ntp的时间服务是否启动：`ps -aux | grep ntp`\n\n执行 `service ntp restart`，重启ntp服务\n\n执行`ntpq -p`查看配置\n\n这个命令可以列出目前我们的 NTP 与相关的上层 NTP 的状态，上头的几个字段的意义为：\n\n* remote: 它指的就是本地机器所连接的远程NTP服务器；\n* refid: 它指的是给远程服务器提供时间同步的服务器；\n* st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端。所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的；\n* when: 几秒钟前曾经做过时间同步化更新的动作；\n* poll: 本地机和远程服务器多少时间进行一次同步(单位为秒).在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小；\n* reach: 已经向上层 NTP 服务器要求更新的次数；\n* delay: 网络传输过程当中延迟的时间，单位为 10^(-6) 秒；\n* offset: 时间补偿的结果，单位与 10^(-3) 秒；\n* jitter: Linux 系统时间与 BIOS 硬件时间的差异时间， 单为 10^(-6) 秒。简单地说这个数值的绝对值越小我们和服务器的时间就越精确；\n* *: 它告诉我们远端的服务器已经被确认为我们的主NTP Server,我们系统的时间将由这台机器所提供；\n* +: 它将作为辅助的NTP Server和带有号的服务器一起为我们提供同步服务. 当号服务器不可用时它就可以接管；\n* -: 远程服务器被clustering algorithm认为是不合格的NTP Server；\n* x: 远程服务器不可用\n\n## 启动HBase\n\n首先启动Hadoop-HA集群，再执行`start-hbase.start`启动HBase，执行jps命令，可以看到：\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141401.png)\n\n从`http://master:16010/`可查看HBase集群信息\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141902.png)\n\n## 群起脚本\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   \techo \"======================== start zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh start\"\n\tdone\n\techo \"======================== start hdfs ========================== \"\n  \tssh master \"source /etc/profile;start-dfs.sh\"\n   \techo \"======================== start yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;start-yarn.sh\"\n   \techo \"======================== start hbase ========================== \"\n   \tssh master \"source /etc/profile;start-hbase.sh\"\n;;\n\"stop\")\n\techo \"======================== stop hbase ========================== \"\n  \tssh master \"source /etc/profile;stop-hbase.sh\"\n\techo \"======================== stop yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;stop-yarn.sh\"\n   \techo \"======================== stop hdfs ========================== \"\n  \tssh master \"source /etc/profile;stop-dfs.sh\"\n  \techo \"======================== stop zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh stop\"\n\tdone\n;;\n*)\n  \techo \"Input Args Error!!!!!\"\n;;\nesac\n```\n\n","source":"_posts/Zookeeper-Hadoop-HBase搭建.md","raw":"---\ntitle: Zookeeper+Hadoop+HBase搭建\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-05-07 21:54:26\npassword:\nsummary:\ncategories: 大数据\nimg:\nkeywords: hadoop zookeeper hbase 大数据\ntags:\n\t- hadoop\n\t- zookeeper\n\t- hbase\n\t- 大数据\n---\n\n[Hadoop搭建](https://zhishuang.tk/2020/04/30/ubuntu1604-da-jian-hadoop-ji-qun/)\n\n[Zookeeper+Hadoopda搭建（Hadoop HA）](https://zhishuang.tk/2020/05/06/hadoop-ha-da-jian/)\n\n这儿实现在Hadoop HA的基础上搭建Hbase\n\n## 安装Hbase\n\n将安装文件`hbase-2.2.4-bin.tar.gz`到`/usr/local`并重命名为`hbase`\n\n```bash\ntar -zxvf hbase-2.2.4-bin.tar.gz -C /usr/local\ncd /usr/local\nmv -r hbase-2.2.4 hbase\n```\n\n## 配置Hbase\n\n**环境变量**，执行`sudo gedit /etc/profile`打开配置文件，添加如下内容\n\n```bash\n#set hbase env\nexport HBASE_HOME=/usr/local/hbase\nexport PATH=$HBASE_HOME/bin:$PATH\n```\n\n记得所有主机都要配置，执行`source /etc/profile`使配置生效\n\n**hbase-env.sh**\n\n```bash\nexport JAVA_HOME=/usr/java/jdk1.8.0_251\nexport HADOOP_HOME=/usr/local/hadoop\nexport HBASE_HOME=/usr/local/hbase\n#关闭自身zookeeper，采用外部的zookeeper\nexport HBASE_MANAGES_ZK=false\n```\n\n**hbase-site.xml**\n\n```xml\n<configuration>\n\t<!-- hadoop集群名称 -->\n    <property>\n        <name>hbase.rootdir</name>\n        <value>hdfs://mycluster/hbase</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.quorum</name>\n        <value>master,slave1,slave2</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.property.clientPort</name>\n        <value>2181</value>\n    </property>\n    <!--  是否是完全分布式 -->\n    <property>\n        <name>hbase.cluster.distributed</name>\n        <value>true</value>\n    </property>\n    <!--  完全分布式式必须为false  -->\n    <property>\n        <name>hbase.unsafe.stream.capability.enforce</name>\n        <value>false</value>\n    </property>\n    <!--  指定缓存文件存储的路径 -->\n    <property>\n        <name>hbase.tmp.dir</name>\n        <value>/usr/local/hadoop/tmp</value>\n    </property>\n    <!--  指定Zookeeper数据存储的路径  -->\n    <property>\n    \t<name>hbase.zookeeper.property.dataDir</name>\n    \t<value>/usr/local/zookeeper/zkData</value>\n    </property>\n</configuration>\n```\n\n**regionservers**\n\n```bash\nmaster\nslave1\nslave2\n```\n\n**配置Hmaster高可用**\n\n为了保证HBase集群的高可靠性，HBase支持多Backup Master 设置。当Active Master挂掉后，Backup Master可以自动接管整个HBase的集群。该配置极其简单：在 $HBASE_HOME/conf/目录下新增文件配置backup-masters，在其内添加要用做Backup Master的节点hostname。\n\t\t执行`gedit /usr/local/hbase/conf/backup-masters`新建并打开文件，添加`slave2`\n\t\t没设置backup-masters之前启动hbase， 只有一台有启动了HMaster进程，设置之后，重新启动整个集群，我们会发现，在backup-masters清单上的主机，都启动了HMaster进程\n\n**分发hbase给其他主机**\n\n```bash\nscp -r /usr/local/hbase/ slave1:/usr/local/\nscp -r /usr/local/hbase/ slave1:/usr/local/\n```\n\n## 时间同步\n\n执行`sudo apt-get install ntp`安装ntp\n\n配置ntp，执行`gedit /etc/ntp.conf`打开配置文件\n\n* master端配置\n\n```bash\n# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\npool 0.ubuntu.pool.ntp.org iburst\npool 1.ubuntu.pool.ntp.org iburst\npool 2.ubuntu.pool.ntp.org iburst\npool 3.ubuntu.pool.ntp.org iburst\n\n# Use Ubuntu's ntp server as a fallback.\npool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\nrestrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n```\n\n* slave端配置\n\n```bash\n# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\n# pool 0.ubuntu.pool.ntp.org iburst\n# pool 1.ubuntu.pool.ntp.org iburst\n# pool 2.ubuntu.pool.ntp.org iburst\n# pool 3.ubuntu.pool.ntp.org iburst\nserver 192.168.79.129\n# Use Ubuntu's ntp server as a fallback.\n# pool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\n# restrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n```\n\n\n\n查看ntp的时间服务是否启动：`ps -aux | grep ntp`\n\n执行 `service ntp restart`，重启ntp服务\n\n执行`ntpq -p`查看配置\n\n这个命令可以列出目前我们的 NTP 与相关的上层 NTP 的状态，上头的几个字段的意义为：\n\n* remote: 它指的就是本地机器所连接的远程NTP服务器；\n* refid: 它指的是给远程服务器提供时间同步的服务器；\n* st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端。所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的；\n* when: 几秒钟前曾经做过时间同步化更新的动作；\n* poll: 本地机和远程服务器多少时间进行一次同步(单位为秒).在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小；\n* reach: 已经向上层 NTP 服务器要求更新的次数；\n* delay: 网络传输过程当中延迟的时间，单位为 10^(-6) 秒；\n* offset: 时间补偿的结果，单位与 10^(-3) 秒；\n* jitter: Linux 系统时间与 BIOS 硬件时间的差异时间， 单为 10^(-6) 秒。简单地说这个数值的绝对值越小我们和服务器的时间就越精确；\n* *: 它告诉我们远端的服务器已经被确认为我们的主NTP Server,我们系统的时间将由这台机器所提供；\n* +: 它将作为辅助的NTP Server和带有号的服务器一起为我们提供同步服务. 当号服务器不可用时它就可以接管；\n* -: 远程服务器被clustering algorithm认为是不合格的NTP Server；\n* x: 远程服务器不可用\n\n## 启动HBase\n\n首先启动Hadoop-HA集群，再执行`start-hbase.start`启动HBase，执行jps命令，可以看到：\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141401.png)\n\n从`http://master:16010/`可查看HBase集群信息\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141902.png)\n\n## 群起脚本\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   \techo \"======================== start zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh start\"\n\tdone\n\techo \"======================== start hdfs ========================== \"\n  \tssh master \"source /etc/profile;start-dfs.sh\"\n   \techo \"======================== start yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;start-yarn.sh\"\n   \techo \"======================== start hbase ========================== \"\n   \tssh master \"source /etc/profile;start-hbase.sh\"\n;;\n\"stop\")\n\techo \"======================== stop hbase ========================== \"\n  \tssh master \"source /etc/profile;stop-hbase.sh\"\n\techo \"======================== stop yarn ========================== \"\n   \tssh slave1 \"source /etc/profile;stop-yarn.sh\"\n   \techo \"======================== stop hdfs ========================== \"\n  \tssh master \"source /etc/profile;stop-dfs.sh\"\n  \techo \"======================== stop zookeeper ========================== \"\n\tfor i in master slave1 slave2\n\tdo\n   \t\techo \"========== $i zookeeper ==========\"\n   \t\tssh $i \"source /etc/profile;zkServer.sh stop\"\n\tdone\n;;\n*)\n  \techo \"Input Args Error!!!!!\"\n;;\nesac\n```\n\n","slug":"Zookeeper-Hadoop-HBase搭建","published":1,"updated":"2020-08-20T08:41:44.645Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0m9u000aewse7l1a1qls","content":"<p><a href=\"https://zhishuang.tk/2020/04/30/ubuntu1604-da-jian-hadoop-ji-qun/\">Hadoop搭建</a></p>\n<p><a href=\"https://zhishuang.tk/2020/05/06/hadoop-ha-da-jian/\">Zookeeper+Hadoopda搭建（Hadoop HA）</a></p>\n<p>这儿实现在Hadoop HA的基础上搭建Hbase</p>\n<h2 id=\"安装Hbase\"><a href=\"#安装Hbase\" class=\"headerlink\" title=\"安装Hbase\"></a>安装Hbase</h2><p>将安装文件<code>hbase-2.2.4-bin.tar.gz</code>到<code>/usr/local</code>并重命名为<code>hbase</code></p>\n<pre><code class=\"lang-bash\">tar -zxvf hbase-2.2.4-bin.tar.gz -C /usr/local\ncd /usr/local\nmv -r hbase-2.2.4 hbase\n</code></pre>\n<h2 id=\"配置Hbase\"><a href=\"#配置Hbase\" class=\"headerlink\" title=\"配置Hbase\"></a>配置Hbase</h2><p><strong>环境变量</strong>，执行<code>sudo gedit /etc/profile</code>打开配置文件，添加如下内容</p>\n<pre><code class=\"lang-bash\">#set hbase env\nexport HBASE_HOME=/usr/local/hbase\nexport PATH=$HBASE_HOME/bin:$PATH\n</code></pre>\n<p>记得所有主机都要配置，执行<code>source /etc/profile</code>使配置生效</p>\n<p><strong>hbase-env.sh</strong></p>\n<pre><code class=\"lang-bash\">export JAVA_HOME=/usr/java/jdk1.8.0_251\nexport HADOOP_HOME=/usr/local/hadoop\nexport HBASE_HOME=/usr/local/hbase\n#关闭自身zookeeper，采用外部的zookeeper\nexport HBASE_MANAGES_ZK=false\n</code></pre>\n<p><strong>hbase-site.xml</strong></p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;!-- hadoop集群名称 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.rootdir&lt;/name&gt;\n        &lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;\n        &lt;value&gt;master,slave1,slave2&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;\n        &lt;value&gt;2181&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  是否是完全分布式 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  完全分布式式必须为false  --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  指定缓存文件存储的路径 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  指定Zookeeper数据存储的路径  --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n        &lt;value&gt;/usr/local/zookeeper/zkData&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p><strong>regionservers</strong></p>\n<pre><code class=\"lang-bash\">master\nslave1\nslave2\n</code></pre>\n<p><strong>配置Hmaster高可用</strong></p>\n<p>为了保证HBase集群的高可靠性，HBase支持多Backup Master 设置。当Active Master挂掉后，Backup Master可以自动接管整个HBase的集群。该配置极其简单：在 $HBASE_HOME/conf/目录下新增文件配置backup-masters，在其内添加要用做Backup Master的节点hostname。<br>        执行<code>gedit /usr/local/hbase/conf/backup-masters</code>新建并打开文件，添加<code>slave2</code><br>        没设置backup-masters之前启动hbase， 只有一台有启动了HMaster进程，设置之后，重新启动整个集群，我们会发现，在backup-masters清单上的主机，都启动了HMaster进程</p>\n<p><strong>分发hbase给其他主机</strong></p>\n<pre><code class=\"lang-bash\">scp -r /usr/local/hbase/ slave1:/usr/local/\nscp -r /usr/local/hbase/ slave1:/usr/local/\n</code></pre>\n<h2 id=\"时间同步\"><a href=\"#时间同步\" class=\"headerlink\" title=\"时间同步\"></a>时间同步</h2><p>执行<code>sudo apt-get install ntp</code>安装ntp</p>\n<p>配置ntp，执行<code>gedit /etc/ntp.conf</code>打开配置文件</p>\n<ul>\n<li>master端配置</li>\n</ul>\n<pre><code class=\"lang-bash\"># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\npool 0.ubuntu.pool.ntp.org iburst\npool 1.ubuntu.pool.ntp.org iburst\npool 2.ubuntu.pool.ntp.org iburst\npool 3.ubuntu.pool.ntp.org iburst\n\n# Use Ubuntu's ntp server as a fallback.\npool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\nrestrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n</code></pre>\n<ul>\n<li>slave端配置</li>\n</ul>\n<pre><code class=\"lang-bash\"># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\n# pool 0.ubuntu.pool.ntp.org iburst\n# pool 1.ubuntu.pool.ntp.org iburst\n# pool 2.ubuntu.pool.ntp.org iburst\n# pool 3.ubuntu.pool.ntp.org iburst\nserver 192.168.79.129\n# Use Ubuntu's ntp server as a fallback.\n# pool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\n# restrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n</code></pre>\n<p>查看ntp的时间服务是否启动：<code>ps -aux | grep ntp</code></p>\n<p>执行 <code>service ntp restart</code>，重启ntp服务</p>\n<p>执行<code>ntpq -p</code>查看配置</p>\n<p>这个命令可以列出目前我们的 NTP 与相关的上层 NTP 的状态，上头的几个字段的意义为：</p>\n<ul>\n<li>remote: 它指的就是本地机器所连接的远程NTP服务器；</li>\n<li>refid: 它指的是给远程服务器提供时间同步的服务器；</li>\n<li>st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端。所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的；</li>\n<li>when: 几秒钟前曾经做过时间同步化更新的动作；</li>\n<li>poll: 本地机和远程服务器多少时间进行一次同步(单位为秒).在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小；</li>\n<li>reach: 已经向上层 NTP 服务器要求更新的次数；</li>\n<li>delay: 网络传输过程当中延迟的时间，单位为 10^(-6) 秒；</li>\n<li>offset: 时间补偿的结果，单位与 10^(-3) 秒；</li>\n<li>jitter: Linux 系统时间与 BIOS 硬件时间的差异时间， 单为 10^(-6) 秒。简单地说这个数值的绝对值越小我们和服务器的时间就越精确；</li>\n<li>*: 它告诉我们远端的服务器已经被确认为我们的主NTP Server,我们系统的时间将由这台机器所提供；</li>\n<li>+: 它将作为辅助的NTP Server和带有号的服务器一起为我们提供同步服务. 当号服务器不可用时它就可以接管；</li>\n<li>-: 远程服务器被clustering algorithm认为是不合格的NTP Server；</li>\n<li>x: 远程服务器不可用</li>\n</ul>\n<h2 id=\"启动HBase\"><a href=\"#启动HBase\" class=\"headerlink\" title=\"启动HBase\"></a>启动HBase</h2><p>首先启动Hadoop-HA集群，再执行<code>start-hbase.start</code>启动HBase，执行jps命令，可以看到：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141401.png\" alt=\"\"></p>\n<p>从<code>http://master:16010/</code>可查看HBase集群信息</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141902.png\" alt=\"\"></p>\n<h2 id=\"群起脚本\"><a href=\"#群起脚本\" class=\"headerlink\" title=\"群起脚本\"></a>群起脚本</h2><pre><code class=\"lang-bash\">#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n       echo \"======================== start zookeeper ========================== \"\n    for i in master slave1 slave2\n    do\n           echo \"========== $i zookeeper ==========\"\n           ssh $i \"source /etc/profile;zkServer.sh start\"\n    done\n    echo \"======================== start hdfs ========================== \"\n      ssh master \"source /etc/profile;start-dfs.sh\"\n       echo \"======================== start yarn ========================== \"\n       ssh slave1 \"source /etc/profile;start-yarn.sh\"\n       echo \"======================== start hbase ========================== \"\n       ssh master \"source /etc/profile;start-hbase.sh\"\n;;\n\"stop\")\n    echo \"======================== stop hbase ========================== \"\n      ssh master \"source /etc/profile;stop-hbase.sh\"\n    echo \"======================== stop yarn ========================== \"\n       ssh slave1 \"source /etc/profile;stop-yarn.sh\"\n       echo \"======================== stop hdfs ========================== \"\n      ssh master \"source /etc/profile;stop-dfs.sh\"\n      echo \"======================== stop zookeeper ========================== \"\n    for i in master slave1 slave2\n    do\n           echo \"========== $i zookeeper ==========\"\n           ssh $i \"source /etc/profile;zkServer.sh stop\"\n    done\n;;\n*)\n      echo \"Input Args Error!!!!!\"\n;;\nesac\n</code></pre>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p><a href=\"https://zhishuang.tk/2020/04/30/ubuntu1604-da-jian-hadoop-ji-qun/\">Hadoop搭建</a></p>\n<p><a href=\"https://zhishuang.tk/2020/05/06/hadoop-ha-da-jian/\">Zookeeper+Hadoopda搭建（Hadoop HA）</a></p>\n<p>这儿实现在Hadoop HA的基础上搭建Hbase</p>\n<h2 id=\"安装Hbase\"><a href=\"#安装Hbase\" class=\"headerlink\" title=\"安装Hbase\"></a>安装Hbase</h2><p>将安装文件<code>hbase-2.2.4-bin.tar.gz</code>到<code>/usr/local</code>并重命名为<code>hbase</code></p>\n<pre><code class=\"lang-bash\">tar -zxvf hbase-2.2.4-bin.tar.gz -C /usr/local\ncd /usr/local\nmv -r hbase-2.2.4 hbase\n</code></pre>\n<h2 id=\"配置Hbase\"><a href=\"#配置Hbase\" class=\"headerlink\" title=\"配置Hbase\"></a>配置Hbase</h2><p><strong>环境变量</strong>，执行<code>sudo gedit /etc/profile</code>打开配置文件，添加如下内容</p>\n<pre><code class=\"lang-bash\">#set hbase env\nexport HBASE_HOME=/usr/local/hbase\nexport PATH=$HBASE_HOME/bin:$PATH\n</code></pre>\n<p>记得所有主机都要配置，执行<code>source /etc/profile</code>使配置生效</p>\n<p><strong>hbase-env.sh</strong></p>\n<pre><code class=\"lang-bash\">export JAVA_HOME=/usr/java/jdk1.8.0_251\nexport HADOOP_HOME=/usr/local/hadoop\nexport HBASE_HOME=/usr/local/hbase\n#关闭自身zookeeper，采用外部的zookeeper\nexport HBASE_MANAGES_ZK=false\n</code></pre>\n<p><strong>hbase-site.xml</strong></p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n    &lt;!-- hadoop集群名称 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.rootdir&lt;/name&gt;\n        &lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;\n        &lt;value&gt;master,slave1,slave2&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;\n        &lt;value&gt;2181&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  是否是完全分布式 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  完全分布式式必须为false  --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  指定缓存文件存储的路径 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;!--  指定Zookeeper数据存储的路径  --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n        &lt;value&gt;/usr/local/zookeeper/zkData&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p><strong>regionservers</strong></p>\n<pre><code class=\"lang-bash\">master\nslave1\nslave2\n</code></pre>\n<p><strong>配置Hmaster高可用</strong></p>\n<p>为了保证HBase集群的高可靠性，HBase支持多Backup Master 设置。当Active Master挂掉后，Backup Master可以自动接管整个HBase的集群。该配置极其简单：在 $HBASE_HOME/conf/目录下新增文件配置backup-masters，在其内添加要用做Backup Master的节点hostname。<br>        执行<code>gedit /usr/local/hbase/conf/backup-masters</code>新建并打开文件，添加<code>slave2</code><br>        没设置backup-masters之前启动hbase， 只有一台有启动了HMaster进程，设置之后，重新启动整个集群，我们会发现，在backup-masters清单上的主机，都启动了HMaster进程</p>\n<p><strong>分发hbase给其他主机</strong></p>\n<pre><code class=\"lang-bash\">scp -r /usr/local/hbase/ slave1:/usr/local/\nscp -r /usr/local/hbase/ slave1:/usr/local/\n</code></pre>\n<h2 id=\"时间同步\"><a href=\"#时间同步\" class=\"headerlink\" title=\"时间同步\"></a>时间同步</h2><p>执行<code>sudo apt-get install ntp</code>安装ntp</p>\n<p>配置ntp，执行<code>gedit /etc/ntp.conf</code>打开配置文件</p>\n<ul>\n<li>master端配置</li>\n</ul>\n<pre><code class=\"lang-bash\"># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\npool 0.ubuntu.pool.ntp.org iburst\npool 1.ubuntu.pool.ntp.org iburst\npool 2.ubuntu.pool.ntp.org iburst\npool 3.ubuntu.pool.ntp.org iburst\n\n# Use Ubuntu&#39;s ntp server as a fallback.\npool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\nrestrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n</code></pre>\n<ul>\n<li>slave端配置</li>\n</ul>\n<pre><code class=\"lang-bash\"># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help\n# 时间差异文件\ndriftfile /var/lib/ntp/ntp.drift\n\n# 分析统计信息\n#statsdir /var/log/ntpstats/\n\nstatistics loopstats peerstats clockstats\nfilegen loopstats file loopstats type day enable\nfilegen peerstats file peerstats type day enable\nfilegen clockstats file clockstats type day enable\n\n# 上层ntp server.\n# pool 0.ubuntu.pool.ntp.org iburst\n# pool 1.ubuntu.pool.ntp.org iburst\n# pool 2.ubuntu.pool.ntp.org iburst\n# pool 3.ubuntu.pool.ntp.org iburst\nserver 192.168.79.129\n# Use Ubuntu&#39;s ntp server as a fallback.\n# pool ntp.ubuntu.com\n\n# 不允许来自公网上ipv4和ipv6客户端的访问\nrestrict -4 default kod notrap nomodify nopeer noquery limited\nrestrict -6 default kod notrap nomodify nopeer noquery limited\n\n# 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.\nrestrict 127.0.0.1\nrestrict ::1\n\n# Needed for adding pool entries\nrestrict source notrap nomodify noquery\n\n# 允许这个网段的对时请求.\n# restrict 192.168.79.0 mask 255.255.255.0 nomodify \n\n# If you want to provide time to your local subnet, change the next line.\n# (Again, the address is an example only.)\n#broadcast 192.168.123.255\n\n# If you want to listen to time broadcasts on your local subnet, de-comment the\n# next lines.  Please do this only if you trust everybody on the network!\n#disable auth\n#broadcastclient\n\n#Changes recquired to use pps synchonisation as explained in documentation:\n#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918\n\n#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS\n#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware\n\n#server 127.127.22.1                   # ATOM(PPS)\n#fudge 127.127.22.1 flag3 1            # enable PPS API\n</code></pre>\n<p>查看ntp的时间服务是否启动：<code>ps -aux | grep ntp</code></p>\n<p>执行 <code>service ntp restart</code>，重启ntp服务</p>\n<p>执行<code>ntpq -p</code>查看配置</p>\n<p>这个命令可以列出目前我们的 NTP 与相关的上层 NTP 的状态，上头的几个字段的意义为：</p>\n<ul>\n<li>remote: 它指的就是本地机器所连接的远程NTP服务器；</li>\n<li>refid: 它指的是给远程服务器提供时间同步的服务器；</li>\n<li>st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端。所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的；</li>\n<li>when: 几秒钟前曾经做过时间同步化更新的动作；</li>\n<li>poll: 本地机和远程服务器多少时间进行一次同步(单位为秒).在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小；</li>\n<li>reach: 已经向上层 NTP 服务器要求更新的次数；</li>\n<li>delay: 网络传输过程当中延迟的时间，单位为 10^(-6) 秒；</li>\n<li>offset: 时间补偿的结果，单位与 10^(-3) 秒；</li>\n<li>jitter: Linux 系统时间与 BIOS 硬件时间的差异时间， 单为 10^(-6) 秒。简单地说这个数值的绝对值越小我们和服务器的时间就越精确；</li>\n<li>*: 它告诉我们远端的服务器已经被确认为我们的主NTP Server,我们系统的时间将由这台机器所提供；</li>\n<li>+: 它将作为辅助的NTP Server和带有号的服务器一起为我们提供同步服务. 当号服务器不可用时它就可以接管；</li>\n<li>-: 远程服务器被clustering algorithm认为是不合格的NTP Server；</li>\n<li>x: 远程服务器不可用</li>\n</ul>\n<h2 id=\"启动HBase\"><a href=\"#启动HBase\" class=\"headerlink\" title=\"启动HBase\"></a>启动HBase</h2><p>首先启动Hadoop-HA集群，再执行<code>start-hbase.start</code>启动HBase，执行jps命令，可以看到：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141401.png\" alt=\"\"></p>\n<p>从<code>http://master:16010/</code>可查看HBase集群信息</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200509141902.png\" alt=\"\"></p>\n<h2 id=\"群起脚本\"><a href=\"#群起脚本\" class=\"headerlink\" title=\"群起脚本\"></a>群起脚本</h2><pre><code class=\"lang-bash\">#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo &quot;No Args Input Error!!!!!&quot;\n   exit\nfi\ncase $1 in \n&quot;start&quot;)\n       echo &quot;======================== start zookeeper ========================== &quot;\n    for i in master slave1 slave2\n    do\n           echo &quot;========== $i zookeeper ==========&quot;\n           ssh $i &quot;source /etc/profile;zkServer.sh start&quot;\n    done\n    echo &quot;======================== start hdfs ========================== &quot;\n      ssh master &quot;source /etc/profile;start-dfs.sh&quot;\n       echo &quot;======================== start yarn ========================== &quot;\n       ssh slave1 &quot;source /etc/profile;start-yarn.sh&quot;\n       echo &quot;======================== start hbase ========================== &quot;\n       ssh master &quot;source /etc/profile;start-hbase.sh&quot;\n;;\n&quot;stop&quot;)\n    echo &quot;======================== stop hbase ========================== &quot;\n      ssh master &quot;source /etc/profile;stop-hbase.sh&quot;\n    echo &quot;======================== stop yarn ========================== &quot;\n       ssh slave1 &quot;source /etc/profile;stop-yarn.sh&quot;\n       echo &quot;======================== stop hdfs ========================== &quot;\n      ssh master &quot;source /etc/profile;stop-dfs.sh&quot;\n      echo &quot;======================== stop zookeeper ========================== &quot;\n    for i in master slave1 slave2\n    do\n           echo &quot;========== $i zookeeper ==========&quot;\n           ssh $i &quot;source /etc/profile;zkServer.sh stop&quot;\n    done\n;;\n*)\n      echo &quot;Input Args Error!!!!!&quot;\n;;\nesac\n</code></pre>\n"},{"title":"mathjax 公式渲染修复","top":false,"cover":false,"toc":true,"mathjax":false,"reprintPolicy":"cc_by","date":"2020-05-19T05:52:59.000Z","author":null,"password":null,"img":null,"summary":null,"keywords":"mathjax渲染 hexo","_content":"\n## 问题\n\nhexo默认使用**hexo-renderer-marked**引擎去渲染网页，它会把利用**Markdown语法**写的文本去转换为相应的**html标签**，使得我们写的MathJax公式被错误渲染，没法正确显示出来。\n\n## 解决\n\n```bash\nnpm uninstall hexo-renderer-marked --save\nnpm install hexo-renderer-kramed --save\n```\n\n[hexo-renderer-kramed](https://github.com/sun11/hexo-renderer-kramed)有人写好的一个修复库\n\n重新部署后如果发现内联公式仍有渲染错误的，找到**../node_modules/kramed/lib/rules/inline.js**文件\n\n```js\n//escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_>])/,      第11行，将其修改为\nescape: /^\\\\([`*\\[\\]()#$+\\-.!_>])/,\n//em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,    第20行，将其修改为\nem: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n```\n\n它取消了该渲染引擎对`\\,{,}`的转义，然后再`hexo clean、hexo g`重新部署，即可解决问题\n\n更多[参考](https://vitaheng.com/2017/08/03/hexo博客MathJax公式渲染问题/)","source":"_posts/mathjax-公式渲染修复.md","raw":"---\ntitle: mathjax 公式渲染修复\ntop: false\ncover: false\ntoc: true\nmathjax: false\nreprintPolicy: cc_by\ndate: 2020-05-19 13:52:59\nauthor:\npassword:\nimg:\nsummary:\ncategories: hexo\nkeywords: mathjax渲染 hexo\ntags:\n\t- mathjax渲染\n\t- hexo\n---\n\n## 问题\n\nhexo默认使用**hexo-renderer-marked**引擎去渲染网页，它会把利用**Markdown语法**写的文本去转换为相应的**html标签**，使得我们写的MathJax公式被错误渲染，没法正确显示出来。\n\n## 解决\n\n```bash\nnpm uninstall hexo-renderer-marked --save\nnpm install hexo-renderer-kramed --save\n```\n\n[hexo-renderer-kramed](https://github.com/sun11/hexo-renderer-kramed)有人写好的一个修复库\n\n重新部署后如果发现内联公式仍有渲染错误的，找到**../node_modules/kramed/lib/rules/inline.js**文件\n\n```js\n//escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_>])/,      第11行，将其修改为\nescape: /^\\\\([`*\\[\\]()#$+\\-.!_>])/,\n//em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,    第20行，将其修改为\nem: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n```\n\n它取消了该渲染引擎对`\\,{,}`的转义，然后再`hexo clean、hexo g`重新部署，即可解决问题\n\n更多[参考](https://vitaheng.com/2017/08/03/hexo博客MathJax公式渲染问题/)","slug":"mathjax-公式渲染修复","published":1,"updated":"2020-08-20T08:41:44.645Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0ma5000eewse9uz66zua","content":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>hexo默认使用<strong>hexo-renderer-marked</strong>引擎去渲染网页，它会把利用<strong>Markdown语法</strong>写的文本去转换为相应的<strong>html标签</strong>，使得我们写的MathJax公式被错误渲染，没法正确显示出来。</p>\n<h2 id=\"解决\"><a href=\"#解决\" class=\"headerlink\" title=\"解决\"></a>解决</h2><pre><code class=\"lang-bash\">npm uninstall hexo-renderer-marked --save\nnpm install hexo-renderer-kramed --save\n</code></pre>\n<p><a href=\"https://github.com/sun11/hexo-renderer-kramed\" target=\"_blank\" rel=\"noopener\">hexo-renderer-kramed</a>有人写好的一个修复库</p>\n<p>重新部署后如果发现内联公式仍有渲染错误的，找到<strong>../node_modules/kramed/lib/rules/inline.js</strong>文件</p>\n<pre><code class=\"lang-js\">//escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_&gt;])/,      第11行，将其修改为\nescape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,\n//em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,    第20行，将其修改为\nem: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n</code></pre>\n<p>它取消了该渲染引擎对<code>\\,{,}</code>的转义，然后再<code>hexo clean、hexo g</code>重新部署，即可解决问题</p>\n<p>更多<a href=\"https://vitaheng.com/2017/08/03/hexo博客MathJax公式渲染问题/\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>hexo默认使用<strong>hexo-renderer-marked</strong>引擎去渲染网页，它会把利用<strong>Markdown语法</strong>写的文本去转换为相应的<strong>html标签</strong>，使得我们写的MathJax公式被错误渲染，没法正确显示出来。</p>\n<h2 id=\"解决\"><a href=\"#解决\" class=\"headerlink\" title=\"解决\"></a>解决</h2><pre><code class=\"lang-bash\">npm uninstall hexo-renderer-marked --save\nnpm install hexo-renderer-kramed --save\n</code></pre>\n<p><a href=\"https://github.com/sun11/hexo-renderer-kramed\" target=\"_blank\" rel=\"noopener\">hexo-renderer-kramed</a>有人写好的一个修复库</p>\n<p>重新部署后如果发现内联公式仍有渲染错误的，找到<strong>../node_modules/kramed/lib/rules/inline.js</strong>文件</p>\n<pre><code class=\"lang-js\">//escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_&gt;])/,      第11行，将其修改为\nescape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,\n//em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,    第20行，将其修改为\nem: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n</code></pre>\n<p>它取消了该渲染引擎对<code>\\,{,}</code>的转义，然后再<code>hexo clean、hexo g</code>重新部署，即可解决问题</p>\n<p>更多<a href=\"https://vitaheng.com/2017/08/03/hexo博客MathJax公式渲染问题/\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n"},{"title":"ubuntu1604 搭建Hadoop集群","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-04-30T08:28:34.000Z","password":null,"summary":null,"img":null,"keywords":"hadoop 集群","_content":"\n## 系统环境\n\n| 名称   | 版本                                                         |\n| ------ | ------------------------------------------------------------ |\n| Ununtu | 1604                                                         |\n| JDK    | [JDK8](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html) |\n| Hadoop | [3.1.3](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz) |\n\n下载JDK8和Hadoop3.1.3，放在ubuntu里Downloads文件夹下\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430165326.png)\n\n## 基本配置\n\n### 设置静态IP\n\n[ubuntu1604 设置静态IP](https://zhishuang.tk/2020/04/30/ubuntu1604-she-zhi-jing-tai-ip/)\n\n### 修改hostname\n\n执行命令`sudo gedit /etc/hostname`,修改hostname为master\n\n### 配置主机映射\n\n执行命令`ifconfig`获取IP地址\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504220459.png)\n\n执行命令`sudo gedit /etc/hosts`,添加映射关系，注意IP地址换成自己的\n\n```bash\n192.168.79.129\tmaster\n```\n\n### 关闭防火墙\n\n执行命名`sudo ufw disable`关闭防火墙\n\n### 配置SSH\n\n>Hadoop名称节点(NameNode)需要启动集群中所有机器的Hadoop守护进程，这个过 程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为 了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们。\n\n#### 配置SSH的无密码登录\n\n安装openssh-server( 通常Linux系统会默认安装openssh的客户端软件openssh-client)，所以需要自己安装一下服务端。\n\n```bash\nsudo apt-get install openssh-server\n```\n\n在终端执行命令`ssh-keygen -t rsa`，然后一直回车，生成密钥。\n\n执行命令`ssh-copy-id localhost`，在提示信息中输入`yes`，然后输入本机密码\n\n执行命令`ssh localhost`，出现登录时间，说明ssh配置成功\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504234742.png)\n\n## 配置JAVA环境\n\n### 创建文件夹\n\n```shell\nsudo mkdir /usr/java\n```\n\n在/usr目录下创建文件夹java，我们将把JDK8解压到这个文件夹中\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430170954.png)\n\n### 解压JDK\n\n```bash\nsudo tar -zxvf ~/Downloads/jdk-8u251-linux-x64.tar.gz -C /usr/java\n```\n\n解压jdk8到/usr/java目录下\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430171614.png)\n\n### 配置环境变量\n\n执行`sudo gedit /etc/profile`命令打开配置文件，在末尾添加以下几行文字，注意自己的jdk版本号。\n\n```bash\n#set java env\nexport JAVA_HOME=/usr/java/jdk1.8.0_251\nexport JRE_HOME=${JAVA_HOME}/jre    \nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    \nexport PATH=${JAVA_HOME}/bin:$PATH\n```\n\n执行`source /etc/profile`命令让配置文件生效\n\n执行`java -version`命令验证是否配置成功\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430173142.png)\n\n## 配置Hadoop环境\n\n### 解压Hadoop\n\n将我们下载的Hadoop解压到 /usr/local/ 中\n\n```bash\nsudo tar -zxvf ~/Downloads/hadoop-3.1.3.tar.gz -C /usr/local\n```\n\n利用`cd /usr/local/`命令切换操作空间，将文件夹名改为hadoop\n\n```bash\nsudo mv ./hadoop-3.1.3/ ./hadoop\n```\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430174115.png)\n\n### 配置Hadoop环境变量\n\n执行`sudo gedit /etc/profile`命令打开配置文件,添加下面的语句\n\n```bash\n#set Hadoop env\nHADOOP_HOME=/usr/local/hadoop\nexport PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\n```\n\n执行`source /etc/profile`命令让配置文件生效\n\n### 修改Hadoop配置文件\n\n执行`sudo gedit hadoop-env.sh`命令打开配置文件，添加语句`export JAVA_HOME=/usr/java/jdk1.8.0_251`\n\n执行`hadoop version`命令验证是否配置成功\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430183459.png)\n\n到这儿 我们Hadoop的单机模式配置完成\n\n## Hadoop伪分布式安装配置\n\n>Hadoop可以在单节点上以伪分布式的方式运行，Hadoop进程以分离的 Java 进程来运行，节点既作为 NameNode也作为DataNode， 同时，读取的是 HDFS 中的文件\n>\n>Hadoop的配置文件位于$HADOOP_HOME/etc/hadoop/中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml\n>\n>Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现\n\n### hadoop 目录说明\n\n>修改配置文件之前，先看一下hadoop下的目录：\n>\n>- bin：hadoop最基本的管理脚本和使用脚本所在目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用hadoop\n>- etc：配置文件存放的目录，包括core-site.xml,hdfs-site.xml,mapred-site.xml等从hadoop1.x继承而来的配置文件和yarn-site.xml等hadoop2.x新增的配置文件\n>- include：对外提供的编程库头文件（具体动态库和静态库在lib目录中，这些头文件军事用c++定义的，通常用于c++程序访问hdfs或者编写mapreduce程序）\n>- Lib：该目录包含了hadoop对外提供的才变成动态库和静态库，与include目录中的头文件结合使用\n>- libexec：各个服务对应的shell配置文件所在目录，可用于配置日志输出目录、启动参数等信息\n>- sbin：hadoop管理脚本所在目录，主要包含hdfs和yarn中各类服务的启动、关闭脚本\n>- share：hadoop各个模块编译后的jar包所在目录。\n\n### 修改配置文件\t\n\n在 `/usr/local/hadoop/etc/hadoop/`目录下，执行命令`sudo gedit core-site.xml`，修改配置文件core-site.xml，内容如下：\n\n```xml\n<configuration> \n <property>\n      <name>hadoop.tmp.dir</name> \n      <value>file:/usr/local/hadoop/tmp</value>\n      <description>Abase for other temporary directories.</description>\n </property> \n <property>\n   <name>fs.defaultFS</name>\n   <value>hdfs://localhost:9000</value> \n  </property>\n</configuration>\n```\n\n- hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包 括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可\n- name为fs.defaultFS的值，表示hdfs路径的逻辑名称\n\n在 `/usr/local/hadoop/etc/hadoop/`目录下，执行命令`sudo gedit hdfs-site.xml`，修改配置文件hdfs-site.xml，内容如下：\n\n```xml\n<configuration> \n   <property>\n       <name>dfs.replication</name>\n       <value>1</value> \n   </property> \n   <property>\n       <name>dfs.namenode.name.dir</name>\n       <value>file:/usr/local/hadoop/tmp/dfs/name</value> \n    </property>\n   <property>\n       <name>dfs.datanode.data.dir</name>         \n       <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n   </property>\n</configuration>\n```\n\n- dfs.replication表示副本的数量，伪分布式要设置为1\n- dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方\n- dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方\n\n### 启动HDFS\t\n\n第一次启动时需要执行 `hdfs namenode - format`格式化节点\n\n执行`start-all.sh`启动HDFS及相关服务，执行`jps`命令，出现以下6个线程说配置成功：\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504235929.png)\n\n进一步确认，可以浏览器访问`localhost:9870`和`localhost:8088`\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000709.png)\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000753.png)\n\n## 配置Hadoop集群\n\n先从master克隆两个虚拟机，分别命名为slave1和slave2。配置方案如下：\n\n|      | master                | slave1                         | slave2                          |\n| ---- | --------------------- | ------------------------------ | ------------------------------- |\n| HDFS | NameNode<br/>DataNode | SecondaryNameNode<br/>DataNode | DataNode                        |\n| YARN | NodeManager           | NodeManager                    | ResourceManager<br/>NodeManager |\n\n### IP，主机名，主机映射配置\n\n#### slave1和slave2配置\n\n执行`sudo gedit /etc/network/interfaces`命令打开网络配置文件，修改slave1的IP为192.168.79.130，修改slave2的IP为192.168.79.131\n\n执行`sudo gedit /etc/hostname`命令，修改slave1的hostname为slave1，修改slave2的hostname为slave2\n\n执行`sudo gedit /etc/hosts`配置主机映射，添加下列语句：\n\n```bash\n#192.168.79.129\tmaster#这条已经存在，不用添加\n192.168.79.130\tslave1\n192.168.79.131\tslave2\n```\n\n注意，上面的配置需要分别在slave1和slave2上执行，配置结束之后执行`reboot`重启主机\n\n#### master配置\n\n执行`sudo gedit /etc/hosts`配置主机映射，添加下列语句\n\n```bash\n192.168.79.130\tslave1\n192.168.79.131\tslave2\n```\n\n### 免密配置\n\n免密配置需要保证三台主机之间都能互相免密登录，**三台主机**都需要**执行**下面的命令\n\n```bash\nssh-keygen -t rsa    //前面配置过则不需要\nssh-copy-id master    //将公钥文件发送给包括自身在内的3台服务器\nssh-copy-id slave1\nssh-copy-id slave2\n\nssh master    //连接其他服务器看还是否需要密码，三台服务器之间都不需要则配置成功\nssh slave1\nssh slave2\n```\n\n### 配置Hadoop配置文件\n\n**在master中配置好后再分发给slave1和slave2**\n\n执行`cd /usr/local/hadoop/etc/hadoop/`进入配置文件所在的文件夹\n\n执行`sudo gedit core-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration> \n <property>\n      <name>hadoop.tmp.dir</name> \n      <value>file:/usr/local/hadoop/tmp</value>\n      <description>Abase for other temporary directories.</description>\n </property>\n<!-- 指定NameNode的地址 --> \n <property>\n   <name>fs.defaultFS</name>\n   <value>hdfs://master:9000</value> \n </property>\n</configuration>\n```\n\n执行`sudo gedit hdfs-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration> \n   <!-- 指定冗余度 -->\n   <property>\n       <name>dfs.replication</name>\n       <value>3</value> \n   </property>\n   <!-- 指定NameNode数据的存储目录 --> \n   <property>\n       <name>dfs.namenode.name.dir</name>\n       <value>file:/usr/local/hadoop/tmp/dfs/name</value> \n   </property>\n   <!-- 指定Datanode数据的存储目录 -->\n   <property>\n       <name>dfs.datanode.data.dir</name>         \n       <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n   </property>\n   <!-- 设置secondarynamenode -->\n   <property>\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>slave1:50090</value>\n   </property>\n</configuration>\n```\n\n执行`sudo gedit yarn-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration>\n   <!--指定mapreduce走shuffle -->\n   <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n   </property>\n   <!-- 指定ResourceManager的地址-->\n   <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>slave2</value>\n   </property>\n</configuration>\n```\n\n执行`sudo gedit mapred-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration>\n<!-- 指定mapreduce运行在yarn上 -->\n<property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n</property>\n</configuration>\n```\n\n执行`sudo gedit workers`打开配置文件，配置如下：\n\n```bash\nmaster\nslave1\nslave2\n```\n\n将配置好的文件分发给slave1和slave2\n\n```bash\nscp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n```\n\n### 启动集群\n\n首次运行之前需要格式化namenode，如果首次运行失败后又重新进行了部分配置，运行之前最好也格式化namenode，格式化namenode之前需要删除hadoop目录下的data,logs以及/tmp目录下的hadoop开头的文件（在没有数据的情况下）\n\n* 在master主机上执行`hadoop namenode -format`命令格式化namenode\n\n* 在master主机上执行`start-dfs.sh`命令启动HDFS，如果正常启动：\n\t* 在master上执行`jps`命令可以看到有namenode，datanode\n\t* 在slave1上执行`jps`命令可以看到有secondarynamenode，datanode\n\t* 在slave2上执行`jps`命令可以看到有datanode\n* 因为resourcemanager部署在slave2上，所以在slave2上执行`start-yarn.sh`命令启动YARN，成功则：\n\t* 在master上执行`jps`命令可以看到有namenode，datanode，nodemanager\n\t* 在slave1上执行`jps`命令可以看到有secondarynamenode，datanode，nodemanager\n\t* 在slave2上执行`jps`命令可以看到有datanode，resourcemanager，nodemanager\n\n至此完成了集群的搭建\n\n## 查看集群\n\n打开`http://master:9870/`观测是否有三分datanode\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505191607.png)\n\n## 测试集群\n\n\n\n## 编写集群启动脚本\n\n为了方便启动集群和关闭集群，编写一个脚本，在master机器上执行`mkdir bin`命令在用户目录下创建`bin`目录，执行`gedit mycluster`命令创建并打开文件`mycluster`,在文件中添加下面的内容：\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   echo \"======================== start hdfs ========================== \"\n   ssh master \"source /etc/profile;start-dfs.sh\"\n   echo \"======================== start yarn ========================== \"\n   ssh slave2 \"source /etc/profile;start-yarn.sh\"\n;;\n\"stop\")\n   echo \"======================== stop yarn ========================== \"\n   ssh slave2 \"source /etc/profile;stop-yarn.sh\"\n   echo \"======================== stop hdfs ========================== \"\n   ssh master \"source /etc/profile;stop-dfs.sh\"\n;;\n*)\n  echo \"Input Args Error!!!!!\"\n;;\nesac\n```\n\n在`bin`目录下执行`gedit myjps`命令创建并打开文件`myjps`,在文件中添加下面的内容：\n\n```bash\n#!/bin/bash\nfor i in master slave1 slave2\ndo\n   echo \"====================== $i JPS =======================\"\n   ssh $i \"source /etc/profile;jps\"\ndone\n```\n\n执行`chmod 777 mycluster`和`chmod 777 myjps`命令修改脚本权限，之后只需要执行`mycluster start`即可启动集群。\n\n## 问题解决\n\n由于JAVA，Hadoop等环境变量配置在/etc/profile文件中，导致每次新打开一个命令窗口都要重新输入 source /etc/profile 才能使jdk等配置文件生效：\n\n解决方法：\n\n打开~/.bashrc 文件\n\n```bash\nsudo gedit ~/.bashrc\n```\n\n加入下列语句\n\n```bash\nsource /etc/profile\n```\n\n","source":"_posts/ubuntu1604-搭建Hadoop集群.md","raw":"---\ntitle: ubuntu1604 搭建Hadoop集群\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-04-30 16:28:34\npassword:\nsummary:\ncategories: 大数据\nimg:\nkeywords: hadoop 集群\ntags:\n\t- hadoop\n\t- 集群\n---\n\n## 系统环境\n\n| 名称   | 版本                                                         |\n| ------ | ------------------------------------------------------------ |\n| Ununtu | 1604                                                         |\n| JDK    | [JDK8](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html) |\n| Hadoop | [3.1.3](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz) |\n\n下载JDK8和Hadoop3.1.3，放在ubuntu里Downloads文件夹下\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430165326.png)\n\n## 基本配置\n\n### 设置静态IP\n\n[ubuntu1604 设置静态IP](https://zhishuang.tk/2020/04/30/ubuntu1604-she-zhi-jing-tai-ip/)\n\n### 修改hostname\n\n执行命令`sudo gedit /etc/hostname`,修改hostname为master\n\n### 配置主机映射\n\n执行命令`ifconfig`获取IP地址\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504220459.png)\n\n执行命令`sudo gedit /etc/hosts`,添加映射关系，注意IP地址换成自己的\n\n```bash\n192.168.79.129\tmaster\n```\n\n### 关闭防火墙\n\n执行命名`sudo ufw disable`关闭防火墙\n\n### 配置SSH\n\n>Hadoop名称节点(NameNode)需要启动集群中所有机器的Hadoop守护进程，这个过 程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为 了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们。\n\n#### 配置SSH的无密码登录\n\n安装openssh-server( 通常Linux系统会默认安装openssh的客户端软件openssh-client)，所以需要自己安装一下服务端。\n\n```bash\nsudo apt-get install openssh-server\n```\n\n在终端执行命令`ssh-keygen -t rsa`，然后一直回车，生成密钥。\n\n执行命令`ssh-copy-id localhost`，在提示信息中输入`yes`，然后输入本机密码\n\n执行命令`ssh localhost`，出现登录时间，说明ssh配置成功\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504234742.png)\n\n## 配置JAVA环境\n\n### 创建文件夹\n\n```shell\nsudo mkdir /usr/java\n```\n\n在/usr目录下创建文件夹java，我们将把JDK8解压到这个文件夹中\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430170954.png)\n\n### 解压JDK\n\n```bash\nsudo tar -zxvf ~/Downloads/jdk-8u251-linux-x64.tar.gz -C /usr/java\n```\n\n解压jdk8到/usr/java目录下\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430171614.png)\n\n### 配置环境变量\n\n执行`sudo gedit /etc/profile`命令打开配置文件，在末尾添加以下几行文字，注意自己的jdk版本号。\n\n```bash\n#set java env\nexport JAVA_HOME=/usr/java/jdk1.8.0_251\nexport JRE_HOME=${JAVA_HOME}/jre    \nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    \nexport PATH=${JAVA_HOME}/bin:$PATH\n```\n\n执行`source /etc/profile`命令让配置文件生效\n\n执行`java -version`命令验证是否配置成功\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430173142.png)\n\n## 配置Hadoop环境\n\n### 解压Hadoop\n\n将我们下载的Hadoop解压到 /usr/local/ 中\n\n```bash\nsudo tar -zxvf ~/Downloads/hadoop-3.1.3.tar.gz -C /usr/local\n```\n\n利用`cd /usr/local/`命令切换操作空间，将文件夹名改为hadoop\n\n```bash\nsudo mv ./hadoop-3.1.3/ ./hadoop\n```\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430174115.png)\n\n### 配置Hadoop环境变量\n\n执行`sudo gedit /etc/profile`命令打开配置文件,添加下面的语句\n\n```bash\n#set Hadoop env\nHADOOP_HOME=/usr/local/hadoop\nexport PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\n```\n\n执行`source /etc/profile`命令让配置文件生效\n\n### 修改Hadoop配置文件\n\n执行`sudo gedit hadoop-env.sh`命令打开配置文件，添加语句`export JAVA_HOME=/usr/java/jdk1.8.0_251`\n\n执行`hadoop version`命令验证是否配置成功\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430183459.png)\n\n到这儿 我们Hadoop的单机模式配置完成\n\n## Hadoop伪分布式安装配置\n\n>Hadoop可以在单节点上以伪分布式的方式运行，Hadoop进程以分离的 Java 进程来运行，节点既作为 NameNode也作为DataNode， 同时，读取的是 HDFS 中的文件\n>\n>Hadoop的配置文件位于$HADOOP_HOME/etc/hadoop/中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml\n>\n>Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现\n\n### hadoop 目录说明\n\n>修改配置文件之前，先看一下hadoop下的目录：\n>\n>- bin：hadoop最基本的管理脚本和使用脚本所在目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用hadoop\n>- etc：配置文件存放的目录，包括core-site.xml,hdfs-site.xml,mapred-site.xml等从hadoop1.x继承而来的配置文件和yarn-site.xml等hadoop2.x新增的配置文件\n>- include：对外提供的编程库头文件（具体动态库和静态库在lib目录中，这些头文件军事用c++定义的，通常用于c++程序访问hdfs或者编写mapreduce程序）\n>- Lib：该目录包含了hadoop对外提供的才变成动态库和静态库，与include目录中的头文件结合使用\n>- libexec：各个服务对应的shell配置文件所在目录，可用于配置日志输出目录、启动参数等信息\n>- sbin：hadoop管理脚本所在目录，主要包含hdfs和yarn中各类服务的启动、关闭脚本\n>- share：hadoop各个模块编译后的jar包所在目录。\n\n### 修改配置文件\t\n\n在 `/usr/local/hadoop/etc/hadoop/`目录下，执行命令`sudo gedit core-site.xml`，修改配置文件core-site.xml，内容如下：\n\n```xml\n<configuration> \n <property>\n      <name>hadoop.tmp.dir</name> \n      <value>file:/usr/local/hadoop/tmp</value>\n      <description>Abase for other temporary directories.</description>\n </property> \n <property>\n   <name>fs.defaultFS</name>\n   <value>hdfs://localhost:9000</value> \n  </property>\n</configuration>\n```\n\n- hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包 括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可\n- name为fs.defaultFS的值，表示hdfs路径的逻辑名称\n\n在 `/usr/local/hadoop/etc/hadoop/`目录下，执行命令`sudo gedit hdfs-site.xml`，修改配置文件hdfs-site.xml，内容如下：\n\n```xml\n<configuration> \n   <property>\n       <name>dfs.replication</name>\n       <value>1</value> \n   </property> \n   <property>\n       <name>dfs.namenode.name.dir</name>\n       <value>file:/usr/local/hadoop/tmp/dfs/name</value> \n    </property>\n   <property>\n       <name>dfs.datanode.data.dir</name>         \n       <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n   </property>\n</configuration>\n```\n\n- dfs.replication表示副本的数量，伪分布式要设置为1\n- dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方\n- dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方\n\n### 启动HDFS\t\n\n第一次启动时需要执行 `hdfs namenode - format`格式化节点\n\n执行`start-all.sh`启动HDFS及相关服务，执行`jps`命令，出现以下6个线程说配置成功：\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504235929.png)\n\n进一步确认，可以浏览器访问`localhost:9870`和`localhost:8088`\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000709.png)\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000753.png)\n\n## 配置Hadoop集群\n\n先从master克隆两个虚拟机，分别命名为slave1和slave2。配置方案如下：\n\n|      | master                | slave1                         | slave2                          |\n| ---- | --------------------- | ------------------------------ | ------------------------------- |\n| HDFS | NameNode<br/>DataNode | SecondaryNameNode<br/>DataNode | DataNode                        |\n| YARN | NodeManager           | NodeManager                    | ResourceManager<br/>NodeManager |\n\n### IP，主机名，主机映射配置\n\n#### slave1和slave2配置\n\n执行`sudo gedit /etc/network/interfaces`命令打开网络配置文件，修改slave1的IP为192.168.79.130，修改slave2的IP为192.168.79.131\n\n执行`sudo gedit /etc/hostname`命令，修改slave1的hostname为slave1，修改slave2的hostname为slave2\n\n执行`sudo gedit /etc/hosts`配置主机映射，添加下列语句：\n\n```bash\n#192.168.79.129\tmaster#这条已经存在，不用添加\n192.168.79.130\tslave1\n192.168.79.131\tslave2\n```\n\n注意，上面的配置需要分别在slave1和slave2上执行，配置结束之后执行`reboot`重启主机\n\n#### master配置\n\n执行`sudo gedit /etc/hosts`配置主机映射，添加下列语句\n\n```bash\n192.168.79.130\tslave1\n192.168.79.131\tslave2\n```\n\n### 免密配置\n\n免密配置需要保证三台主机之间都能互相免密登录，**三台主机**都需要**执行**下面的命令\n\n```bash\nssh-keygen -t rsa    //前面配置过则不需要\nssh-copy-id master    //将公钥文件发送给包括自身在内的3台服务器\nssh-copy-id slave1\nssh-copy-id slave2\n\nssh master    //连接其他服务器看还是否需要密码，三台服务器之间都不需要则配置成功\nssh slave1\nssh slave2\n```\n\n### 配置Hadoop配置文件\n\n**在master中配置好后再分发给slave1和slave2**\n\n执行`cd /usr/local/hadoop/etc/hadoop/`进入配置文件所在的文件夹\n\n执行`sudo gedit core-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration> \n <property>\n      <name>hadoop.tmp.dir</name> \n      <value>file:/usr/local/hadoop/tmp</value>\n      <description>Abase for other temporary directories.</description>\n </property>\n<!-- 指定NameNode的地址 --> \n <property>\n   <name>fs.defaultFS</name>\n   <value>hdfs://master:9000</value> \n </property>\n</configuration>\n```\n\n执行`sudo gedit hdfs-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration> \n   <!-- 指定冗余度 -->\n   <property>\n       <name>dfs.replication</name>\n       <value>3</value> \n   </property>\n   <!-- 指定NameNode数据的存储目录 --> \n   <property>\n       <name>dfs.namenode.name.dir</name>\n       <value>file:/usr/local/hadoop/tmp/dfs/name</value> \n   </property>\n   <!-- 指定Datanode数据的存储目录 -->\n   <property>\n       <name>dfs.datanode.data.dir</name>         \n       <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n   </property>\n   <!-- 设置secondarynamenode -->\n   <property>\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>slave1:50090</value>\n   </property>\n</configuration>\n```\n\n执行`sudo gedit yarn-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration>\n   <!--指定mapreduce走shuffle -->\n   <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n   </property>\n   <!-- 指定ResourceManager的地址-->\n   <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>slave2</value>\n   </property>\n</configuration>\n```\n\n执行`sudo gedit mapred-site.xml`打开配置文件，配置如下：\n\n```xml\n<configuration>\n<!-- 指定mapreduce运行在yarn上 -->\n<property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n</property>\n</configuration>\n```\n\n执行`sudo gedit workers`打开配置文件，配置如下：\n\n```bash\nmaster\nslave1\nslave2\n```\n\n将配置好的文件分发给slave1和slave2\n\n```bash\nscp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n```\n\n### 启动集群\n\n首次运行之前需要格式化namenode，如果首次运行失败后又重新进行了部分配置，运行之前最好也格式化namenode，格式化namenode之前需要删除hadoop目录下的data,logs以及/tmp目录下的hadoop开头的文件（在没有数据的情况下）\n\n* 在master主机上执行`hadoop namenode -format`命令格式化namenode\n\n* 在master主机上执行`start-dfs.sh`命令启动HDFS，如果正常启动：\n\t* 在master上执行`jps`命令可以看到有namenode，datanode\n\t* 在slave1上执行`jps`命令可以看到有secondarynamenode，datanode\n\t* 在slave2上执行`jps`命令可以看到有datanode\n* 因为resourcemanager部署在slave2上，所以在slave2上执行`start-yarn.sh`命令启动YARN，成功则：\n\t* 在master上执行`jps`命令可以看到有namenode，datanode，nodemanager\n\t* 在slave1上执行`jps`命令可以看到有secondarynamenode，datanode，nodemanager\n\t* 在slave2上执行`jps`命令可以看到有datanode，resourcemanager，nodemanager\n\n至此完成了集群的搭建\n\n## 查看集群\n\n打开`http://master:9870/`观测是否有三分datanode\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505191607.png)\n\n## 测试集群\n\n\n\n## 编写集群启动脚本\n\n为了方便启动集群和关闭集群，编写一个脚本，在master机器上执行`mkdir bin`命令在用户目录下创建`bin`目录，执行`gedit mycluster`命令创建并打开文件`mycluster`,在文件中添加下面的内容：\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   echo \"======================== start hdfs ========================== \"\n   ssh master \"source /etc/profile;start-dfs.sh\"\n   echo \"======================== start yarn ========================== \"\n   ssh slave2 \"source /etc/profile;start-yarn.sh\"\n;;\n\"stop\")\n   echo \"======================== stop yarn ========================== \"\n   ssh slave2 \"source /etc/profile;stop-yarn.sh\"\n   echo \"======================== stop hdfs ========================== \"\n   ssh master \"source /etc/profile;stop-dfs.sh\"\n;;\n*)\n  echo \"Input Args Error!!!!!\"\n;;\nesac\n```\n\n在`bin`目录下执行`gedit myjps`命令创建并打开文件`myjps`,在文件中添加下面的内容：\n\n```bash\n#!/bin/bash\nfor i in master slave1 slave2\ndo\n   echo \"====================== $i JPS =======================\"\n   ssh $i \"source /etc/profile;jps\"\ndone\n```\n\n执行`chmod 777 mycluster`和`chmod 777 myjps`命令修改脚本权限，之后只需要执行`mycluster start`即可启动集群。\n\n## 问题解决\n\n由于JAVA，Hadoop等环境变量配置在/etc/profile文件中，导致每次新打开一个命令窗口都要重新输入 source /etc/profile 才能使jdk等配置文件生效：\n\n解决方法：\n\n打开~/.bashrc 文件\n\n```bash\nsudo gedit ~/.bashrc\n```\n\n加入下列语句\n\n```bash\nsource /etc/profile\n```\n\n","slug":"ubuntu1604-搭建Hadoop集群","published":1,"updated":"2020-08-20T08:41:44.645Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0ma8000gewseb968a02x","content":"<h2 id=\"系统环境\"><a href=\"#系统环境\" class=\"headerlink\" title=\"系统环境\"></a>系统环境</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>版本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Ununtu</td>\n<td>1604</td>\n</tr>\n<tr>\n<td>JDK</td>\n<td><a href=\"https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\" target=\"_blank\" rel=\"noopener\">JDK8</a></td>\n</tr>\n<tr>\n<td>Hadoop</td>\n<td><a href=\"https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\" target=\"_blank\" rel=\"noopener\">3.1.3</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>下载JDK8和Hadoop3.1.3，放在ubuntu里Downloads文件夹下</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430165326.png\" alt=\"\"></p>\n<h2 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h2><h3 id=\"设置静态IP\"><a href=\"#设置静态IP\" class=\"headerlink\" title=\"设置静态IP\"></a>设置静态IP</h3><p><a href=\"https://zhishuang.tk/2020/04/30/ubuntu1604-she-zhi-jing-tai-ip/\">ubuntu1604 设置静态IP</a></p>\n<h3 id=\"修改hostname\"><a href=\"#修改hostname\" class=\"headerlink\" title=\"修改hostname\"></a>修改hostname</h3><p>执行命令<code>sudo gedit /etc/hostname</code>,修改hostname为master</p>\n<h3 id=\"配置主机映射\"><a href=\"#配置主机映射\" class=\"headerlink\" title=\"配置主机映射\"></a>配置主机映射</h3><p>执行命令<code>ifconfig</code>获取IP地址</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504220459.png\" alt=\"\"></p>\n<p>执行命令<code>sudo gedit /etc/hosts</code>,添加映射关系，注意IP地址换成自己的</p>\n<pre><code class=\"lang-bash\">192.168.79.129    master\n</code></pre>\n<h3 id=\"关闭防火墙\"><a href=\"#关闭防火墙\" class=\"headerlink\" title=\"关闭防火墙\"></a>关闭防火墙</h3><p>执行命名<code>sudo ufw disable</code>关闭防火墙</p>\n<h3 id=\"配置SSH\"><a href=\"#配置SSH\" class=\"headerlink\" title=\"配置SSH\"></a>配置SSH</h3><blockquote>\n<p>Hadoop名称节点(NameNode)需要启动集群中所有机器的Hadoop守护进程，这个过 程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为 了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们。</p>\n</blockquote>\n<h4 id=\"配置SSH的无密码登录\"><a href=\"#配置SSH的无密码登录\" class=\"headerlink\" title=\"配置SSH的无密码登录\"></a>配置SSH的无密码登录</h4><p>安装openssh-server( 通常Linux系统会默认安装openssh的客户端软件openssh-client)，所以需要自己安装一下服务端。</p>\n<pre><code class=\"lang-bash\">sudo apt-get install openssh-server\n</code></pre>\n<p>在终端执行命令<code>ssh-keygen -t rsa</code>，然后一直回车，生成密钥。</p>\n<p>执行命令<code>ssh-copy-id localhost</code>，在提示信息中输入<code>yes</code>，然后输入本机密码</p>\n<p>执行命令<code>ssh localhost</code>，出现登录时间，说明ssh配置成功</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504234742.png\" alt=\"\"></p>\n<h2 id=\"配置JAVA环境\"><a href=\"#配置JAVA环境\" class=\"headerlink\" title=\"配置JAVA环境\"></a>配置JAVA环境</h2><h3 id=\"创建文件夹\"><a href=\"#创建文件夹\" class=\"headerlink\" title=\"创建文件夹\"></a>创建文件夹</h3><pre><code class=\"lang-shell\">sudo mkdir /usr/java\n</code></pre>\n<p>在/usr目录下创建文件夹java，我们将把JDK8解压到这个文件夹中</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430170954.png\" alt=\"\"></p>\n<h3 id=\"解压JDK\"><a href=\"#解压JDK\" class=\"headerlink\" title=\"解压JDK\"></a>解压JDK</h3><pre><code class=\"lang-bash\">sudo tar -zxvf ~/Downloads/jdk-8u251-linux-x64.tar.gz -C /usr/java\n</code></pre>\n<p>解压jdk8到/usr/java目录下</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430171614.png\" alt=\"\"></p>\n<h3 id=\"配置环境变量\"><a href=\"#配置环境变量\" class=\"headerlink\" title=\"配置环境变量\"></a>配置环境变量</h3><p>执行<code>sudo gedit /etc/profile</code>命令打开配置文件，在末尾添加以下几行文字，注意自己的jdk版本号。</p>\n<pre><code class=\"lang-bash\">#set java env\nexport JAVA_HOME=/usr/java/jdk1.8.0_251\nexport JRE_HOME=${JAVA_HOME}/jre    \nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    \nexport PATH=${JAVA_HOME}/bin:$PATH\n</code></pre>\n<p>执行<code>source /etc/profile</code>命令让配置文件生效</p>\n<p>执行<code>java -version</code>命令验证是否配置成功</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430173142.png\" alt=\"\"></p>\n<h2 id=\"配置Hadoop环境\"><a href=\"#配置Hadoop环境\" class=\"headerlink\" title=\"配置Hadoop环境\"></a>配置Hadoop环境</h2><h3 id=\"解压Hadoop\"><a href=\"#解压Hadoop\" class=\"headerlink\" title=\"解压Hadoop\"></a>解压Hadoop</h3><p>将我们下载的Hadoop解压到 /usr/local/ 中</p>\n<pre><code class=\"lang-bash\">sudo tar -zxvf ~/Downloads/hadoop-3.1.3.tar.gz -C /usr/local\n</code></pre>\n<p>利用<code>cd /usr/local/</code>命令切换操作空间，将文件夹名改为hadoop</p>\n<pre><code class=\"lang-bash\">sudo mv ./hadoop-3.1.3/ ./hadoop\n</code></pre>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430174115.png\" alt=\"\"></p>\n<h3 id=\"配置Hadoop环境变量\"><a href=\"#配置Hadoop环境变量\" class=\"headerlink\" title=\"配置Hadoop环境变量\"></a>配置Hadoop环境变量</h3><p>执行<code>sudo gedit /etc/profile</code>命令打开配置文件,添加下面的语句</p>\n<pre><code class=\"lang-bash\">#set Hadoop env\nHADOOP_HOME=/usr/local/hadoop\nexport PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\n</code></pre>\n<p>执行<code>source /etc/profile</code>命令让配置文件生效</p>\n<h3 id=\"修改Hadoop配置文件\"><a href=\"#修改Hadoop配置文件\" class=\"headerlink\" title=\"修改Hadoop配置文件\"></a>修改Hadoop配置文件</h3><p>执行<code>sudo gedit hadoop-env.sh</code>命令打开配置文件，添加语句<code>export JAVA_HOME=/usr/java/jdk1.8.0_251</code></p>\n<p>执行<code>hadoop version</code>命令验证是否配置成功</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430183459.png\" alt=\"\"></p>\n<p>到这儿 我们Hadoop的单机模式配置完成</p>\n<h2 id=\"Hadoop伪分布式安装配置\"><a href=\"#Hadoop伪分布式安装配置\" class=\"headerlink\" title=\"Hadoop伪分布式安装配置\"></a>Hadoop伪分布式安装配置</h2><blockquote>\n<p>Hadoop可以在单节点上以伪分布式的方式运行，Hadoop进程以分离的 Java 进程来运行，节点既作为 NameNode也作为DataNode， 同时，读取的是 HDFS 中的文件</p>\n<p>Hadoop的配置文件位于$HADOOP_HOME/etc/hadoop/中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml</p>\n<p>Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现</p>\n</blockquote>\n<h3 id=\"hadoop-目录说明\"><a href=\"#hadoop-目录说明\" class=\"headerlink\" title=\"hadoop 目录说明\"></a>hadoop 目录说明</h3><blockquote>\n<p>修改配置文件之前，先看一下hadoop下的目录：</p>\n<ul>\n<li>bin：hadoop最基本的管理脚本和使用脚本所在目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用hadoop</li>\n<li>etc：配置文件存放的目录，包括core-site.xml,hdfs-site.xml,mapred-site.xml等从hadoop1.x继承而来的配置文件和yarn-site.xml等hadoop2.x新增的配置文件</li>\n<li>include：对外提供的编程库头文件（具体动态库和静态库在lib目录中，这些头文件军事用c++定义的，通常用于c++程序访问hdfs或者编写mapreduce程序）</li>\n<li>Lib：该目录包含了hadoop对外提供的才变成动态库和静态库，与include目录中的头文件结合使用</li>\n<li>libexec：各个服务对应的shell配置文件所在目录，可用于配置日志输出目录、启动参数等信息</li>\n<li>sbin：hadoop管理脚本所在目录，主要包含hdfs和yarn中各类服务的启动、关闭脚本</li>\n<li>share：hadoop各个模块编译后的jar包所在目录。</li>\n</ul>\n</blockquote>\n<h3 id=\"修改配置文件\"><a href=\"#修改配置文件\" class=\"headerlink\" title=\"修改配置文件\"></a>修改配置文件</h3><p>在 <code>/usr/local/hadoop/etc/hadoop/</code>目录下，执行命令<code>sudo gedit core-site.xml</code>，修改配置文件core-site.xml，内容如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n &lt;property&gt;\n      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; \n      &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;\n      &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;\n &lt;/property&gt; \n &lt;property&gt;\n   &lt;name&gt;fs.defaultFS&lt;/name&gt;\n   &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; \n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包 括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可</li>\n<li>name为fs.defaultFS的值，表示hdfs路径的逻辑名称</li>\n</ul>\n<p>在 <code>/usr/local/hadoop/etc/hadoop/</code>目录下，执行命令<code>sudo gedit hdfs-site.xml</code>，修改配置文件hdfs-site.xml，内容如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n   &lt;property&gt;\n       &lt;name&gt;dfs.replication&lt;/name&gt;\n       &lt;value&gt;1&lt;/value&gt; \n   &lt;/property&gt; \n   &lt;property&gt;\n       &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; \n    &lt;/property&gt;\n   &lt;property&gt;\n       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;         \n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>dfs.replication表示副本的数量，伪分布式要设置为1</li>\n<li>dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方</li>\n<li>dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方</li>\n</ul>\n<h3 id=\"启动HDFS\"><a href=\"#启动HDFS\" class=\"headerlink\" title=\"启动HDFS\"></a>启动HDFS</h3><p>第一次启动时需要执行 <code>hdfs namenode - format</code>格式化节点</p>\n<p>执行<code>start-all.sh</code>启动HDFS及相关服务，执行<code>jps</code>命令，出现以下6个线程说配置成功：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504235929.png\" alt=\"\"></p>\n<p>进一步确认，可以浏览器访问<code>localhost:9870</code>和<code>localhost:8088</code></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000709.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000753.png\" alt=\"\"></p>\n<h2 id=\"配置Hadoop集群\"><a href=\"#配置Hadoop集群\" class=\"headerlink\" title=\"配置Hadoop集群\"></a>配置Hadoop集群</h2><p>先从master克隆两个虚拟机，分别命名为slave1和slave2。配置方案如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>master</th>\n<th>slave1</th>\n<th>slave2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>HDFS</td>\n<td>NameNode<br>DataNode</td>\n<td>SecondaryNameNode<br>DataNode</td>\n<td>DataNode</td>\n</tr>\n<tr>\n<td>YARN</td>\n<td>NodeManager</td>\n<td>NodeManager</td>\n<td>ResourceManager<br>NodeManager</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"IP，主机名，主机映射配置\"><a href=\"#IP，主机名，主机映射配置\" class=\"headerlink\" title=\"IP，主机名，主机映射配置\"></a>IP，主机名，主机映射配置</h3><h4 id=\"slave1和slave2配置\"><a href=\"#slave1和slave2配置\" class=\"headerlink\" title=\"slave1和slave2配置\"></a>slave1和slave2配置</h4><p>执行<code>sudo gedit /etc/network/interfaces</code>命令打开网络配置文件，修改slave1的IP为192.168.79.130，修改slave2的IP为192.168.79.131</p>\n<p>执行<code>sudo gedit /etc/hostname</code>命令，修改slave1的hostname为slave1，修改slave2的hostname为slave2</p>\n<p>执行<code>sudo gedit /etc/hosts</code>配置主机映射，添加下列语句：</p>\n<pre><code class=\"lang-bash\">#192.168.79.129    master#这条已经存在，不用添加\n192.168.79.130    slave1\n192.168.79.131    slave2\n</code></pre>\n<p>注意，上面的配置需要分别在slave1和slave2上执行，配置结束之后执行<code>reboot</code>重启主机</p>\n<h4 id=\"master配置\"><a href=\"#master配置\" class=\"headerlink\" title=\"master配置\"></a>master配置</h4><p>执行<code>sudo gedit /etc/hosts</code>配置主机映射，添加下列语句</p>\n<pre><code class=\"lang-bash\">192.168.79.130    slave1\n192.168.79.131    slave2\n</code></pre>\n<h3 id=\"免密配置\"><a href=\"#免密配置\" class=\"headerlink\" title=\"免密配置\"></a>免密配置</h3><p>免密配置需要保证三台主机之间都能互相免密登录，<strong>三台主机</strong>都需要<strong>执行</strong>下面的命令</p>\n<pre><code class=\"lang-bash\">ssh-keygen -t rsa    //前面配置过则不需要\nssh-copy-id master    //将公钥文件发送给包括自身在内的3台服务器\nssh-copy-id slave1\nssh-copy-id slave2\n\nssh master    //连接其他服务器看还是否需要密码，三台服务器之间都不需要则配置成功\nssh slave1\nssh slave2\n</code></pre>\n<h3 id=\"配置Hadoop配置文件\"><a href=\"#配置Hadoop配置文件\" class=\"headerlink\" title=\"配置Hadoop配置文件\"></a>配置Hadoop配置文件</h3><p><strong>在master中配置好后再分发给slave1和slave2</strong></p>\n<p>执行<code>cd /usr/local/hadoop/etc/hadoop/</code>进入配置文件所在的文件夹</p>\n<p>执行<code>sudo gedit core-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n &lt;property&gt;\n      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; \n      &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;\n      &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;\n &lt;/property&gt;\n&lt;!-- 指定NameNode的地址 --&gt; \n &lt;property&gt;\n   &lt;name&gt;fs.defaultFS&lt;/name&gt;\n   &lt;value&gt;hdfs://master:9000&lt;/value&gt; \n &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit hdfs-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n   &lt;!-- 指定冗余度 --&gt;\n   &lt;property&gt;\n       &lt;name&gt;dfs.replication&lt;/name&gt;\n       &lt;value&gt;3&lt;/value&gt; \n   &lt;/property&gt;\n   &lt;!-- 指定NameNode数据的存储目录 --&gt; \n   &lt;property&gt;\n       &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; \n   &lt;/property&gt;\n   &lt;!-- 指定Datanode数据的存储目录 --&gt;\n   &lt;property&gt;\n       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;         \n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;!-- 设置secondarynamenode --&gt;\n   &lt;property&gt;\n        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n        &lt;value&gt;slave1:50090&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit yarn-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n   &lt;!--指定mapreduce走shuffle --&gt;\n   &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;!-- 指定ResourceManager的地址--&gt;\n   &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;slave2&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit mapred-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n&lt;!-- 指定mapreduce运行在yarn上 --&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit workers</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-bash\">master\nslave1\nslave2\n</code></pre>\n<p>将配置好的文件分发给slave1和slave2</p>\n<pre><code class=\"lang-bash\">scp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n</code></pre>\n<h3 id=\"启动集群\"><a href=\"#启动集群\" class=\"headerlink\" title=\"启动集群\"></a>启动集群</h3><p>首次运行之前需要格式化namenode，如果首次运行失败后又重新进行了部分配置，运行之前最好也格式化namenode，格式化namenode之前需要删除hadoop目录下的data,logs以及/tmp目录下的hadoop开头的文件（在没有数据的情况下）</p>\n<ul>\n<li><p>在master主机上执行<code>hadoop namenode -format</code>命令格式化namenode</p>\n</li>\n<li><p>在master主机上执行<code>start-dfs.sh</code>命令启动HDFS，如果正常启动：</p>\n<ul>\n<li>在master上执行<code>jps</code>命令可以看到有namenode，datanode</li>\n<li>在slave1上执行<code>jps</code>命令可以看到有secondarynamenode，datanode</li>\n<li>在slave2上执行<code>jps</code>命令可以看到有datanode</li>\n</ul>\n</li>\n<li>因为resourcemanager部署在slave2上，所以在slave2上执行<code>start-yarn.sh</code>命令启动YARN，成功则：<ul>\n<li>在master上执行<code>jps</code>命令可以看到有namenode，datanode，nodemanager</li>\n<li>在slave1上执行<code>jps</code>命令可以看到有secondarynamenode，datanode，nodemanager</li>\n<li>在slave2上执行<code>jps</code>命令可以看到有datanode，resourcemanager，nodemanager</li>\n</ul>\n</li>\n</ul>\n<p>至此完成了集群的搭建</p>\n<h2 id=\"查看集群\"><a href=\"#查看集群\" class=\"headerlink\" title=\"查看集群\"></a>查看集群</h2><p>打开<code>http://master:9870/</code>观测是否有三分datanode</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505191607.png\" alt=\"\"></p>\n<h2 id=\"测试集群\"><a href=\"#测试集群\" class=\"headerlink\" title=\"测试集群\"></a>测试集群</h2><h2 id=\"编写集群启动脚本\"><a href=\"#编写集群启动脚本\" class=\"headerlink\" title=\"编写集群启动脚本\"></a>编写集群启动脚本</h2><p>为了方便启动集群和关闭集群，编写一个脚本，在master机器上执行<code>mkdir bin</code>命令在用户目录下创建<code>bin</code>目录，执行<code>gedit mycluster</code>命令创建并打开文件<code>mycluster</code>,在文件中添加下面的内容：</p>\n<pre><code class=\"lang-bash\">#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo \"No Args Input Error!!!!!\"\n   exit\nfi\ncase $1 in \n\"start\")\n   echo \"======================== start hdfs ========================== \"\n   ssh master \"source /etc/profile;start-dfs.sh\"\n   echo \"======================== start yarn ========================== \"\n   ssh slave2 \"source /etc/profile;start-yarn.sh\"\n;;\n\"stop\")\n   echo \"======================== stop yarn ========================== \"\n   ssh slave2 \"source /etc/profile;stop-yarn.sh\"\n   echo \"======================== stop hdfs ========================== \"\n   ssh master \"source /etc/profile;stop-dfs.sh\"\n;;\n*)\n  echo \"Input Args Error!!!!!\"\n;;\nesac\n</code></pre>\n<p>在<code>bin</code>目录下执行<code>gedit myjps</code>命令创建并打开文件<code>myjps</code>,在文件中添加下面的内容：</p>\n<pre><code class=\"lang-bash\">#!/bin/bash\nfor i in master slave1 slave2\ndo\n   echo \"====================== $i JPS =======================\"\n   ssh $i \"source /etc/profile;jps\"\ndone\n</code></pre>\n<p>执行<code>chmod 777 mycluster</code>和<code>chmod 777 myjps</code>命令修改脚本权限，之后只需要执行<code>mycluster start</code>即可启动集群。</p>\n<h2 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h2><p>由于JAVA，Hadoop等环境变量配置在/etc/profile文件中，导致每次新打开一个命令窗口都要重新输入 source /etc/profile 才能使jdk等配置文件生效：</p>\n<p>解决方法：</p>\n<p>打开~/.bashrc 文件</p>\n<pre><code class=\"lang-bash\">sudo gedit ~/.bashrc\n</code></pre>\n<p>加入下列语句</p>\n<pre><code class=\"lang-bash\">source /etc/profile\n</code></pre>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h2 id=\"系统环境\"><a href=\"#系统环境\" class=\"headerlink\" title=\"系统环境\"></a>系统环境</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>版本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Ununtu</td>\n<td>1604</td>\n</tr>\n<tr>\n<td>JDK</td>\n<td><a href=\"https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\" target=\"_blank\" rel=\"noopener\">JDK8</a></td>\n</tr>\n<tr>\n<td>Hadoop</td>\n<td><a href=\"https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\" target=\"_blank\" rel=\"noopener\">3.1.3</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>下载JDK8和Hadoop3.1.3，放在ubuntu里Downloads文件夹下</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430165326.png\" alt=\"\"></p>\n<h2 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h2><h3 id=\"设置静态IP\"><a href=\"#设置静态IP\" class=\"headerlink\" title=\"设置静态IP\"></a>设置静态IP</h3><p><a href=\"https://zhishuang.tk/2020/04/30/ubuntu1604-she-zhi-jing-tai-ip/\">ubuntu1604 设置静态IP</a></p>\n<h3 id=\"修改hostname\"><a href=\"#修改hostname\" class=\"headerlink\" title=\"修改hostname\"></a>修改hostname</h3><p>执行命令<code>sudo gedit /etc/hostname</code>,修改hostname为master</p>\n<h3 id=\"配置主机映射\"><a href=\"#配置主机映射\" class=\"headerlink\" title=\"配置主机映射\"></a>配置主机映射</h3><p>执行命令<code>ifconfig</code>获取IP地址</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504220459.png\" alt=\"\"></p>\n<p>执行命令<code>sudo gedit /etc/hosts</code>,添加映射关系，注意IP地址换成自己的</p>\n<pre><code class=\"lang-bash\">192.168.79.129    master\n</code></pre>\n<h3 id=\"关闭防火墙\"><a href=\"#关闭防火墙\" class=\"headerlink\" title=\"关闭防火墙\"></a>关闭防火墙</h3><p>执行命名<code>sudo ufw disable</code>关闭防火墙</p>\n<h3 id=\"配置SSH\"><a href=\"#配置SSH\" class=\"headerlink\" title=\"配置SSH\"></a>配置SSH</h3><blockquote>\n<p>Hadoop名称节点(NameNode)需要启动集群中所有机器的Hadoop守护进程，这个过 程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为 了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们。</p>\n</blockquote>\n<h4 id=\"配置SSH的无密码登录\"><a href=\"#配置SSH的无密码登录\" class=\"headerlink\" title=\"配置SSH的无密码登录\"></a>配置SSH的无密码登录</h4><p>安装openssh-server( 通常Linux系统会默认安装openssh的客户端软件openssh-client)，所以需要自己安装一下服务端。</p>\n<pre><code class=\"lang-bash\">sudo apt-get install openssh-server\n</code></pre>\n<p>在终端执行命令<code>ssh-keygen -t rsa</code>，然后一直回车，生成密钥。</p>\n<p>执行命令<code>ssh-copy-id localhost</code>，在提示信息中输入<code>yes</code>，然后输入本机密码</p>\n<p>执行命令<code>ssh localhost</code>，出现登录时间，说明ssh配置成功</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504234742.png\" alt=\"\"></p>\n<h2 id=\"配置JAVA环境\"><a href=\"#配置JAVA环境\" class=\"headerlink\" title=\"配置JAVA环境\"></a>配置JAVA环境</h2><h3 id=\"创建文件夹\"><a href=\"#创建文件夹\" class=\"headerlink\" title=\"创建文件夹\"></a>创建文件夹</h3><pre><code class=\"lang-shell\">sudo mkdir /usr/java\n</code></pre>\n<p>在/usr目录下创建文件夹java，我们将把JDK8解压到这个文件夹中</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430170954.png\" alt=\"\"></p>\n<h3 id=\"解压JDK\"><a href=\"#解压JDK\" class=\"headerlink\" title=\"解压JDK\"></a>解压JDK</h3><pre><code class=\"lang-bash\">sudo tar -zxvf ~/Downloads/jdk-8u251-linux-x64.tar.gz -C /usr/java\n</code></pre>\n<p>解压jdk8到/usr/java目录下</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430171614.png\" alt=\"\"></p>\n<h3 id=\"配置环境变量\"><a href=\"#配置环境变量\" class=\"headerlink\" title=\"配置环境变量\"></a>配置环境变量</h3><p>执行<code>sudo gedit /etc/profile</code>命令打开配置文件，在末尾添加以下几行文字，注意自己的jdk版本号。</p>\n<pre><code class=\"lang-bash\">#set java env\nexport JAVA_HOME=/usr/java/jdk1.8.0_251\nexport JRE_HOME=${JAVA_HOME}/jre    \nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    \nexport PATH=${JAVA_HOME}/bin:$PATH\n</code></pre>\n<p>执行<code>source /etc/profile</code>命令让配置文件生效</p>\n<p>执行<code>java -version</code>命令验证是否配置成功</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430173142.png\" alt=\"\"></p>\n<h2 id=\"配置Hadoop环境\"><a href=\"#配置Hadoop环境\" class=\"headerlink\" title=\"配置Hadoop环境\"></a>配置Hadoop环境</h2><h3 id=\"解压Hadoop\"><a href=\"#解压Hadoop\" class=\"headerlink\" title=\"解压Hadoop\"></a>解压Hadoop</h3><p>将我们下载的Hadoop解压到 /usr/local/ 中</p>\n<pre><code class=\"lang-bash\">sudo tar -zxvf ~/Downloads/hadoop-3.1.3.tar.gz -C /usr/local\n</code></pre>\n<p>利用<code>cd /usr/local/</code>命令切换操作空间，将文件夹名改为hadoop</p>\n<pre><code class=\"lang-bash\">sudo mv ./hadoop-3.1.3/ ./hadoop\n</code></pre>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430174115.png\" alt=\"\"></p>\n<h3 id=\"配置Hadoop环境变量\"><a href=\"#配置Hadoop环境变量\" class=\"headerlink\" title=\"配置Hadoop环境变量\"></a>配置Hadoop环境变量</h3><p>执行<code>sudo gedit /etc/profile</code>命令打开配置文件,添加下面的语句</p>\n<pre><code class=\"lang-bash\">#set Hadoop env\nHADOOP_HOME=/usr/local/hadoop\nexport PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\n</code></pre>\n<p>执行<code>source /etc/profile</code>命令让配置文件生效</p>\n<h3 id=\"修改Hadoop配置文件\"><a href=\"#修改Hadoop配置文件\" class=\"headerlink\" title=\"修改Hadoop配置文件\"></a>修改Hadoop配置文件</h3><p>执行<code>sudo gedit hadoop-env.sh</code>命令打开配置文件，添加语句<code>export JAVA_HOME=/usr/java/jdk1.8.0_251</code></p>\n<p>执行<code>hadoop version</code>命令验证是否配置成功</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430183459.png\" alt=\"\"></p>\n<p>到这儿 我们Hadoop的单机模式配置完成</p>\n<h2 id=\"Hadoop伪分布式安装配置\"><a href=\"#Hadoop伪分布式安装配置\" class=\"headerlink\" title=\"Hadoop伪分布式安装配置\"></a>Hadoop伪分布式安装配置</h2><blockquote>\n<p>Hadoop可以在单节点上以伪分布式的方式运行，Hadoop进程以分离的 Java 进程来运行，节点既作为 NameNode也作为DataNode， 同时，读取的是 HDFS 中的文件</p>\n<p>Hadoop的配置文件位于$HADOOP_HOME/etc/hadoop/中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml</p>\n<p>Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现</p>\n</blockquote>\n<h3 id=\"hadoop-目录说明\"><a href=\"#hadoop-目录说明\" class=\"headerlink\" title=\"hadoop 目录说明\"></a>hadoop 目录说明</h3><blockquote>\n<p>修改配置文件之前，先看一下hadoop下的目录：</p>\n<ul>\n<li>bin：hadoop最基本的管理脚本和使用脚本所在目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用hadoop</li>\n<li>etc：配置文件存放的目录，包括core-site.xml,hdfs-site.xml,mapred-site.xml等从hadoop1.x继承而来的配置文件和yarn-site.xml等hadoop2.x新增的配置文件</li>\n<li>include：对外提供的编程库头文件（具体动态库和静态库在lib目录中，这些头文件军事用c++定义的，通常用于c++程序访问hdfs或者编写mapreduce程序）</li>\n<li>Lib：该目录包含了hadoop对外提供的才变成动态库和静态库，与include目录中的头文件结合使用</li>\n<li>libexec：各个服务对应的shell配置文件所在目录，可用于配置日志输出目录、启动参数等信息</li>\n<li>sbin：hadoop管理脚本所在目录，主要包含hdfs和yarn中各类服务的启动、关闭脚本</li>\n<li>share：hadoop各个模块编译后的jar包所在目录。</li>\n</ul>\n</blockquote>\n<h3 id=\"修改配置文件\"><a href=\"#修改配置文件\" class=\"headerlink\" title=\"修改配置文件\"></a>修改配置文件</h3><p>在 <code>/usr/local/hadoop/etc/hadoop/</code>目录下，执行命令<code>sudo gedit core-site.xml</code>，修改配置文件core-site.xml，内容如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n &lt;property&gt;\n      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; \n      &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;\n      &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;\n &lt;/property&gt; \n &lt;property&gt;\n   &lt;name&gt;fs.defaultFS&lt;/name&gt;\n   &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; \n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包 括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可</li>\n<li>name为fs.defaultFS的值，表示hdfs路径的逻辑名称</li>\n</ul>\n<p>在 <code>/usr/local/hadoop/etc/hadoop/</code>目录下，执行命令<code>sudo gedit hdfs-site.xml</code>，修改配置文件hdfs-site.xml，内容如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n   &lt;property&gt;\n       &lt;name&gt;dfs.replication&lt;/name&gt;\n       &lt;value&gt;1&lt;/value&gt; \n   &lt;/property&gt; \n   &lt;property&gt;\n       &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; \n    &lt;/property&gt;\n   &lt;property&gt;\n       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;         \n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ul>\n<li>dfs.replication表示副本的数量，伪分布式要设置为1</li>\n<li>dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方</li>\n<li>dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方</li>\n</ul>\n<h3 id=\"启动HDFS\"><a href=\"#启动HDFS\" class=\"headerlink\" title=\"启动HDFS\"></a>启动HDFS</h3><p>第一次启动时需要执行 <code>hdfs namenode - format</code>格式化节点</p>\n<p>执行<code>start-all.sh</code>启动HDFS及相关服务，执行<code>jps</code>命令，出现以下6个线程说配置成功：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200504235929.png\" alt=\"\"></p>\n<p>进一步确认，可以浏览器访问<code>localhost:9870</code>和<code>localhost:8088</code></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000709.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505000753.png\" alt=\"\"></p>\n<h2 id=\"配置Hadoop集群\"><a href=\"#配置Hadoop集群\" class=\"headerlink\" title=\"配置Hadoop集群\"></a>配置Hadoop集群</h2><p>先从master克隆两个虚拟机，分别命名为slave1和slave2。配置方案如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>master</th>\n<th>slave1</th>\n<th>slave2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>HDFS</td>\n<td>NameNode<br/>DataNode</td>\n<td>SecondaryNameNode<br/>DataNode</td>\n<td>DataNode</td>\n</tr>\n<tr>\n<td>YARN</td>\n<td>NodeManager</td>\n<td>NodeManager</td>\n<td>ResourceManager<br/>NodeManager</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"IP，主机名，主机映射配置\"><a href=\"#IP，主机名，主机映射配置\" class=\"headerlink\" title=\"IP，主机名，主机映射配置\"></a>IP，主机名，主机映射配置</h3><h4 id=\"slave1和slave2配置\"><a href=\"#slave1和slave2配置\" class=\"headerlink\" title=\"slave1和slave2配置\"></a>slave1和slave2配置</h4><p>执行<code>sudo gedit /etc/network/interfaces</code>命令打开网络配置文件，修改slave1的IP为192.168.79.130，修改slave2的IP为192.168.79.131</p>\n<p>执行<code>sudo gedit /etc/hostname</code>命令，修改slave1的hostname为slave1，修改slave2的hostname为slave2</p>\n<p>执行<code>sudo gedit /etc/hosts</code>配置主机映射，添加下列语句：</p>\n<pre><code class=\"lang-bash\">#192.168.79.129    master#这条已经存在，不用添加\n192.168.79.130    slave1\n192.168.79.131    slave2\n</code></pre>\n<p>注意，上面的配置需要分别在slave1和slave2上执行，配置结束之后执行<code>reboot</code>重启主机</p>\n<h4 id=\"master配置\"><a href=\"#master配置\" class=\"headerlink\" title=\"master配置\"></a>master配置</h4><p>执行<code>sudo gedit /etc/hosts</code>配置主机映射，添加下列语句</p>\n<pre><code class=\"lang-bash\">192.168.79.130    slave1\n192.168.79.131    slave2\n</code></pre>\n<h3 id=\"免密配置\"><a href=\"#免密配置\" class=\"headerlink\" title=\"免密配置\"></a>免密配置</h3><p>免密配置需要保证三台主机之间都能互相免密登录，<strong>三台主机</strong>都需要<strong>执行</strong>下面的命令</p>\n<pre><code class=\"lang-bash\">ssh-keygen -t rsa    //前面配置过则不需要\nssh-copy-id master    //将公钥文件发送给包括自身在内的3台服务器\nssh-copy-id slave1\nssh-copy-id slave2\n\nssh master    //连接其他服务器看还是否需要密码，三台服务器之间都不需要则配置成功\nssh slave1\nssh slave2\n</code></pre>\n<h3 id=\"配置Hadoop配置文件\"><a href=\"#配置Hadoop配置文件\" class=\"headerlink\" title=\"配置Hadoop配置文件\"></a>配置Hadoop配置文件</h3><p><strong>在master中配置好后再分发给slave1和slave2</strong></p>\n<p>执行<code>cd /usr/local/hadoop/etc/hadoop/</code>进入配置文件所在的文件夹</p>\n<p>执行<code>sudo gedit core-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n &lt;property&gt;\n      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; \n      &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;\n      &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;\n &lt;/property&gt;\n&lt;!-- 指定NameNode的地址 --&gt; \n &lt;property&gt;\n   &lt;name&gt;fs.defaultFS&lt;/name&gt;\n   &lt;value&gt;hdfs://master:9000&lt;/value&gt; \n &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit hdfs-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt; \n   &lt;!-- 指定冗余度 --&gt;\n   &lt;property&gt;\n       &lt;name&gt;dfs.replication&lt;/name&gt;\n       &lt;value&gt;3&lt;/value&gt; \n   &lt;/property&gt;\n   &lt;!-- 指定NameNode数据的存储目录 --&gt; \n   &lt;property&gt;\n       &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; \n   &lt;/property&gt;\n   &lt;!-- 指定Datanode数据的存储目录 --&gt;\n   &lt;property&gt;\n       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;         \n       &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;!-- 设置secondarynamenode --&gt;\n   &lt;property&gt;\n        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n        &lt;value&gt;slave1:50090&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit yarn-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n   &lt;!--指定mapreduce走shuffle --&gt;\n   &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n   &lt;/property&gt;\n   &lt;!-- 指定ResourceManager的地址--&gt;\n   &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;slave2&lt;/value&gt;\n   &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit mapred-site.xml</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-xml\">&lt;configuration&gt;\n&lt;!-- 指定mapreduce运行在yarn上 --&gt;\n&lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>执行<code>sudo gedit workers</code>打开配置文件，配置如下：</p>\n<pre><code class=\"lang-bash\">master\nslave1\nslave2\n</code></pre>\n<p>将配置好的文件分发给slave1和slave2</p>\n<pre><code class=\"lang-bash\">scp -r /usr/local/hadoop/etc/hadoop/ slave1:/usr/local/hadoop/etc/\nscp -r /usr/local/hadoop/etc/hadoop/ slave2:/usr/local/hadoop/etc/\n</code></pre>\n<h3 id=\"启动集群\"><a href=\"#启动集群\" class=\"headerlink\" title=\"启动集群\"></a>启动集群</h3><p>首次运行之前需要格式化namenode，如果首次运行失败后又重新进行了部分配置，运行之前最好也格式化namenode，格式化namenode之前需要删除hadoop目录下的data,logs以及/tmp目录下的hadoop开头的文件（在没有数据的情况下）</p>\n<ul>\n<li><p>在master主机上执行<code>hadoop namenode -format</code>命令格式化namenode</p>\n</li>\n<li><p>在master主机上执行<code>start-dfs.sh</code>命令启动HDFS，如果正常启动：</p>\n<ul>\n<li>在master上执行<code>jps</code>命令可以看到有namenode，datanode</li>\n<li>在slave1上执行<code>jps</code>命令可以看到有secondarynamenode，datanode</li>\n<li>在slave2上执行<code>jps</code>命令可以看到有datanode</li>\n</ul>\n</li>\n<li>因为resourcemanager部署在slave2上，所以在slave2上执行<code>start-yarn.sh</code>命令启动YARN，成功则：<ul>\n<li>在master上执行<code>jps</code>命令可以看到有namenode，datanode，nodemanager</li>\n<li>在slave1上执行<code>jps</code>命令可以看到有secondarynamenode，datanode，nodemanager</li>\n<li>在slave2上执行<code>jps</code>命令可以看到有datanode，resourcemanager，nodemanager</li>\n</ul>\n</li>\n</ul>\n<p>至此完成了集群的搭建</p>\n<h2 id=\"查看集群\"><a href=\"#查看集群\" class=\"headerlink\" title=\"查看集群\"></a>查看集群</h2><p>打开<code>http://master:9870/</code>观测是否有三分datanode</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200505191607.png\" alt=\"\"></p>\n<h2 id=\"测试集群\"><a href=\"#测试集群\" class=\"headerlink\" title=\"测试集群\"></a>测试集群</h2><h2 id=\"编写集群启动脚本\"><a href=\"#编写集群启动脚本\" class=\"headerlink\" title=\"编写集群启动脚本\"></a>编写集群启动脚本</h2><p>为了方便启动集群和关闭集群，编写一个脚本，在master机器上执行<code>mkdir bin</code>命令在用户目录下创建<code>bin</code>目录，执行<code>gedit mycluster</code>命令创建并打开文件<code>mycluster</code>,在文件中添加下面的内容：</p>\n<pre><code class=\"lang-bash\">#!/bin/bash\nif [ $# -lt 1 ]\n then \n   echo &quot;No Args Input Error!!!!!&quot;\n   exit\nfi\ncase $1 in \n&quot;start&quot;)\n   echo &quot;======================== start hdfs ========================== &quot;\n   ssh master &quot;source /etc/profile;start-dfs.sh&quot;\n   echo &quot;======================== start yarn ========================== &quot;\n   ssh slave2 &quot;source /etc/profile;start-yarn.sh&quot;\n;;\n&quot;stop&quot;)\n   echo &quot;======================== stop yarn ========================== &quot;\n   ssh slave2 &quot;source /etc/profile;stop-yarn.sh&quot;\n   echo &quot;======================== stop hdfs ========================== &quot;\n   ssh master &quot;source /etc/profile;stop-dfs.sh&quot;\n;;\n*)\n  echo &quot;Input Args Error!!!!!&quot;\n;;\nesac\n</code></pre>\n<p>在<code>bin</code>目录下执行<code>gedit myjps</code>命令创建并打开文件<code>myjps</code>,在文件中添加下面的内容：</p>\n<pre><code class=\"lang-bash\">#!/bin/bash\nfor i in master slave1 slave2\ndo\n   echo &quot;====================== $i JPS =======================&quot;\n   ssh $i &quot;source /etc/profile;jps&quot;\ndone\n</code></pre>\n<p>执行<code>chmod 777 mycluster</code>和<code>chmod 777 myjps</code>命令修改脚本权限，之后只需要执行<code>mycluster start</code>即可启动集群。</p>\n<h2 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h2><p>由于JAVA，Hadoop等环境变量配置在/etc/profile文件中，导致每次新打开一个命令窗口都要重新输入 source /etc/profile 才能使jdk等配置文件生效：</p>\n<p>解决方法：</p>\n<p>打开~/.bashrc 文件</p>\n<pre><code class=\"lang-bash\">sudo gedit ~/.bashrc\n</code></pre>\n<p>加入下列语句</p>\n<pre><code class=\"lang-bash\">source /etc/profile\n</code></pre>\n"},{"title":"ubuntu1604 设置静态IP","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-04-30T07:29:44.000Z","password":null,"summary":null,"img":null,"keywords":"IP 静态IP Ubuntu","_content":"\n1. 首先确保是NAT连接模式\n\n2. 在VMware Workstation的“编辑”选项找到虚拟网络编辑器，在虚拟网络编辑器面板中找到NAT设置，获取子网掩码和网关。\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430154842.png)\n\n3. 在虚拟机中打开终端，输入命令`ifconfig`查看网卡信息\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430160112.png)\n\n4. 输入命令`sudo gedit /etc/network/interfaces`手动配置网络\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161332.png)\n\n5. 输入命令`reboot`重启虚拟机\n\n6. 重启后输入命令`ifconfig`可看到IP地址变成了我们手动设置的IP\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161920.png)\n\n7. 改好了呢","source":"_posts/ubuntu1604-设置静态IP.md","raw":"---\ntitle: ubuntu1604 设置静态IP\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-04-30 15:29:44\npassword:\nsummary:\ncategories: Linux\nimg:\nkeywords: IP 静态IP Ubuntu\ntags:\n\t- IP\n\t- 静态IP\n\t- Ubuntu\n---\n\n1. 首先确保是NAT连接模式\n\n2. 在VMware Workstation的“编辑”选项找到虚拟网络编辑器，在虚拟网络编辑器面板中找到NAT设置，获取子网掩码和网关。\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430154842.png)\n\n3. 在虚拟机中打开终端，输入命令`ifconfig`查看网卡信息\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430160112.png)\n\n4. 输入命令`sudo gedit /etc/network/interfaces`手动配置网络\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161332.png)\n\n5. 输入命令`reboot`重启虚拟机\n\n6. 重启后输入命令`ifconfig`可看到IP地址变成了我们手动设置的IP\n\n\t![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161920.png)\n\n7. 改好了呢","slug":"ubuntu1604-设置静态IP","published":1,"updated":"2020-08-20T08:41:44.646Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mae000kewsebf9v0vqr","content":"<ol>\n<li><p>首先确保是NAT连接模式</p>\n</li>\n<li><p>在VMware Workstation的“编辑”选项找到虚拟网络编辑器，在虚拟网络编辑器面板中找到NAT设置，获取子网掩码和网关。</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430154842.png\" alt=\"\"></p>\n</li>\n<li><p>在虚拟机中打开终端，输入命令<code>ifconfig</code>查看网卡信息</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430160112.png\" alt=\"\"></p>\n</li>\n<li><p>输入命令<code>sudo gedit /etc/network/interfaces</code>手动配置网络</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161332.png\" alt=\"\"></p>\n</li>\n<li><p>输入命令<code>reboot</code>重启虚拟机</p>\n</li>\n<li><p>重启后输入命令<code>ifconfig</code>可看到IP地址变成了我们手动设置的IP</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161920.png\" alt=\"\"></p>\n</li>\n<li><p>改好了呢</p>\n</li>\n</ol>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<ol>\n<li><p>首先确保是NAT连接模式</p>\n</li>\n<li><p>在VMware Workstation的“编辑”选项找到虚拟网络编辑器，在虚拟网络编辑器面板中找到NAT设置，获取子网掩码和网关。</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430154842.png\" alt=\"\"></p>\n</li>\n<li><p>在虚拟机中打开终端，输入命令<code>ifconfig</code>查看网卡信息</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430160112.png\" alt=\"\"></p>\n</li>\n<li><p>输入命令<code>sudo gedit /etc/network/interfaces</code>手动配置网络</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161332.png\" alt=\"\"></p>\n</li>\n<li><p>输入命令<code>reboot</code>重启虚拟机</p>\n</li>\n<li><p>重启后输入命令<code>ifconfig</code>可看到IP地址变成了我们手动设置的IP</p>\n<p> <img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430161920.png\" alt=\"\"></p>\n</li>\n<li><p>改好了呢</p>\n</li>\n</ol>\n"},{"title":"【求甚解】怎样评价一个模型的泛化能力","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2020-05-03T14:34:59.000Z","password":null,"summary":null,"img":null,"keywords":"求甚解","_content":"\n一般来说通过泛化误差来评价一个模型的泛化能力。那什么是泛化误差呢？所谓的泛化误差就是用来衡量一个学习机器推广未知数据的能力，即根据从样本数据中学习到的规则能够应用到新数据的能力。\n\n泛化误差也称作期望误差，与之相对的有一个经验误差，经验误差就是模型在训练数据集上的误差，泛化误差就是模型在所有数据（不仅仅是测试数据）上的误差。泛化误差和经验误差中的误差指的是模型的标签与真实标签之间的误差。\n\n理解了概念，我们来看一下数学形式，以回归问题为例（结论适用于其他模型），我们假设样本的真实分布为$P_r(𝒙,𝑦)$(即包含所有的数据)，数据集$\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$是真实分布中数据集的一个子集，并采用平方损失函数，模型$f_\\mathcal{D}(x)$的期望误差为：\n$$\nErr(f)=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[(y-f_\\mathcal{D}(x))^{2}\\right]\n$$\n\n模型$f_\\mathcal{D}(x)$的经验误差为：\n$$\nErr(f)=\\frac{1}{N} \\sum_{n=1}^{N}\\left[(y_n-f_\\mathcal{D}(x_n))^{2}\\right]\n$$\n其中$\\mathbb{E}$表示求期望，$f_\\mathcal{D}(x)$表示在训练集$\\mathcal{D}$上训练得到的模型，$(x,y)\\sim P_r(x,y)$表示$(x,y)$服从$P_r(x,y)$分布，即所有的数据。$(y-f_\\mathcal{D}(x))^{2}$表示真实标签与模型预测标签的平方损失（忽略了系数1/2）。\n\n假定$f^*(x)$是假设空间中的最优模型，则期望误差可分解为\n$$\n\\begin{aligned}\nErr(f) &=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[\\left(y-f^*(x)+f^*(x)-f(x)\\right)^{2}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{r}(x)}\\left[\\left(f(x)-f^{*}(x)\\right)^{2}\\right]+\\epsilon\n\\end{aligned}\n$$\n\n\n\n\n","source":"_posts/【求甚解】怎样评价一个模型的泛化能力.md","raw":"---\ntitle: 【求甚解】怎样评价一个模型的泛化能力\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2020-05-03 22:34:59\npassword:\nsummary:\ncategories: 求甚解\nimg:\nkeywords: 求甚解\ntags:\n\t- 求甚解\n---\n\n一般来说通过泛化误差来评价一个模型的泛化能力。那什么是泛化误差呢？所谓的泛化误差就是用来衡量一个学习机器推广未知数据的能力，即根据从样本数据中学习到的规则能够应用到新数据的能力。\n\n泛化误差也称作期望误差，与之相对的有一个经验误差，经验误差就是模型在训练数据集上的误差，泛化误差就是模型在所有数据（不仅仅是测试数据）上的误差。泛化误差和经验误差中的误差指的是模型的标签与真实标签之间的误差。\n\n理解了概念，我们来看一下数学形式，以回归问题为例（结论适用于其他模型），我们假设样本的真实分布为$P_r(𝒙,𝑦)$(即包含所有的数据)，数据集$\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$是真实分布中数据集的一个子集，并采用平方损失函数，模型$f_\\mathcal{D}(x)$的期望误差为：\n$$\nErr(f)=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[(y-f_\\mathcal{D}(x))^{2}\\right]\n$$\n\n模型$f_\\mathcal{D}(x)$的经验误差为：\n$$\nErr(f)=\\frac{1}{N} \\sum_{n=1}^{N}\\left[(y_n-f_\\mathcal{D}(x_n))^{2}\\right]\n$$\n其中$\\mathbb{E}$表示求期望，$f_\\mathcal{D}(x)$表示在训练集$\\mathcal{D}$上训练得到的模型，$(x,y)\\sim P_r(x,y)$表示$(x,y)$服从$P_r(x,y)$分布，即所有的数据。$(y-f_\\mathcal{D}(x))^{2}$表示真实标签与模型预测标签的平方损失（忽略了系数1/2）。\n\n假定$f^*(x)$是假设空间中的最优模型，则期望误差可分解为\n$$\n\\begin{aligned}\nErr(f) &=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[\\left(y-f^*(x)+f^*(x)-f(x)\\right)^{2}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{r}(x)}\\left[\\left(f(x)-f^{*}(x)\\right)^{2}\\right]+\\epsilon\n\\end{aligned}\n$$\n\n\n\n\n","slug":"【求甚解】怎样评价一个模型的泛化能力","published":1,"updated":"2020-08-20T08:41:44.648Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mag000newsegiov5d5k","content":"<p>一般来说通过泛化误差来评价一个模型的泛化能力。那什么是泛化误差呢？所谓的泛化误差就是用来衡量一个学习机器推广未知数据的能力，即根据从样本数据中学习到的规则能够应用到新数据的能力。</p>\n<p>泛化误差也称作期望误差，与之相对的有一个经验误差，经验误差就是模型在训练数据集上的误差，泛化误差就是模型在所有数据（不仅仅是测试数据）上的误差。泛化误差和经验误差中的误差指的是模型的标签与真实标签之间的误差。</p>\n<p>理解了概念，我们来看一下数学形式，以回归问题为例（结论适用于其他模型），我们假设样本的真实分布为$P_r(𝒙,𝑦)$(即包含所有的数据)，数据集$\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$是真实分布中数据集的一个子集，并采用平方损失函数，模型$f_\\mathcal{D}(x)$的期望误差为：</p>\n<script type=\"math/tex; mode=display\">\nErr(f)=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[(y-f_\\mathcal{D}(x))^{2}\\right]</script><p>模型$f_\\mathcal{D}(x)$的经验误差为：</p>\n<script type=\"math/tex; mode=display\">\nErr(f)=\\frac{1}{N} \\sum_{n=1}^{N}\\left[(y_n-f_\\mathcal{D}(x_n))^{2}\\right]</script><p>其中$\\mathbb{E}$表示求期望，$f_\\mathcal{D}(x)$表示在训练集$\\mathcal{D}$上训练得到的模型，$(x,y)\\sim P_r(x,y)$表示$(x,y)$服从$P_r(x,y)$分布，即所有的数据。$(y-f_\\mathcal{D}(x))^{2}$表示真实标签与模型预测标签的平方损失（忽略了系数1/2）。</p>\n<p>假定$f^*(x)$是假设空间中的最优模型，则期望误差可分解为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nErr(f) &=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[\\left(y-f^*(x)+f^*(x)-f(x)\\right)^{2}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{r}(x)}\\left[\\left(f(x)-f^{*}(x)\\right)^{2}\\right]+\\epsilon\n\\end{aligned}</script><script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>一般来说通过泛化误差来评价一个模型的泛化能力。那什么是泛化误差呢？所谓的泛化误差就是用来衡量一个学习机器推广未知数据的能力，即根据从样本数据中学习到的规则能够应用到新数据的能力。</p>\n<p>泛化误差也称作期望误差，与之相对的有一个经验误差，经验误差就是模型在训练数据集上的误差，泛化误差就是模型在所有数据（不仅仅是测试数据）上的误差。泛化误差和经验误差中的误差指的是模型的标签与真实标签之间的误差。</p>\n<p>理解了概念，我们来看一下数学形式，以回归问题为例（结论适用于其他模型），我们假设样本的真实分布为$P_r(𝒙,𝑦)$(即包含所有的数据)，数据集$\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$是真实分布中数据集的一个子集，并采用平方损失函数，模型$f_\\mathcal{D}(x)$的期望误差为：</p>\n<script type=\"math/tex; mode=display\">\nErr(f)=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[(y-f_\\mathcal{D}(x))^{2}\\right]</script><p>模型$f_\\mathcal{D}(x)$的经验误差为：</p>\n<script type=\"math/tex; mode=display\">\nErr(f)=\\frac{1}{N} \\sum_{n=1}^{N}\\left[(y_n-f_\\mathcal{D}(x_n))^{2}\\right]</script><p>其中$\\mathbb{E}$表示求期望，$f_\\mathcal{D}(x)$表示在训练集$\\mathcal{D}$上训练得到的模型，$(x,y)\\sim P_r(x,y)$表示$(x,y)$服从$P_r(x,y)$分布，即所有的数据。$(y-f_\\mathcal{D}(x))^{2}$表示真实标签与模型预测标签的平方损失（忽略了系数1/2）。</p>\n<p>假定$f^*(x)$是假设空间中的最优模型，则期望误差可分解为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nErr(f) &=\\mathbb{E}_{(x, y) \\sim p_{r}(x, y)}\\left[\\left(y-f^*(x)+f^*(x)-f(x)\\right)^{2}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{r}(x)}\\left[\\left(f(x)-f^{*}(x)\\right)^{2}\\right]+\\epsilon\n\\end{aligned}</script>"},{"title":"【文献翻译】基于深度学习的文本分类：全面回顾","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-04-27T12:42:51.000Z","password":null,"summary":null,"img":null,"keywords":"文本分类 综述 文献翻译","_content":"\n### 摘要\n\n基于深度学习的模型已经在各种文本分类任务中超过了经典的基于机器学的方法，例如情感分析，新闻分类，问答以及自然语言处理。在这次工作中，我们对近些年开发的150多种基于深度学习的文本分类模型进行了详尽的回顾，讨论了他们的技术贡献，相似点以及优点。我们还对广泛应用于文本分类的40多个流行数据集进行了总结。最后，我们对不同的深度学习模型在**流行基准**上的性能进行了定量分析。\n\n### Introduction\n\n文本分类是自然语言处理中的一个经典问题，旨在为文本单元（例如句子，**询问**，段落和文档）分配标签。文本分类有十分广泛的应用，例如问答，垃圾邮件检测，情感分析，新闻分类，用户意图识别，内容审核等等。文本数据可以来自不同的数据源，例如网页数据，邮件，聊天，社交媒体，机票，保险理赔，用户评论，客户服务中的问题和解答等等。文本中含有极其丰富的信息，但由于它的非结构化特征，想要从中提取信息便极具挑战和耗时。\n\n文本分类可以通过人工标注和自动标注两种方式进行，随着工业应用中文本数据规模不断增大，自动文本分类变得越来越重要。自动文本分类的方法可以被分为3类：\n\n* 基于规则的方法\n* 基于机器学习的方法（数据驱动）\n* 混合方法\n\n基于规则的方法使用一组预定义的规则将文本分为不同的类别。例如，所有包含“足球”，”篮球“或者”棒球“的文档都被标记为”运动“标签。\n\n### 2 用于文本分类的深度学习模型\n\n在这个部分，我们回顾了针对各种文本分类问题提出的150多种深度学习框架。为了更易于遵循，我们根据模型的主要架构贡献将其分为以下类别：\n\n* 基于前馈网络的模型，该模型将文本视为一堆单词（a bag of words）（第2.1节）\n* 基于RNN的模型，该模型将文本视为单词序列，旨在捕获单词相关性和文本结构（第2.2节）\n* 基于CNN的模型，经过训练可识别文本中的模式（例如关键短语）以进行分类（第2.3节）\n* 胶囊网络(Capsule networks)解决了CNN的池化操作所带来的信息丢失问题，最近已应用于文本分类（第2.4节）\n* 注意机制(Attention mechanism)可有效识别文本中的相关单词，并已成为开发深度学习模型的有用工具（第2.5节）\n* 记忆增强网络(Memory-augmented networks)，将神经网络与外部记忆形式结合在一起，模型可以从中读取和写入（第2.6节）\n* Transformers，允许比RNN更多的并行化，因此可以使用GPU集群有效地（预）训练非常大的语言模型（第2.7节）\n* 图神经网络(Graph neural networks)，旨在捕获自然语言的内部图结构，例如句法和语义解析树（第2.8节）\n* 孪生神经网络(Siamese Neural Networks)，用于文本匹配，文本匹配是文本分类的一种特殊情况（第2.9节）\n* 混合模型(Hybrid models)，结合注意力，RNN，CNN等模型来捕获句子和文档的局部和全局特征（第2.10节）\n* 最后，在2.11节中，我们回顾了有监督学习之外的建模技术，包括使用自动编码器(Autoencoder)和对抗训练(Adversarial training)的无监督学习(Unsupervised learning)，以及强化学习(Reinforcement learning)\n\n### 2.1 前馈神经网络\n\n前馈网络是用于文本表示的最简单的深度学习模型之一。但是，它们已经在许多文本分类基准上达到了很高的准确性。 这些模型将文本视为一堆单词。对于每个单词，他们使用诸如word2vec [8]或Glove [9]之类的嵌入模型学习向量表示，将嵌入向量的和或平均值作为文本的表示，将其通过一个或多个前馈层，称为多层感知器（MLP），然后使用诸如逻辑回归、朴素贝叶斯或SVM等分类器对最终层的表示进行分类[10]。一个例子如深度平均网络（Deep Average Network，DAN）[10]，其体系结构如图1所示。尽管简单，但DAN却胜过了其他更复杂的模型，这些模型旨在显式地学习文本的组成。例如，DAN在语法差异较大的数据集上的表现优于语法模型。Joulin等[11]提出了一种简单而有效的文本分类器，称为fastText。 像DAN一样，fastText将文本视为一堆单词，与DAN不同的是，fastText使用n-gram作为附加特征来捕获局部单词顺序信息。 事实证明，这在实践中非常有效，同时可达到与显式使用单词顺序的方法[12]相当的结果。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200428171503.png)\n\nLe和Mikolov [13]提出了doc2vec，它使用一种无监督算法来学习可变长度文本（例如句子，段落和文档）的定长特征表示。如图2所示，doc2vec的体系结构类似于连续词袋（CBOW）模型的体系结构[8，14]。唯一的区别是附加的段落标记通过矩阵D映射到段落向量。在doc2vec中，此向量与三个单词的上下文的连接或平均值用于预测第四个单词。 段落向量表示当前上下文中丢失的信息，可以用作该段落的主题记忆。经过训练后，段落向量将用作段落的特征并送入分类器进行预测。  Doc2vec在发布时，在一些文本分类和情感分析任务上获得了最优结果。\n\n### 2.2 基于RNN的模型\n\n基于RNN的模型将文本视为一系列单词，旨在捕获单词依赖性和文本结构以进行文本分类。但是，普通的RNN(vanilla RNN)模型不能很好地工作，并且通常表现不如前馈神经网络。 在RNN的许多变体中，长短期记忆网络（LSTM）是最受欢迎的结构，旨在更好地捕获长期依赖关系。LSTM通过引入存储单元以记住任意时间间隔的值以及三个门（输入门，输出门，遗忘门）来调节信息的流动，从而解决了普通RNN遇到的梯度消失或爆炸问题。已经有工作通过捕获更丰富的信息（例如自然语言的树结构，文本中的大跨度单词关系，文档主题等）来改进用于文本分类的RNN和LSTM模型。\n\nTai等[15]已经开发了Tree-LSTM模型，将LSTM推广到树结构网络类型，以学习丰富的语义表示。作者认为，针对自然语言处理任务，Tree-LSTM比链结构LSTM更好，因为自然语言具有句法属性，可以自然地将单词和短语组合在一起。他们在两个任务上验证了Tree-LSTM的有效性：情感分类和预测两个句子的语义相关性。这些模型的架构如图3所示。 [16]通过使用存储单元在递归过程中存储多个子单元或多个后代单元的历史将chain-structured LSTM展到树状结构。他们认为，新模型提供了一种原则上的方法，可以考虑在层次结构（例如语言或图像解析结构）上进行长距离交互。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430103448.png)\n\n为了对机器学习的大跨度单词关系进行建模，Cheng等人[17]用一个存储网络代替单个存储单元来增强LSTM体系结构。这可以在神经注意力复发期间启用自适应内存使用，从而提供一种弱化标记之间关系的方法。 该模型在语言建模，情感分析和NLI上取得了可喜的结果。\n\n多时标LSTM（MT-LSTM）神经网络[18]被设计为通过捕获具有不同时标的有价值的信息来对长文本（例如句子和文档）建模。MT-LSTM将标准LSTM模型的隐藏状态分为几组。 每个组在不同的时间段被激活和更新。 因此，MT-LSTM可以对很长的文档进行建模。MT-LSTM在文本分类方面优于包括基于LSTM和RNN的模型在内的基准。\n\nRNN擅长捕获单词序列的局部结构，但是面对远距离依赖关系会有点力不从心。相反，潜在主题模型（latent topic models）能够捕获文档的全局语义结构，但不考虑单词顺序。Bieng等 [19]提出了TopicRNN模型，以整合RNN和潜在主题模型的优点。 它使用RNN捕获局部（语法）依赖性，并使用潜在主题捕获全局（语义）依赖性。 TopicRNN在情感分析方面优于RNN基线。\n\n还有其他有趣的基于RNN的模型。 刘等[20]使用多任务学习来训练RNN，以利用来自多个相关任务的标记训练数据。Johnson和Rie [21]探索了使用LSTM的文本区域嵌入方法。周等 [22]集成了双向LSTM（Bi-LSTM）模型和二维最大池来捕获文本特征。Wang等[23]提出了在“matching-aggregation”框架下的双边多视角匹配模型。  Wan等[24]使用双向LSMT模型生成的多个位置句子表示来探索语义匹配。\n\n### 2.3 基于CNN的模型\n\n训练RNN识别跨时间的模式，而CNN学会识别跨空间的模式[25]。在需要理解远程语义的POS标签或QA等NLP任务中，RNN效果很好，而在检测局部和位置不变模式很重要的情况下，CNN效果很好。这些模式可能是表达特定情绪（例如“我喜欢”）或主题（例如“濒危物种”）的关键短语。 因此，CNN已成为最受欢迎的文本分类模型体系结构之一。\n\nKalchbrenner等人提出了最早的基于CNN的文本分类模型之一[26]。 该模型使用动态k-max池，称为动态CNN（DCNN）。如图4所示，DCNN的第一层使用对句子中每个单词的嵌入来构造句子矩阵。 然后使用将宽卷积层与动态k-max池给定的动态池层交替的卷积体系结构来生成句子的特征映射，该特征映射能够显式捕获单词和短语的短时和长时关系。可以根据句子大小和卷积层次结构的级别来动态选择池化参数k。\n\n后来，Kim [27]提出了一种比DCNN简单得多的基于CNN的模型，用于文本分类。 如图5所示，Kim的模型仅在从无监督神经语言模型（即word2vec）获得的单词向量上使用一层卷积。Kim还比较了四种学习单词嵌入的方法：\n\n* CNN-rand，其中所有单词嵌入都在训练过程中被随机初始化，然后进行修改\n* CNN-static，在模型训练期间使用预训练的word2vec嵌入并保持固定\n* CNN-non-static，其中word2vec嵌入在针对每个任务的训练过程中进行了微调\n* CNN-multi-channel，其中使用了两组词嵌入向量集，都使用word2vec进行了初始化，其中一个在模型训练期间进行了更新，而另一个则在固定的情况下进行了更新。\n\n这些基于CNN的模型将改进情感分析和问题分类的SOTA。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164349.png)\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164600.png)\n\n[26，27]已经做出了一些努力来改进基于CNN模型的体系结构。刘等[28]提出了一种新的基于CNN的模型，该模型对TextCNN [27]的体系结构进行了两次修改。首先，采用动态最大池化方案来从文档的不同区域捕获更多细粒度的特征。其次，在池化层和输出层之间插入一个隐藏的瓶颈层（bottleneck layer）学习紧凑的文档表示形式，以减小模型大小并提高模型性能。在[29，30]中，作者没有使用预先训练的低维词向量作为CNN的输入，而是直接将CNN应用于高维文本数据，以学习小文本区域的嵌入进行分类。\n\n字符级的CNN也已经被用于文本分类[31，32]。Zhang等人提出了最早的此类模型之一[31]。如图6所示，该模型以固定大小的字符作为输入，将其编码为一个one-hot向量，然后将它们通过一个深CNN模型，该模型由具有池化操作的六个卷积层和三个全连接层组成。Prusa等[33]提出了一种使用CNN编码文本的方法，该方法大大减少了学习字符级文本表示所需的内存消耗和训练时间。此方法可根据字母大小很好地缩放，从而可以保留原始文本中的更多信息以增强分类性能\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164852.png)\n\n有研究调查词嵌入和CNN架构对模型性能的影响。受到VGG [34]和ResNets [35]的启发，Conneau等人 [36]提出了一种非常深的CNN（VDCNN）模型用于文本处理。它直接在字符级别上操作，并且仅使用小的卷积和池化操作。 研究表明，VDCNN的性能随着深度的增加而增加。杜克等[37]修改了VDCNN的结构，以适应移动平台的限制并保持性能。他们能够将模型大小压缩10倍至20倍，而精度损失在0.4％至1.3％之间。Le等[38]表明，当文本输入表示为字符序列时，深层模型确实优于浅层模型。但是，一个简单的浅层和广域网络在词输入方面胜过DenseNet [39]等深层模型。郭等[40]研究了词嵌入的影响，并提出通过多通道CNN模型使用加权词嵌入。张等[41]研究了不同词嵌入方法和池化机制的影响，发现使用非静态word2vec和GloVe优于one-hot向量，并且最大池化始终优于其他池化方法。\n\n还有其他有趣的基于CNN的模型。Mou等[42]提出了一种基于树的CNN来捕获句子级语义。庞等[43]将文本匹配转换为图像识别任务，并使用多层CNN识别显著n-gram模式。Wang等[44]提出了一种基于CNN的模型，该模型结合了短文本的显式和隐式表示形式进行分类。 将CNN应用于生物医学文本分类的兴趣也越来越高[45-48]。\n\n### 2.4 胶囊网络\n\nCNN通过使用连续的卷积和池化层对图像或文本进行分类。 尽管池化操作可识别显著特征并降低卷积操作的计算复杂性，但它们会丢失有关空间关系的信息，并可能根据其方向或比例对实体进行错误分类\n\n为了解决池化带来的问题，Geoffrey Hinton提出了一种新方法，称为胶囊网络（CapsNets）[49，50]。一个胶囊是一组神经元，其活动向量代表特定类型的实体（例如对象或对象部分）的不同属性。向量的长度代表实体存在的概率，向量的方向代表实体的属性。与CNN的最大池化（选择一些信息并丢弃其余信息）不同，胶囊使用网络中直到最后一层的所有可用信息，将底层的每个胶囊“路由”到上层最匹配的父胶囊。可以使用不同的算法来实现路由，例如协议动态路由[50]或EM算法[51]。\n\n近来，胶囊网络已经被应用于文本分类，其中胶囊适于将句子或文档表示为向量。  [52–54]提出了一种基于CapsNets变体的文本分类模型。该模型由四层组成：（1）n-gram卷积层，（2）胶囊层，（3）卷积胶囊层，以及（4）完全连接的胶囊层。 作者尝试了三种策略来稳定动态路由过程，以减轻包含背景信息（例如停用词或与任何文档类别无关的词）的噪声包的干扰。 他们还探索了两种胶囊架构，如图7所示。分别为Capsule-A和Capsule-B。Capsule-A与[50]中的CapsNet类似。  Capsule-B使用三个并行网络，并在n-gram卷积层中使用具有不同窗口大小的过滤器，以学习更全面的文本表示形式。  CapsNet-B在实验中表现更好。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165324.png)\n\nKim等人提出的基于CapsNet的模型[55]使用类似的架构。 该模型包括（1）一个输入层，该输入层将文档作为单词嵌入的序列；（2）卷积层，生成特征图并使用门控线性单元保留空间信息；（3）卷积胶囊层，通过聚合卷积层检测到的局部特征形成整体特征；（4）文本胶囊层以预测类标签。作者观察到，相比于图像，对象可以更自由地组合在文本中。 例如，即使某些句子的顺序改变了，文档的语义也可以保持不变，这与人脸上的眼睛和鼻子的位置不同。 因此，他们使用静态路由方案，该方案始终优于动态路由[50]进行文本分类。Aly等[56]提议使用CapsNets进行分层多标签分类（HMC），认为CapsNet编码子代关系的能力使其比传统方法更好地解决了HMC任务，在HMC任务中，文档被分配了一个或多个分类标签，这些标签被组织在一个层次结构。 他们模型的架构类似于[52，53，55]中的架构。任等人 [57]提出了CapsNets的另一种变体，它使用了胶囊之间的成分编码机制和基于k-means聚类的新路由算法。 首先，使用codebooks中的所有 codeword vectors 形成单词嵌入。 然后，通过k均值路由将下层胶囊捕获的特征汇总到高层胶囊中。\n\n### 2.5 基于注意力机制模型\n\n在开发用于NLP的深度学习模型时，注意力已成为越来越流行的概念和有用的工具[58，59]。 简而言之，语言模型中的注意力可被解释为重要权重的向量。 为了预测句子中的单词，我们使用注意力向量来估计它与其他单词的相关性或“与之相关”的程度，然后将注意力向量加权的值之和作为目标的近似值\n\n本节回顾了一些最突出的注意力模型，这些模型在发布时就在文本分类任务上取得了SOTA。\n\n杨等[60]提出了一种用于文本分类的分层注意力网络。 该模型具有两个鲜明的特征：（1）反映文档的层次结构的层次结构，（2）在单词和句子级别上应用的两个级别的注意力机制，使它能够在构建文档表示形式时以不同的方式参加重要或不重要的内容。 在六个文本分类任务上，该模型大大优于以前的方法。周等[61]将分层注意力模型扩展到跨语言情感分类。 在每种语言中，都使用LSTM网络对文档进行建模。 然后，通过使用分层注意机制实现分类，其中句子级别的注意模型了解文档的哪些句子对于确定总体情绪更重要。 而词级注意力模型则学习每个句子中哪些词具有决定性。\n\n沉等[62]提出了一种定向自我注意网络，用于无RNN / CNN语言理解，其中来自输入序列的元素之间的注意力是定向的和多维的。 轻量级神经网络仅基于所提出的注意力而无需任何RNN / CNN结构即可用于学习句子嵌入。刘等[63]提出了一个具有inner-attention的LSTM模型,用来做NLI任务。 该模型使用两阶段过程对句子进行编码。 首先，在词级Bi-LSTM上使用平均池化以生成第一阶段句子表示。 其次，采用注意力机制来代替同一句子的平均池，以获得更好的表示。 句子的第一阶段表示法用于出现在句子中的单词。\n\n注意模型也广泛应用于成对排序（pair-wise ranking）或匹配任务。  Santos等[64]提出了一种双向注意机制，称为Attentive Pooling（AP），用于成对排名。  AP可以使池化层知道当前的输入对（例如，问题-答案对），以使两个输入项的信息可以直接影响彼此表示的计算。 除了学习输入对的表示之外，AP联合学习该对投影段的相似性度量，然后为每个输入导出相应的注意力向量以指导池化。  AP是独立于底层表示学习的通用框架，并且可以应用于CNN和RNN，如图8（a）所示。Wang等[65]将文本分类视为标签-单词匹配问题：每个标签与单词向量一起嵌入相同的空间。作者介绍了一种注意力框架，该框架通过余弦相似性来度量文本序列和标签之间嵌入的兼容性，如图8（b）所示。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165837.png)\n\nKim等[66]提出了一种使用密集连接的循环共同注意力网络的语义句子匹配方法。 类似于DenseNet [39]，该模型的每一层都使用所有先前的递归层的注意特征以及隐藏特征的级联信息。它可以保留从最底层单词嵌入层到最上层循环层的原始和共同注意特征信息。Yin等[67]提出了另一种基于注意力的CNN模型，用于句子对匹配。 他们提出了三种将句子之间的相互影响整合到CNN中的注意力方案，以便每个句子的表示都考虑到其成对的句子。 这些相互依赖的句子对表示形式比孤立的句子表示形式更为强大，这在包括答案选择，复述识别和文本蕴涵在内的多个分类任务中得到了验证。Tan等[68]在匹配聚合框架下采用了多种注意函数来匹配句子对。 杨等。  [69]介绍了一种基于注意力的神经匹配模型，用于对简短答案文本进行排名。 他们采用价值共享加权方案代替位置共享加权方案来组合不同的匹配信号，并使用问题关注网络将问题术语重要性学习纳入其中。 该模型在TREC QA数据集上取得了可喜的结果。\n\n还有其他有趣的注意力模型。  Lin等[70]使用自注意力来提取可解释的句子嵌入。  Wang等[71]提出了一种具有多尺度特征关注度的紧密连接的CNN，以产生可变的n-gram特征。  Yamada和Shindo [72] 使用neural attentive bag-of-entities 模型（使用知识库中的实体）进行文本分类。  Parikh等。  [73]使用注意力将问题分解为可以单独解决的子问题。  Chen等[74]探索了通用的池化方法来增强句子嵌入，并提出了一个基于向量的多头注意力模型。  Liu和Lane [75]提出了一种基于注意力的RNN模型，用于联合意图检测和空缺填充。\n\n### 2.6 记忆增强网络\n\n注意力模型在编码过程中存储的隐藏向量可以看作是模型内部记忆，而记忆增强网络则将神经网络与外部记忆结合在一起，模型可以对其进行读写。\n\nMunkhdalai和Yu[76]提出了一种记忆增强的神经网络，称为神经语义编码器（NSE），用于文本分类和QA。  NSE配备了一个可变大小的编码记忆，该编码记忆会随着时间的推移而发展，并通过读取，编写和写入操作保持对输入序列的理解，如图9所示。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200503113637.png)\n\n韦斯顿等[77]设计了一个用于综合QA任务的记忆网络，在该网络中，向模型提供了一系列语句（记忆记录），作为问题的支持事实。 该模型会根据问题和先前检索到的记忆来一次从记忆中检索一个条目。Sukhbaatar等[78]扩展了这项工作，并提出了端到端的记忆网络，在记忆网络中以柔和的方式利用注意力机制检索记忆条目，从而实现了端到端的训练。 他们表明，通过多次回合（跳数），该模型能够检索并推理几个支持事实，以回答特定问题。\n\nKumar等[79]提出了一种动态记忆方法（DMN），它处理输入序列和问题，形成情节记忆，并产生相关的答案。 问题触发迭代注意进程，该进程允许模型将注意条件设置为输入和先前迭代的结果。 然后，在分层递归序列模型中对这些结果进行推理以生成答案。 对DMN进行端到端训练，并获得有关QA和POS标记的最新结果。 熊等[80]提出了DMN的详细分析，并改进了其就医和输入模块。","source":"_posts/【文献翻译】基于深度学习的文本分类：全面回顾.md","raw":"---\ntitle: 【文献翻译】基于深度学习的文本分类：全面回顾\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-04-27 20:42:51\npassword:\nsummary:\ncategories: 文献翻译\nimg:\nkeywords: 文本分类 综述 文献翻译\ntags:\n\t- 文本分类\n\t- 综述\n\t- 文献翻译\n---\n\n### 摘要\n\n基于深度学习的模型已经在各种文本分类任务中超过了经典的基于机器学的方法，例如情感分析，新闻分类，问答以及自然语言处理。在这次工作中，我们对近些年开发的150多种基于深度学习的文本分类模型进行了详尽的回顾，讨论了他们的技术贡献，相似点以及优点。我们还对广泛应用于文本分类的40多个流行数据集进行了总结。最后，我们对不同的深度学习模型在**流行基准**上的性能进行了定量分析。\n\n### Introduction\n\n文本分类是自然语言处理中的一个经典问题，旨在为文本单元（例如句子，**询问**，段落和文档）分配标签。文本分类有十分广泛的应用，例如问答，垃圾邮件检测，情感分析，新闻分类，用户意图识别，内容审核等等。文本数据可以来自不同的数据源，例如网页数据，邮件，聊天，社交媒体，机票，保险理赔，用户评论，客户服务中的问题和解答等等。文本中含有极其丰富的信息，但由于它的非结构化特征，想要从中提取信息便极具挑战和耗时。\n\n文本分类可以通过人工标注和自动标注两种方式进行，随着工业应用中文本数据规模不断增大，自动文本分类变得越来越重要。自动文本分类的方法可以被分为3类：\n\n* 基于规则的方法\n* 基于机器学习的方法（数据驱动）\n* 混合方法\n\n基于规则的方法使用一组预定义的规则将文本分为不同的类别。例如，所有包含“足球”，”篮球“或者”棒球“的文档都被标记为”运动“标签。\n\n### 2 用于文本分类的深度学习模型\n\n在这个部分，我们回顾了针对各种文本分类问题提出的150多种深度学习框架。为了更易于遵循，我们根据模型的主要架构贡献将其分为以下类别：\n\n* 基于前馈网络的模型，该模型将文本视为一堆单词（a bag of words）（第2.1节）\n* 基于RNN的模型，该模型将文本视为单词序列，旨在捕获单词相关性和文本结构（第2.2节）\n* 基于CNN的模型，经过训练可识别文本中的模式（例如关键短语）以进行分类（第2.3节）\n* 胶囊网络(Capsule networks)解决了CNN的池化操作所带来的信息丢失问题，最近已应用于文本分类（第2.4节）\n* 注意机制(Attention mechanism)可有效识别文本中的相关单词，并已成为开发深度学习模型的有用工具（第2.5节）\n* 记忆增强网络(Memory-augmented networks)，将神经网络与外部记忆形式结合在一起，模型可以从中读取和写入（第2.6节）\n* Transformers，允许比RNN更多的并行化，因此可以使用GPU集群有效地（预）训练非常大的语言模型（第2.7节）\n* 图神经网络(Graph neural networks)，旨在捕获自然语言的内部图结构，例如句法和语义解析树（第2.8节）\n* 孪生神经网络(Siamese Neural Networks)，用于文本匹配，文本匹配是文本分类的一种特殊情况（第2.9节）\n* 混合模型(Hybrid models)，结合注意力，RNN，CNN等模型来捕获句子和文档的局部和全局特征（第2.10节）\n* 最后，在2.11节中，我们回顾了有监督学习之外的建模技术，包括使用自动编码器(Autoencoder)和对抗训练(Adversarial training)的无监督学习(Unsupervised learning)，以及强化学习(Reinforcement learning)\n\n### 2.1 前馈神经网络\n\n前馈网络是用于文本表示的最简单的深度学习模型之一。但是，它们已经在许多文本分类基准上达到了很高的准确性。 这些模型将文本视为一堆单词。对于每个单词，他们使用诸如word2vec [8]或Glove [9]之类的嵌入模型学习向量表示，将嵌入向量的和或平均值作为文本的表示，将其通过一个或多个前馈层，称为多层感知器（MLP），然后使用诸如逻辑回归、朴素贝叶斯或SVM等分类器对最终层的表示进行分类[10]。一个例子如深度平均网络（Deep Average Network，DAN）[10]，其体系结构如图1所示。尽管简单，但DAN却胜过了其他更复杂的模型，这些模型旨在显式地学习文本的组成。例如，DAN在语法差异较大的数据集上的表现优于语法模型。Joulin等[11]提出了一种简单而有效的文本分类器，称为fastText。 像DAN一样，fastText将文本视为一堆单词，与DAN不同的是，fastText使用n-gram作为附加特征来捕获局部单词顺序信息。 事实证明，这在实践中非常有效，同时可达到与显式使用单词顺序的方法[12]相当的结果。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200428171503.png)\n\nLe和Mikolov [13]提出了doc2vec，它使用一种无监督算法来学习可变长度文本（例如句子，段落和文档）的定长特征表示。如图2所示，doc2vec的体系结构类似于连续词袋（CBOW）模型的体系结构[8，14]。唯一的区别是附加的段落标记通过矩阵D映射到段落向量。在doc2vec中，此向量与三个单词的上下文的连接或平均值用于预测第四个单词。 段落向量表示当前上下文中丢失的信息，可以用作该段落的主题记忆。经过训练后，段落向量将用作段落的特征并送入分类器进行预测。  Doc2vec在发布时，在一些文本分类和情感分析任务上获得了最优结果。\n\n### 2.2 基于RNN的模型\n\n基于RNN的模型将文本视为一系列单词，旨在捕获单词依赖性和文本结构以进行文本分类。但是，普通的RNN(vanilla RNN)模型不能很好地工作，并且通常表现不如前馈神经网络。 在RNN的许多变体中，长短期记忆网络（LSTM）是最受欢迎的结构，旨在更好地捕获长期依赖关系。LSTM通过引入存储单元以记住任意时间间隔的值以及三个门（输入门，输出门，遗忘门）来调节信息的流动，从而解决了普通RNN遇到的梯度消失或爆炸问题。已经有工作通过捕获更丰富的信息（例如自然语言的树结构，文本中的大跨度单词关系，文档主题等）来改进用于文本分类的RNN和LSTM模型。\n\nTai等[15]已经开发了Tree-LSTM模型，将LSTM推广到树结构网络类型，以学习丰富的语义表示。作者认为，针对自然语言处理任务，Tree-LSTM比链结构LSTM更好，因为自然语言具有句法属性，可以自然地将单词和短语组合在一起。他们在两个任务上验证了Tree-LSTM的有效性：情感分类和预测两个句子的语义相关性。这些模型的架构如图3所示。 [16]通过使用存储单元在递归过程中存储多个子单元或多个后代单元的历史将chain-structured LSTM展到树状结构。他们认为，新模型提供了一种原则上的方法，可以考虑在层次结构（例如语言或图像解析结构）上进行长距离交互。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430103448.png)\n\n为了对机器学习的大跨度单词关系进行建模，Cheng等人[17]用一个存储网络代替单个存储单元来增强LSTM体系结构。这可以在神经注意力复发期间启用自适应内存使用，从而提供一种弱化标记之间关系的方法。 该模型在语言建模，情感分析和NLI上取得了可喜的结果。\n\n多时标LSTM（MT-LSTM）神经网络[18]被设计为通过捕获具有不同时标的有价值的信息来对长文本（例如句子和文档）建模。MT-LSTM将标准LSTM模型的隐藏状态分为几组。 每个组在不同的时间段被激活和更新。 因此，MT-LSTM可以对很长的文档进行建模。MT-LSTM在文本分类方面优于包括基于LSTM和RNN的模型在内的基准。\n\nRNN擅长捕获单词序列的局部结构，但是面对远距离依赖关系会有点力不从心。相反，潜在主题模型（latent topic models）能够捕获文档的全局语义结构，但不考虑单词顺序。Bieng等 [19]提出了TopicRNN模型，以整合RNN和潜在主题模型的优点。 它使用RNN捕获局部（语法）依赖性，并使用潜在主题捕获全局（语义）依赖性。 TopicRNN在情感分析方面优于RNN基线。\n\n还有其他有趣的基于RNN的模型。 刘等[20]使用多任务学习来训练RNN，以利用来自多个相关任务的标记训练数据。Johnson和Rie [21]探索了使用LSTM的文本区域嵌入方法。周等 [22]集成了双向LSTM（Bi-LSTM）模型和二维最大池来捕获文本特征。Wang等[23]提出了在“matching-aggregation”框架下的双边多视角匹配模型。  Wan等[24]使用双向LSMT模型生成的多个位置句子表示来探索语义匹配。\n\n### 2.3 基于CNN的模型\n\n训练RNN识别跨时间的模式，而CNN学会识别跨空间的模式[25]。在需要理解远程语义的POS标签或QA等NLP任务中，RNN效果很好，而在检测局部和位置不变模式很重要的情况下，CNN效果很好。这些模式可能是表达特定情绪（例如“我喜欢”）或主题（例如“濒危物种”）的关键短语。 因此，CNN已成为最受欢迎的文本分类模型体系结构之一。\n\nKalchbrenner等人提出了最早的基于CNN的文本分类模型之一[26]。 该模型使用动态k-max池，称为动态CNN（DCNN）。如图4所示，DCNN的第一层使用对句子中每个单词的嵌入来构造句子矩阵。 然后使用将宽卷积层与动态k-max池给定的动态池层交替的卷积体系结构来生成句子的特征映射，该特征映射能够显式捕获单词和短语的短时和长时关系。可以根据句子大小和卷积层次结构的级别来动态选择池化参数k。\n\n后来，Kim [27]提出了一种比DCNN简单得多的基于CNN的模型，用于文本分类。 如图5所示，Kim的模型仅在从无监督神经语言模型（即word2vec）获得的单词向量上使用一层卷积。Kim还比较了四种学习单词嵌入的方法：\n\n* CNN-rand，其中所有单词嵌入都在训练过程中被随机初始化，然后进行修改\n* CNN-static，在模型训练期间使用预训练的word2vec嵌入并保持固定\n* CNN-non-static，其中word2vec嵌入在针对每个任务的训练过程中进行了微调\n* CNN-multi-channel，其中使用了两组词嵌入向量集，都使用word2vec进行了初始化，其中一个在模型训练期间进行了更新，而另一个则在固定的情况下进行了更新。\n\n这些基于CNN的模型将改进情感分析和问题分类的SOTA。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164349.png)\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164600.png)\n\n[26，27]已经做出了一些努力来改进基于CNN模型的体系结构。刘等[28]提出了一种新的基于CNN的模型，该模型对TextCNN [27]的体系结构进行了两次修改。首先，采用动态最大池化方案来从文档的不同区域捕获更多细粒度的特征。其次，在池化层和输出层之间插入一个隐藏的瓶颈层（bottleneck layer）学习紧凑的文档表示形式，以减小模型大小并提高模型性能。在[29，30]中，作者没有使用预先训练的低维词向量作为CNN的输入，而是直接将CNN应用于高维文本数据，以学习小文本区域的嵌入进行分类。\n\n字符级的CNN也已经被用于文本分类[31，32]。Zhang等人提出了最早的此类模型之一[31]。如图6所示，该模型以固定大小的字符作为输入，将其编码为一个one-hot向量，然后将它们通过一个深CNN模型，该模型由具有池化操作的六个卷积层和三个全连接层组成。Prusa等[33]提出了一种使用CNN编码文本的方法，该方法大大减少了学习字符级文本表示所需的内存消耗和训练时间。此方法可根据字母大小很好地缩放，从而可以保留原始文本中的更多信息以增强分类性能\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164852.png)\n\n有研究调查词嵌入和CNN架构对模型性能的影响。受到VGG [34]和ResNets [35]的启发，Conneau等人 [36]提出了一种非常深的CNN（VDCNN）模型用于文本处理。它直接在字符级别上操作，并且仅使用小的卷积和池化操作。 研究表明，VDCNN的性能随着深度的增加而增加。杜克等[37]修改了VDCNN的结构，以适应移动平台的限制并保持性能。他们能够将模型大小压缩10倍至20倍，而精度损失在0.4％至1.3％之间。Le等[38]表明，当文本输入表示为字符序列时，深层模型确实优于浅层模型。但是，一个简单的浅层和广域网络在词输入方面胜过DenseNet [39]等深层模型。郭等[40]研究了词嵌入的影响，并提出通过多通道CNN模型使用加权词嵌入。张等[41]研究了不同词嵌入方法和池化机制的影响，发现使用非静态word2vec和GloVe优于one-hot向量，并且最大池化始终优于其他池化方法。\n\n还有其他有趣的基于CNN的模型。Mou等[42]提出了一种基于树的CNN来捕获句子级语义。庞等[43]将文本匹配转换为图像识别任务，并使用多层CNN识别显著n-gram模式。Wang等[44]提出了一种基于CNN的模型，该模型结合了短文本的显式和隐式表示形式进行分类。 将CNN应用于生物医学文本分类的兴趣也越来越高[45-48]。\n\n### 2.4 胶囊网络\n\nCNN通过使用连续的卷积和池化层对图像或文本进行分类。 尽管池化操作可识别显著特征并降低卷积操作的计算复杂性，但它们会丢失有关空间关系的信息，并可能根据其方向或比例对实体进行错误分类\n\n为了解决池化带来的问题，Geoffrey Hinton提出了一种新方法，称为胶囊网络（CapsNets）[49，50]。一个胶囊是一组神经元，其活动向量代表特定类型的实体（例如对象或对象部分）的不同属性。向量的长度代表实体存在的概率，向量的方向代表实体的属性。与CNN的最大池化（选择一些信息并丢弃其余信息）不同，胶囊使用网络中直到最后一层的所有可用信息，将底层的每个胶囊“路由”到上层最匹配的父胶囊。可以使用不同的算法来实现路由，例如协议动态路由[50]或EM算法[51]。\n\n近来，胶囊网络已经被应用于文本分类，其中胶囊适于将句子或文档表示为向量。  [52–54]提出了一种基于CapsNets变体的文本分类模型。该模型由四层组成：（1）n-gram卷积层，（2）胶囊层，（3）卷积胶囊层，以及（4）完全连接的胶囊层。 作者尝试了三种策略来稳定动态路由过程，以减轻包含背景信息（例如停用词或与任何文档类别无关的词）的噪声包的干扰。 他们还探索了两种胶囊架构，如图7所示。分别为Capsule-A和Capsule-B。Capsule-A与[50]中的CapsNet类似。  Capsule-B使用三个并行网络，并在n-gram卷积层中使用具有不同窗口大小的过滤器，以学习更全面的文本表示形式。  CapsNet-B在实验中表现更好。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165324.png)\n\nKim等人提出的基于CapsNet的模型[55]使用类似的架构。 该模型包括（1）一个输入层，该输入层将文档作为单词嵌入的序列；（2）卷积层，生成特征图并使用门控线性单元保留空间信息；（3）卷积胶囊层，通过聚合卷积层检测到的局部特征形成整体特征；（4）文本胶囊层以预测类标签。作者观察到，相比于图像，对象可以更自由地组合在文本中。 例如，即使某些句子的顺序改变了，文档的语义也可以保持不变，这与人脸上的眼睛和鼻子的位置不同。 因此，他们使用静态路由方案，该方案始终优于动态路由[50]进行文本分类。Aly等[56]提议使用CapsNets进行分层多标签分类（HMC），认为CapsNet编码子代关系的能力使其比传统方法更好地解决了HMC任务，在HMC任务中，文档被分配了一个或多个分类标签，这些标签被组织在一个层次结构。 他们模型的架构类似于[52，53，55]中的架构。任等人 [57]提出了CapsNets的另一种变体，它使用了胶囊之间的成分编码机制和基于k-means聚类的新路由算法。 首先，使用codebooks中的所有 codeword vectors 形成单词嵌入。 然后，通过k均值路由将下层胶囊捕获的特征汇总到高层胶囊中。\n\n### 2.5 基于注意力机制模型\n\n在开发用于NLP的深度学习模型时，注意力已成为越来越流行的概念和有用的工具[58，59]。 简而言之，语言模型中的注意力可被解释为重要权重的向量。 为了预测句子中的单词，我们使用注意力向量来估计它与其他单词的相关性或“与之相关”的程度，然后将注意力向量加权的值之和作为目标的近似值\n\n本节回顾了一些最突出的注意力模型，这些模型在发布时就在文本分类任务上取得了SOTA。\n\n杨等[60]提出了一种用于文本分类的分层注意力网络。 该模型具有两个鲜明的特征：（1）反映文档的层次结构的层次结构，（2）在单词和句子级别上应用的两个级别的注意力机制，使它能够在构建文档表示形式时以不同的方式参加重要或不重要的内容。 在六个文本分类任务上，该模型大大优于以前的方法。周等[61]将分层注意力模型扩展到跨语言情感分类。 在每种语言中，都使用LSTM网络对文档进行建模。 然后，通过使用分层注意机制实现分类，其中句子级别的注意模型了解文档的哪些句子对于确定总体情绪更重要。 而词级注意力模型则学习每个句子中哪些词具有决定性。\n\n沉等[62]提出了一种定向自我注意网络，用于无RNN / CNN语言理解，其中来自输入序列的元素之间的注意力是定向的和多维的。 轻量级神经网络仅基于所提出的注意力而无需任何RNN / CNN结构即可用于学习句子嵌入。刘等[63]提出了一个具有inner-attention的LSTM模型,用来做NLI任务。 该模型使用两阶段过程对句子进行编码。 首先，在词级Bi-LSTM上使用平均池化以生成第一阶段句子表示。 其次，采用注意力机制来代替同一句子的平均池，以获得更好的表示。 句子的第一阶段表示法用于出现在句子中的单词。\n\n注意模型也广泛应用于成对排序（pair-wise ranking）或匹配任务。  Santos等[64]提出了一种双向注意机制，称为Attentive Pooling（AP），用于成对排名。  AP可以使池化层知道当前的输入对（例如，问题-答案对），以使两个输入项的信息可以直接影响彼此表示的计算。 除了学习输入对的表示之外，AP联合学习该对投影段的相似性度量，然后为每个输入导出相应的注意力向量以指导池化。  AP是独立于底层表示学习的通用框架，并且可以应用于CNN和RNN，如图8（a）所示。Wang等[65]将文本分类视为标签-单词匹配问题：每个标签与单词向量一起嵌入相同的空间。作者介绍了一种注意力框架，该框架通过余弦相似性来度量文本序列和标签之间嵌入的兼容性，如图8（b）所示。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165837.png)\n\nKim等[66]提出了一种使用密集连接的循环共同注意力网络的语义句子匹配方法。 类似于DenseNet [39]，该模型的每一层都使用所有先前的递归层的注意特征以及隐藏特征的级联信息。它可以保留从最底层单词嵌入层到最上层循环层的原始和共同注意特征信息。Yin等[67]提出了另一种基于注意力的CNN模型，用于句子对匹配。 他们提出了三种将句子之间的相互影响整合到CNN中的注意力方案，以便每个句子的表示都考虑到其成对的句子。 这些相互依赖的句子对表示形式比孤立的句子表示形式更为强大，这在包括答案选择，复述识别和文本蕴涵在内的多个分类任务中得到了验证。Tan等[68]在匹配聚合框架下采用了多种注意函数来匹配句子对。 杨等。  [69]介绍了一种基于注意力的神经匹配模型，用于对简短答案文本进行排名。 他们采用价值共享加权方案代替位置共享加权方案来组合不同的匹配信号，并使用问题关注网络将问题术语重要性学习纳入其中。 该模型在TREC QA数据集上取得了可喜的结果。\n\n还有其他有趣的注意力模型。  Lin等[70]使用自注意力来提取可解释的句子嵌入。  Wang等[71]提出了一种具有多尺度特征关注度的紧密连接的CNN，以产生可变的n-gram特征。  Yamada和Shindo [72] 使用neural attentive bag-of-entities 模型（使用知识库中的实体）进行文本分类。  Parikh等。  [73]使用注意力将问题分解为可以单独解决的子问题。  Chen等[74]探索了通用的池化方法来增强句子嵌入，并提出了一个基于向量的多头注意力模型。  Liu和Lane [75]提出了一种基于注意力的RNN模型，用于联合意图检测和空缺填充。\n\n### 2.6 记忆增强网络\n\n注意力模型在编码过程中存储的隐藏向量可以看作是模型内部记忆，而记忆增强网络则将神经网络与外部记忆结合在一起，模型可以对其进行读写。\n\nMunkhdalai和Yu[76]提出了一种记忆增强的神经网络，称为神经语义编码器（NSE），用于文本分类和QA。  NSE配备了一个可变大小的编码记忆，该编码记忆会随着时间的推移而发展，并通过读取，编写和写入操作保持对输入序列的理解，如图9所示。\n\n![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200503113637.png)\n\n韦斯顿等[77]设计了一个用于综合QA任务的记忆网络，在该网络中，向模型提供了一系列语句（记忆记录），作为问题的支持事实。 该模型会根据问题和先前检索到的记忆来一次从记忆中检索一个条目。Sukhbaatar等[78]扩展了这项工作，并提出了端到端的记忆网络，在记忆网络中以柔和的方式利用注意力机制检索记忆条目，从而实现了端到端的训练。 他们表明，通过多次回合（跳数），该模型能够检索并推理几个支持事实，以回答特定问题。\n\nKumar等[79]提出了一种动态记忆方法（DMN），它处理输入序列和问题，形成情节记忆，并产生相关的答案。 问题触发迭代注意进程，该进程允许模型将注意条件设置为输入和先前迭代的结果。 然后，在分层递归序列模型中对这些结果进行推理以生成答案。 对DMN进行端到端训练，并获得有关QA和POS标记的最新结果。 熊等[80]提出了DMN的详细分析，并改进了其就医和输入模块。","slug":"【文献翻译】基于深度学习的文本分类：全面回顾","published":1,"updated":"2020-08-20T08:41:44.647Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mal000sewse9td0ekj0","content":"<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p>基于深度学习的模型已经在各种文本分类任务中超过了经典的基于机器学的方法，例如情感分析，新闻分类，问答以及自然语言处理。在这次工作中，我们对近些年开发的150多种基于深度学习的文本分类模型进行了详尽的回顾，讨论了他们的技术贡献，相似点以及优点。我们还对广泛应用于文本分类的40多个流行数据集进行了总结。最后，我们对不同的深度学习模型在<strong>流行基准</strong>上的性能进行了定量分析。</p>\n<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>文本分类是自然语言处理中的一个经典问题，旨在为文本单元（例如句子，<strong>询问</strong>，段落和文档）分配标签。文本分类有十分广泛的应用，例如问答，垃圾邮件检测，情感分析，新闻分类，用户意图识别，内容审核等等。文本数据可以来自不同的数据源，例如网页数据，邮件，聊天，社交媒体，机票，保险理赔，用户评论，客户服务中的问题和解答等等。文本中含有极其丰富的信息，但由于它的非结构化特征，想要从中提取信息便极具挑战和耗时。</p>\n<p>文本分类可以通过人工标注和自动标注两种方式进行，随着工业应用中文本数据规模不断增大，自动文本分类变得越来越重要。自动文本分类的方法可以被分为3类：</p>\n<ul>\n<li>基于规则的方法</li>\n<li>基于机器学习的方法（数据驱动）</li>\n<li>混合方法</li>\n</ul>\n<p>基于规则的方法使用一组预定义的规则将文本分为不同的类别。例如，所有包含“足球”，”篮球“或者”棒球“的文档都被标记为”运动“标签。</p>\n<h3 id=\"2-用于文本分类的深度学习模型\"><a href=\"#2-用于文本分类的深度学习模型\" class=\"headerlink\" title=\"2 用于文本分类的深度学习模型\"></a>2 用于文本分类的深度学习模型</h3><p>在这个部分，我们回顾了针对各种文本分类问题提出的150多种深度学习框架。为了更易于遵循，我们根据模型的主要架构贡献将其分为以下类别：</p>\n<ul>\n<li>基于前馈网络的模型，该模型将文本视为一堆单词（a bag of words）（第2.1节）</li>\n<li>基于RNN的模型，该模型将文本视为单词序列，旨在捕获单词相关性和文本结构（第2.2节）</li>\n<li>基于CNN的模型，经过训练可识别文本中的模式（例如关键短语）以进行分类（第2.3节）</li>\n<li>胶囊网络(Capsule networks)解决了CNN的池化操作所带来的信息丢失问题，最近已应用于文本分类（第2.4节）</li>\n<li>注意机制(Attention mechanism)可有效识别文本中的相关单词，并已成为开发深度学习模型的有用工具（第2.5节）</li>\n<li>记忆增强网络(Memory-augmented networks)，将神经网络与外部记忆形式结合在一起，模型可以从中读取和写入（第2.6节）</li>\n<li>Transformers，允许比RNN更多的并行化，因此可以使用GPU集群有效地（预）训练非常大的语言模型（第2.7节）</li>\n<li>图神经网络(Graph neural networks)，旨在捕获自然语言的内部图结构，例如句法和语义解析树（第2.8节）</li>\n<li>孪生神经网络(Siamese Neural Networks)，用于文本匹配，文本匹配是文本分类的一种特殊情况（第2.9节）</li>\n<li>混合模型(Hybrid models)，结合注意力，RNN，CNN等模型来捕获句子和文档的局部和全局特征（第2.10节）</li>\n<li>最后，在2.11节中，我们回顾了有监督学习之外的建模技术，包括使用自动编码器(Autoencoder)和对抗训练(Adversarial training)的无监督学习(Unsupervised learning)，以及强化学习(Reinforcement learning)</li>\n</ul>\n<h3 id=\"2-1-前馈神经网络\"><a href=\"#2-1-前馈神经网络\" class=\"headerlink\" title=\"2.1 前馈神经网络\"></a>2.1 前馈神经网络</h3><p>前馈网络是用于文本表示的最简单的深度学习模型之一。但是，它们已经在许多文本分类基准上达到了很高的准确性。 这些模型将文本视为一堆单词。对于每个单词，他们使用诸如word2vec [8]或Glove [9]之类的嵌入模型学习向量表示，将嵌入向量的和或平均值作为文本的表示，将其通过一个或多个前馈层，称为多层感知器（MLP），然后使用诸如逻辑回归、朴素贝叶斯或SVM等分类器对最终层的表示进行分类[10]。一个例子如深度平均网络（Deep Average Network，DAN）[10]，其体系结构如图1所示。尽管简单，但DAN却胜过了其他更复杂的模型，这些模型旨在显式地学习文本的组成。例如，DAN在语法差异较大的数据集上的表现优于语法模型。Joulin等[11]提出了一种简单而有效的文本分类器，称为fastText。 像DAN一样，fastText将文本视为一堆单词，与DAN不同的是，fastText使用n-gram作为附加特征来捕获局部单词顺序信息。 事实证明，这在实践中非常有效，同时可达到与显式使用单词顺序的方法[12]相当的结果。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200428171503.png\" alt=\"\"></p>\n<p>Le和Mikolov [13]提出了doc2vec，它使用一种无监督算法来学习可变长度文本（例如句子，段落和文档）的定长特征表示。如图2所示，doc2vec的体系结构类似于连续词袋（CBOW）模型的体系结构[8，14]。唯一的区别是附加的段落标记通过矩阵D映射到段落向量。在doc2vec中，此向量与三个单词的上下文的连接或平均值用于预测第四个单词。 段落向量表示当前上下文中丢失的信息，可以用作该段落的主题记忆。经过训练后，段落向量将用作段落的特征并送入分类器进行预测。  Doc2vec在发布时，在一些文本分类和情感分析任务上获得了最优结果。</p>\n<h3 id=\"2-2-基于RNN的模型\"><a href=\"#2-2-基于RNN的模型\" class=\"headerlink\" title=\"2.2 基于RNN的模型\"></a>2.2 基于RNN的模型</h3><p>基于RNN的模型将文本视为一系列单词，旨在捕获单词依赖性和文本结构以进行文本分类。但是，普通的RNN(vanilla RNN)模型不能很好地工作，并且通常表现不如前馈神经网络。 在RNN的许多变体中，长短期记忆网络（LSTM）是最受欢迎的结构，旨在更好地捕获长期依赖关系。LSTM通过引入存储单元以记住任意时间间隔的值以及三个门（输入门，输出门，遗忘门）来调节信息的流动，从而解决了普通RNN遇到的梯度消失或爆炸问题。已经有工作通过捕获更丰富的信息（例如自然语言的树结构，文本中的大跨度单词关系，文档主题等）来改进用于文本分类的RNN和LSTM模型。</p>\n<p>Tai等[15]已经开发了Tree-LSTM模型，将LSTM推广到树结构网络类型，以学习丰富的语义表示。作者认为，针对自然语言处理任务，Tree-LSTM比链结构LSTM更好，因为自然语言具有句法属性，可以自然地将单词和短语组合在一起。他们在两个任务上验证了Tree-LSTM的有效性：情感分类和预测两个句子的语义相关性。这些模型的架构如图3所示。 [16]通过使用存储单元在递归过程中存储多个子单元或多个后代单元的历史将chain-structured LSTM展到树状结构。他们认为，新模型提供了一种原则上的方法，可以考虑在层次结构（例如语言或图像解析结构）上进行长距离交互。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430103448.png\" alt=\"\"></p>\n<p>为了对机器学习的大跨度单词关系进行建模，Cheng等人[17]用一个存储网络代替单个存储单元来增强LSTM体系结构。这可以在神经注意力复发期间启用自适应内存使用，从而提供一种弱化标记之间关系的方法。 该模型在语言建模，情感分析和NLI上取得了可喜的结果。</p>\n<p>多时标LSTM（MT-LSTM）神经网络[18]被设计为通过捕获具有不同时标的有价值的信息来对长文本（例如句子和文档）建模。MT-LSTM将标准LSTM模型的隐藏状态分为几组。 每个组在不同的时间段被激活和更新。 因此，MT-LSTM可以对很长的文档进行建模。MT-LSTM在文本分类方面优于包括基于LSTM和RNN的模型在内的基准。</p>\n<p>RNN擅长捕获单词序列的局部结构，但是面对远距离依赖关系会有点力不从心。相反，潜在主题模型（latent topic models）能够捕获文档的全局语义结构，但不考虑单词顺序。Bieng等 [19]提出了TopicRNN模型，以整合RNN和潜在主题模型的优点。 它使用RNN捕获局部（语法）依赖性，并使用潜在主题捕获全局（语义）依赖性。 TopicRNN在情感分析方面优于RNN基线。</p>\n<p>还有其他有趣的基于RNN的模型。 刘等[20]使用多任务学习来训练RNN，以利用来自多个相关任务的标记训练数据。Johnson和Rie [21]探索了使用LSTM的文本区域嵌入方法。周等 [22]集成了双向LSTM（Bi-LSTM）模型和二维最大池来捕获文本特征。Wang等[23]提出了在“matching-aggregation”框架下的双边多视角匹配模型。  Wan等[24]使用双向LSMT模型生成的多个位置句子表示来探索语义匹配。</p>\n<h3 id=\"2-3-基于CNN的模型\"><a href=\"#2-3-基于CNN的模型\" class=\"headerlink\" title=\"2.3 基于CNN的模型\"></a>2.3 基于CNN的模型</h3><p>训练RNN识别跨时间的模式，而CNN学会识别跨空间的模式[25]。在需要理解远程语义的POS标签或QA等NLP任务中，RNN效果很好，而在检测局部和位置不变模式很重要的情况下，CNN效果很好。这些模式可能是表达特定情绪（例如“我喜欢”）或主题（例如“濒危物种”）的关键短语。 因此，CNN已成为最受欢迎的文本分类模型体系结构之一。</p>\n<p>Kalchbrenner等人提出了最早的基于CNN的文本分类模型之一[26]。 该模型使用动态k-max池，称为动态CNN（DCNN）。如图4所示，DCNN的第一层使用对句子中每个单词的嵌入来构造句子矩阵。 然后使用将宽卷积层与动态k-max池给定的动态池层交替的卷积体系结构来生成句子的特征映射，该特征映射能够显式捕获单词和短语的短时和长时关系。可以根据句子大小和卷积层次结构的级别来动态选择池化参数k。</p>\n<p>后来，Kim [27]提出了一种比DCNN简单得多的基于CNN的模型，用于文本分类。 如图5所示，Kim的模型仅在从无监督神经语言模型（即word2vec）获得的单词向量上使用一层卷积。Kim还比较了四种学习单词嵌入的方法：</p>\n<ul>\n<li>CNN-rand，其中所有单词嵌入都在训练过程中被随机初始化，然后进行修改</li>\n<li>CNN-static，在模型训练期间使用预训练的word2vec嵌入并保持固定</li>\n<li>CNN-non-static，其中word2vec嵌入在针对每个任务的训练过程中进行了微调</li>\n<li>CNN-multi-channel，其中使用了两组词嵌入向量集，都使用word2vec进行了初始化，其中一个在模型训练期间进行了更新，而另一个则在固定的情况下进行了更新。</li>\n</ul>\n<p>这些基于CNN的模型将改进情感分析和问题分类的SOTA。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164349.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164600.png\" alt=\"\"></p>\n<p>[26，27]已经做出了一些努力来改进基于CNN模型的体系结构。刘等[28]提出了一种新的基于CNN的模型，该模型对TextCNN [27]的体系结构进行了两次修改。首先，采用动态最大池化方案来从文档的不同区域捕获更多细粒度的特征。其次，在池化层和输出层之间插入一个隐藏的瓶颈层（bottleneck layer）学习紧凑的文档表示形式，以减小模型大小并提高模型性能。在[29，30]中，作者没有使用预先训练的低维词向量作为CNN的输入，而是直接将CNN应用于高维文本数据，以学习小文本区域的嵌入进行分类。</p>\n<p>字符级的CNN也已经被用于文本分类[31，32]。Zhang等人提出了最早的此类模型之一[31]。如图6所示，该模型以固定大小的字符作为输入，将其编码为一个one-hot向量，然后将它们通过一个深CNN模型，该模型由具有池化操作的六个卷积层和三个全连接层组成。Prusa等[33]提出了一种使用CNN编码文本的方法，该方法大大减少了学习字符级文本表示所需的内存消耗和训练时间。此方法可根据字母大小很好地缩放，从而可以保留原始文本中的更多信息以增强分类性能</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164852.png\" alt=\"\"></p>\n<p>有研究调查词嵌入和CNN架构对模型性能的影响。受到VGG [34]和ResNets [35]的启发，Conneau等人 [36]提出了一种非常深的CNN（VDCNN）模型用于文本处理。它直接在字符级别上操作，并且仅使用小的卷积和池化操作。 研究表明，VDCNN的性能随着深度的增加而增加。杜克等[37]修改了VDCNN的结构，以适应移动平台的限制并保持性能。他们能够将模型大小压缩10倍至20倍，而精度损失在0.4％至1.3％之间。Le等[38]表明，当文本输入表示为字符序列时，深层模型确实优于浅层模型。但是，一个简单的浅层和广域网络在词输入方面胜过DenseNet [39]等深层模型。郭等[40]研究了词嵌入的影响，并提出通过多通道CNN模型使用加权词嵌入。张等[41]研究了不同词嵌入方法和池化机制的影响，发现使用非静态word2vec和GloVe优于one-hot向量，并且最大池化始终优于其他池化方法。</p>\n<p>还有其他有趣的基于CNN的模型。Mou等[42]提出了一种基于树的CNN来捕获句子级语义。庞等[43]将文本匹配转换为图像识别任务，并使用多层CNN识别显著n-gram模式。Wang等[44]提出了一种基于CNN的模型，该模型结合了短文本的显式和隐式表示形式进行分类。 将CNN应用于生物医学文本分类的兴趣也越来越高[45-48]。</p>\n<h3 id=\"2-4-胶囊网络\"><a href=\"#2-4-胶囊网络\" class=\"headerlink\" title=\"2.4 胶囊网络\"></a>2.4 胶囊网络</h3><p>CNN通过使用连续的卷积和池化层对图像或文本进行分类。 尽管池化操作可识别显著特征并降低卷积操作的计算复杂性，但它们会丢失有关空间关系的信息，并可能根据其方向或比例对实体进行错误分类</p>\n<p>为了解决池化带来的问题，Geoffrey Hinton提出了一种新方法，称为胶囊网络（CapsNets）[49，50]。一个胶囊是一组神经元，其活动向量代表特定类型的实体（例如对象或对象部分）的不同属性。向量的长度代表实体存在的概率，向量的方向代表实体的属性。与CNN的最大池化（选择一些信息并丢弃其余信息）不同，胶囊使用网络中直到最后一层的所有可用信息，将底层的每个胶囊“路由”到上层最匹配的父胶囊。可以使用不同的算法来实现路由，例如协议动态路由[50]或EM算法[51]。</p>\n<p>近来，胶囊网络已经被应用于文本分类，其中胶囊适于将句子或文档表示为向量。  [52–54]提出了一种基于CapsNets变体的文本分类模型。该模型由四层组成：（1）n-gram卷积层，（2）胶囊层，（3）卷积胶囊层，以及（4）完全连接的胶囊层。 作者尝试了三种策略来稳定动态路由过程，以减轻包含背景信息（例如停用词或与任何文档类别无关的词）的噪声包的干扰。 他们还探索了两种胶囊架构，如图7所示。分别为Capsule-A和Capsule-B。Capsule-A与[50]中的CapsNet类似。  Capsule-B使用三个并行网络，并在n-gram卷积层中使用具有不同窗口大小的过滤器，以学习更全面的文本表示形式。  CapsNet-B在实验中表现更好。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165324.png\" alt=\"\"></p>\n<p>Kim等人提出的基于CapsNet的模型[55]使用类似的架构。 该模型包括（1）一个输入层，该输入层将文档作为单词嵌入的序列；（2）卷积层，生成特征图并使用门控线性单元保留空间信息；（3）卷积胶囊层，通过聚合卷积层检测到的局部特征形成整体特征；（4）文本胶囊层以预测类标签。作者观察到，相比于图像，对象可以更自由地组合在文本中。 例如，即使某些句子的顺序改变了，文档的语义也可以保持不变，这与人脸上的眼睛和鼻子的位置不同。 因此，他们使用静态路由方案，该方案始终优于动态路由[50]进行文本分类。Aly等[56]提议使用CapsNets进行分层多标签分类（HMC），认为CapsNet编码子代关系的能力使其比传统方法更好地解决了HMC任务，在HMC任务中，文档被分配了一个或多个分类标签，这些标签被组织在一个层次结构。 他们模型的架构类似于[52，53，55]中的架构。任等人 [57]提出了CapsNets的另一种变体，它使用了胶囊之间的成分编码机制和基于k-means聚类的新路由算法。 首先，使用codebooks中的所有 codeword vectors 形成单词嵌入。 然后，通过k均值路由将下层胶囊捕获的特征汇总到高层胶囊中。</p>\n<h3 id=\"2-5-基于注意力机制模型\"><a href=\"#2-5-基于注意力机制模型\" class=\"headerlink\" title=\"2.5 基于注意力机制模型\"></a>2.5 基于注意力机制模型</h3><p>在开发用于NLP的深度学习模型时，注意力已成为越来越流行的概念和有用的工具[58，59]。 简而言之，语言模型中的注意力可被解释为重要权重的向量。 为了预测句子中的单词，我们使用注意力向量来估计它与其他单词的相关性或“与之相关”的程度，然后将注意力向量加权的值之和作为目标的近似值</p>\n<p>本节回顾了一些最突出的注意力模型，这些模型在发布时就在文本分类任务上取得了SOTA。</p>\n<p>杨等[60]提出了一种用于文本分类的分层注意力网络。 该模型具有两个鲜明的特征：（1）反映文档的层次结构的层次结构，（2）在单词和句子级别上应用的两个级别的注意力机制，使它能够在构建文档表示形式时以不同的方式参加重要或不重要的内容。 在六个文本分类任务上，该模型大大优于以前的方法。周等[61]将分层注意力模型扩展到跨语言情感分类。 在每种语言中，都使用LSTM网络对文档进行建模。 然后，通过使用分层注意机制实现分类，其中句子级别的注意模型了解文档的哪些句子对于确定总体情绪更重要。 而词级注意力模型则学习每个句子中哪些词具有决定性。</p>\n<p>沉等[62]提出了一种定向自我注意网络，用于无RNN / CNN语言理解，其中来自输入序列的元素之间的注意力是定向的和多维的。 轻量级神经网络仅基于所提出的注意力而无需任何RNN / CNN结构即可用于学习句子嵌入。刘等[63]提出了一个具有inner-attention的LSTM模型,用来做NLI任务。 该模型使用两阶段过程对句子进行编码。 首先，在词级Bi-LSTM上使用平均池化以生成第一阶段句子表示。 其次，采用注意力机制来代替同一句子的平均池，以获得更好的表示。 句子的第一阶段表示法用于出现在句子中的单词。</p>\n<p>注意模型也广泛应用于成对排序（pair-wise ranking）或匹配任务。  Santos等[64]提出了一种双向注意机制，称为Attentive Pooling（AP），用于成对排名。  AP可以使池化层知道当前的输入对（例如，问题-答案对），以使两个输入项的信息可以直接影响彼此表示的计算。 除了学习输入对的表示之外，AP联合学习该对投影段的相似性度量，然后为每个输入导出相应的注意力向量以指导池化。  AP是独立于底层表示学习的通用框架，并且可以应用于CNN和RNN，如图8（a）所示。Wang等[65]将文本分类视为标签-单词匹配问题：每个标签与单词向量一起嵌入相同的空间。作者介绍了一种注意力框架，该框架通过余弦相似性来度量文本序列和标签之间嵌入的兼容性，如图8（b）所示。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165837.png\" alt=\"\"></p>\n<p>Kim等[66]提出了一种使用密集连接的循环共同注意力网络的语义句子匹配方法。 类似于DenseNet [39]，该模型的每一层都使用所有先前的递归层的注意特征以及隐藏特征的级联信息。它可以保留从最底层单词嵌入层到最上层循环层的原始和共同注意特征信息。Yin等[67]提出了另一种基于注意力的CNN模型，用于句子对匹配。 他们提出了三种将句子之间的相互影响整合到CNN中的注意力方案，以便每个句子的表示都考虑到其成对的句子。 这些相互依赖的句子对表示形式比孤立的句子表示形式更为强大，这在包括答案选择，复述识别和文本蕴涵在内的多个分类任务中得到了验证。Tan等[68]在匹配聚合框架下采用了多种注意函数来匹配句子对。 杨等。  [69]介绍了一种基于注意力的神经匹配模型，用于对简短答案文本进行排名。 他们采用价值共享加权方案代替位置共享加权方案来组合不同的匹配信号，并使用问题关注网络将问题术语重要性学习纳入其中。 该模型在TREC QA数据集上取得了可喜的结果。</p>\n<p>还有其他有趣的注意力模型。  Lin等[70]使用自注意力来提取可解释的句子嵌入。  Wang等[71]提出了一种具有多尺度特征关注度的紧密连接的CNN，以产生可变的n-gram特征。  Yamada和Shindo [72] 使用neural attentive bag-of-entities 模型（使用知识库中的实体）进行文本分类。  Parikh等。  [73]使用注意力将问题分解为可以单独解决的子问题。  Chen等[74]探索了通用的池化方法来增强句子嵌入，并提出了一个基于向量的多头注意力模型。  Liu和Lane [75]提出了一种基于注意力的RNN模型，用于联合意图检测和空缺填充。</p>\n<h3 id=\"2-6-记忆增强网络\"><a href=\"#2-6-记忆增强网络\" class=\"headerlink\" title=\"2.6 记忆增强网络\"></a>2.6 记忆增强网络</h3><p>注意力模型在编码过程中存储的隐藏向量可以看作是模型内部记忆，而记忆增强网络则将神经网络与外部记忆结合在一起，模型可以对其进行读写。</p>\n<p>Munkhdalai和Yu[76]提出了一种记忆增强的神经网络，称为神经语义编码器（NSE），用于文本分类和QA。  NSE配备了一个可变大小的编码记忆，该编码记忆会随着时间的推移而发展，并通过读取，编写和写入操作保持对输入序列的理解，如图9所示。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200503113637.png\" alt=\"\"></p>\n<p>韦斯顿等[77]设计了一个用于综合QA任务的记忆网络，在该网络中，向模型提供了一系列语句（记忆记录），作为问题的支持事实。 该模型会根据问题和先前检索到的记忆来一次从记忆中检索一个条目。Sukhbaatar等[78]扩展了这项工作，并提出了端到端的记忆网络，在记忆网络中以柔和的方式利用注意力机制检索记忆条目，从而实现了端到端的训练。 他们表明，通过多次回合（跳数），该模型能够检索并推理几个支持事实，以回答特定问题。</p>\n<p>Kumar等[79]提出了一种动态记忆方法（DMN），它处理输入序列和问题，形成情节记忆，并产生相关的答案。 问题触发迭代注意进程，该进程允许模型将注意条件设置为输入和先前迭代的结果。 然后，在分层递归序列模型中对这些结果进行推理以生成答案。 对DMN进行端到端训练，并获得有关QA和POS标记的最新结果。 熊等[80]提出了DMN的详细分析，并改进了其就医和输入模块。</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p>基于深度学习的模型已经在各种文本分类任务中超过了经典的基于机器学的方法，例如情感分析，新闻分类，问答以及自然语言处理。在这次工作中，我们对近些年开发的150多种基于深度学习的文本分类模型进行了详尽的回顾，讨论了他们的技术贡献，相似点以及优点。我们还对广泛应用于文本分类的40多个流行数据集进行了总结。最后，我们对不同的深度学习模型在<strong>流行基准</strong>上的性能进行了定量分析。</p>\n<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>文本分类是自然语言处理中的一个经典问题，旨在为文本单元（例如句子，<strong>询问</strong>，段落和文档）分配标签。文本分类有十分广泛的应用，例如问答，垃圾邮件检测，情感分析，新闻分类，用户意图识别，内容审核等等。文本数据可以来自不同的数据源，例如网页数据，邮件，聊天，社交媒体，机票，保险理赔，用户评论，客户服务中的问题和解答等等。文本中含有极其丰富的信息，但由于它的非结构化特征，想要从中提取信息便极具挑战和耗时。</p>\n<p>文本分类可以通过人工标注和自动标注两种方式进行，随着工业应用中文本数据规模不断增大，自动文本分类变得越来越重要。自动文本分类的方法可以被分为3类：</p>\n<ul>\n<li>基于规则的方法</li>\n<li>基于机器学习的方法（数据驱动）</li>\n<li>混合方法</li>\n</ul>\n<p>基于规则的方法使用一组预定义的规则将文本分为不同的类别。例如，所有包含“足球”，”篮球“或者”棒球“的文档都被标记为”运动“标签。</p>\n<h3 id=\"2-用于文本分类的深度学习模型\"><a href=\"#2-用于文本分类的深度学习模型\" class=\"headerlink\" title=\"2 用于文本分类的深度学习模型\"></a>2 用于文本分类的深度学习模型</h3><p>在这个部分，我们回顾了针对各种文本分类问题提出的150多种深度学习框架。为了更易于遵循，我们根据模型的主要架构贡献将其分为以下类别：</p>\n<ul>\n<li>基于前馈网络的模型，该模型将文本视为一堆单词（a bag of words）（第2.1节）</li>\n<li>基于RNN的模型，该模型将文本视为单词序列，旨在捕获单词相关性和文本结构（第2.2节）</li>\n<li>基于CNN的模型，经过训练可识别文本中的模式（例如关键短语）以进行分类（第2.3节）</li>\n<li>胶囊网络(Capsule networks)解决了CNN的池化操作所带来的信息丢失问题，最近已应用于文本分类（第2.4节）</li>\n<li>注意机制(Attention mechanism)可有效识别文本中的相关单词，并已成为开发深度学习模型的有用工具（第2.5节）</li>\n<li>记忆增强网络(Memory-augmented networks)，将神经网络与外部记忆形式结合在一起，模型可以从中读取和写入（第2.6节）</li>\n<li>Transformers，允许比RNN更多的并行化，因此可以使用GPU集群有效地（预）训练非常大的语言模型（第2.7节）</li>\n<li>图神经网络(Graph neural networks)，旨在捕获自然语言的内部图结构，例如句法和语义解析树（第2.8节）</li>\n<li>孪生神经网络(Siamese Neural Networks)，用于文本匹配，文本匹配是文本分类的一种特殊情况（第2.9节）</li>\n<li>混合模型(Hybrid models)，结合注意力，RNN，CNN等模型来捕获句子和文档的局部和全局特征（第2.10节）</li>\n<li>最后，在2.11节中，我们回顾了有监督学习之外的建模技术，包括使用自动编码器(Autoencoder)和对抗训练(Adversarial training)的无监督学习(Unsupervised learning)，以及强化学习(Reinforcement learning)</li>\n</ul>\n<h3 id=\"2-1-前馈神经网络\"><a href=\"#2-1-前馈神经网络\" class=\"headerlink\" title=\"2.1 前馈神经网络\"></a>2.1 前馈神经网络</h3><p>前馈网络是用于文本表示的最简单的深度学习模型之一。但是，它们已经在许多文本分类基准上达到了很高的准确性。 这些模型将文本视为一堆单词。对于每个单词，他们使用诸如word2vec [8]或Glove [9]之类的嵌入模型学习向量表示，将嵌入向量的和或平均值作为文本的表示，将其通过一个或多个前馈层，称为多层感知器（MLP），然后使用诸如逻辑回归、朴素贝叶斯或SVM等分类器对最终层的表示进行分类[10]。一个例子如深度平均网络（Deep Average Network，DAN）[10]，其体系结构如图1所示。尽管简单，但DAN却胜过了其他更复杂的模型，这些模型旨在显式地学习文本的组成。例如，DAN在语法差异较大的数据集上的表现优于语法模型。Joulin等[11]提出了一种简单而有效的文本分类器，称为fastText。 像DAN一样，fastText将文本视为一堆单词，与DAN不同的是，fastText使用n-gram作为附加特征来捕获局部单词顺序信息。 事实证明，这在实践中非常有效，同时可达到与显式使用单词顺序的方法[12]相当的结果。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200428171503.png\" alt=\"\"></p>\n<p>Le和Mikolov [13]提出了doc2vec，它使用一种无监督算法来学习可变长度文本（例如句子，段落和文档）的定长特征表示。如图2所示，doc2vec的体系结构类似于连续词袋（CBOW）模型的体系结构[8，14]。唯一的区别是附加的段落标记通过矩阵D映射到段落向量。在doc2vec中，此向量与三个单词的上下文的连接或平均值用于预测第四个单词。 段落向量表示当前上下文中丢失的信息，可以用作该段落的主题记忆。经过训练后，段落向量将用作段落的特征并送入分类器进行预测。  Doc2vec在发布时，在一些文本分类和情感分析任务上获得了最优结果。</p>\n<h3 id=\"2-2-基于RNN的模型\"><a href=\"#2-2-基于RNN的模型\" class=\"headerlink\" title=\"2.2 基于RNN的模型\"></a>2.2 基于RNN的模型</h3><p>基于RNN的模型将文本视为一系列单词，旨在捕获单词依赖性和文本结构以进行文本分类。但是，普通的RNN(vanilla RNN)模型不能很好地工作，并且通常表现不如前馈神经网络。 在RNN的许多变体中，长短期记忆网络（LSTM）是最受欢迎的结构，旨在更好地捕获长期依赖关系。LSTM通过引入存储单元以记住任意时间间隔的值以及三个门（输入门，输出门，遗忘门）来调节信息的流动，从而解决了普通RNN遇到的梯度消失或爆炸问题。已经有工作通过捕获更丰富的信息（例如自然语言的树结构，文本中的大跨度单词关系，文档主题等）来改进用于文本分类的RNN和LSTM模型。</p>\n<p>Tai等[15]已经开发了Tree-LSTM模型，将LSTM推广到树结构网络类型，以学习丰富的语义表示。作者认为，针对自然语言处理任务，Tree-LSTM比链结构LSTM更好，因为自然语言具有句法属性，可以自然地将单词和短语组合在一起。他们在两个任务上验证了Tree-LSTM的有效性：情感分类和预测两个句子的语义相关性。这些模型的架构如图3所示。 [16]通过使用存储单元在递归过程中存储多个子单元或多个后代单元的历史将chain-structured LSTM展到树状结构。他们认为，新模型提供了一种原则上的方法，可以考虑在层次结构（例如语言或图像解析结构）上进行长距离交互。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430103448.png\" alt=\"\"></p>\n<p>为了对机器学习的大跨度单词关系进行建模，Cheng等人[17]用一个存储网络代替单个存储单元来增强LSTM体系结构。这可以在神经注意力复发期间启用自适应内存使用，从而提供一种弱化标记之间关系的方法。 该模型在语言建模，情感分析和NLI上取得了可喜的结果。</p>\n<p>多时标LSTM（MT-LSTM）神经网络[18]被设计为通过捕获具有不同时标的有价值的信息来对长文本（例如句子和文档）建模。MT-LSTM将标准LSTM模型的隐藏状态分为几组。 每个组在不同的时间段被激活和更新。 因此，MT-LSTM可以对很长的文档进行建模。MT-LSTM在文本分类方面优于包括基于LSTM和RNN的模型在内的基准。</p>\n<p>RNN擅长捕获单词序列的局部结构，但是面对远距离依赖关系会有点力不从心。相反，潜在主题模型（latent topic models）能够捕获文档的全局语义结构，但不考虑单词顺序。Bieng等 [19]提出了TopicRNN模型，以整合RNN和潜在主题模型的优点。 它使用RNN捕获局部（语法）依赖性，并使用潜在主题捕获全局（语义）依赖性。 TopicRNN在情感分析方面优于RNN基线。</p>\n<p>还有其他有趣的基于RNN的模型。 刘等[20]使用多任务学习来训练RNN，以利用来自多个相关任务的标记训练数据。Johnson和Rie [21]探索了使用LSTM的文本区域嵌入方法。周等 [22]集成了双向LSTM（Bi-LSTM）模型和二维最大池来捕获文本特征。Wang等[23]提出了在“matching-aggregation”框架下的双边多视角匹配模型。  Wan等[24]使用双向LSMT模型生成的多个位置句子表示来探索语义匹配。</p>\n<h3 id=\"2-3-基于CNN的模型\"><a href=\"#2-3-基于CNN的模型\" class=\"headerlink\" title=\"2.3 基于CNN的模型\"></a>2.3 基于CNN的模型</h3><p>训练RNN识别跨时间的模式，而CNN学会识别跨空间的模式[25]。在需要理解远程语义的POS标签或QA等NLP任务中，RNN效果很好，而在检测局部和位置不变模式很重要的情况下，CNN效果很好。这些模式可能是表达特定情绪（例如“我喜欢”）或主题（例如“濒危物种”）的关键短语。 因此，CNN已成为最受欢迎的文本分类模型体系结构之一。</p>\n<p>Kalchbrenner等人提出了最早的基于CNN的文本分类模型之一[26]。 该模型使用动态k-max池，称为动态CNN（DCNN）。如图4所示，DCNN的第一层使用对句子中每个单词的嵌入来构造句子矩阵。 然后使用将宽卷积层与动态k-max池给定的动态池层交替的卷积体系结构来生成句子的特征映射，该特征映射能够显式捕获单词和短语的短时和长时关系。可以根据句子大小和卷积层次结构的级别来动态选择池化参数k。</p>\n<p>后来，Kim [27]提出了一种比DCNN简单得多的基于CNN的模型，用于文本分类。 如图5所示，Kim的模型仅在从无监督神经语言模型（即word2vec）获得的单词向量上使用一层卷积。Kim还比较了四种学习单词嵌入的方法：</p>\n<ul>\n<li>CNN-rand，其中所有单词嵌入都在训练过程中被随机初始化，然后进行修改</li>\n<li>CNN-static，在模型训练期间使用预训练的word2vec嵌入并保持固定</li>\n<li>CNN-non-static，其中word2vec嵌入在针对每个任务的训练过程中进行了微调</li>\n<li>CNN-multi-channel，其中使用了两组词嵌入向量集，都使用word2vec进行了初始化，其中一个在模型训练期间进行了更新，而另一个则在固定的情况下进行了更新。</li>\n</ul>\n<p>这些基于CNN的模型将改进情感分析和问题分类的SOTA。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164349.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164600.png\" alt=\"\"></p>\n<p>[26，27]已经做出了一些努力来改进基于CNN模型的体系结构。刘等[28]提出了一种新的基于CNN的模型，该模型对TextCNN [27]的体系结构进行了两次修改。首先，采用动态最大池化方案来从文档的不同区域捕获更多细粒度的特征。其次，在池化层和输出层之间插入一个隐藏的瓶颈层（bottleneck layer）学习紧凑的文档表示形式，以减小模型大小并提高模型性能。在[29，30]中，作者没有使用预先训练的低维词向量作为CNN的输入，而是直接将CNN应用于高维文本数据，以学习小文本区域的嵌入进行分类。</p>\n<p>字符级的CNN也已经被用于文本分类[31，32]。Zhang等人提出了最早的此类模型之一[31]。如图6所示，该模型以固定大小的字符作为输入，将其编码为一个one-hot向量，然后将它们通过一个深CNN模型，该模型由具有池化操作的六个卷积层和三个全连接层组成。Prusa等[33]提出了一种使用CNN编码文本的方法，该方法大大减少了学习字符级文本表示所需的内存消耗和训练时间。此方法可根据字母大小很好地缩放，从而可以保留原始文本中的更多信息以增强分类性能</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164852.png\" alt=\"\"></p>\n<p>有研究调查词嵌入和CNN架构对模型性能的影响。受到VGG [34]和ResNets [35]的启发，Conneau等人 [36]提出了一种非常深的CNN（VDCNN）模型用于文本处理。它直接在字符级别上操作，并且仅使用小的卷积和池化操作。 研究表明，VDCNN的性能随着深度的增加而增加。杜克等[37]修改了VDCNN的结构，以适应移动平台的限制并保持性能。他们能够将模型大小压缩10倍至20倍，而精度损失在0.4％至1.3％之间。Le等[38]表明，当文本输入表示为字符序列时，深层模型确实优于浅层模型。但是，一个简单的浅层和广域网络在词输入方面胜过DenseNet [39]等深层模型。郭等[40]研究了词嵌入的影响，并提出通过多通道CNN模型使用加权词嵌入。张等[41]研究了不同词嵌入方法和池化机制的影响，发现使用非静态word2vec和GloVe优于one-hot向量，并且最大池化始终优于其他池化方法。</p>\n<p>还有其他有趣的基于CNN的模型。Mou等[42]提出了一种基于树的CNN来捕获句子级语义。庞等[43]将文本匹配转换为图像识别任务，并使用多层CNN识别显著n-gram模式。Wang等[44]提出了一种基于CNN的模型，该模型结合了短文本的显式和隐式表示形式进行分类。 将CNN应用于生物医学文本分类的兴趣也越来越高[45-48]。</p>\n<h3 id=\"2-4-胶囊网络\"><a href=\"#2-4-胶囊网络\" class=\"headerlink\" title=\"2.4 胶囊网络\"></a>2.4 胶囊网络</h3><p>CNN通过使用连续的卷积和池化层对图像或文本进行分类。 尽管池化操作可识别显著特征并降低卷积操作的计算复杂性，但它们会丢失有关空间关系的信息，并可能根据其方向或比例对实体进行错误分类</p>\n<p>为了解决池化带来的问题，Geoffrey Hinton提出了一种新方法，称为胶囊网络（CapsNets）[49，50]。一个胶囊是一组神经元，其活动向量代表特定类型的实体（例如对象或对象部分）的不同属性。向量的长度代表实体存在的概率，向量的方向代表实体的属性。与CNN的最大池化（选择一些信息并丢弃其余信息）不同，胶囊使用网络中直到最后一层的所有可用信息，将底层的每个胶囊“路由”到上层最匹配的父胶囊。可以使用不同的算法来实现路由，例如协议动态路由[50]或EM算法[51]。</p>\n<p>近来，胶囊网络已经被应用于文本分类，其中胶囊适于将句子或文档表示为向量。  [52–54]提出了一种基于CapsNets变体的文本分类模型。该模型由四层组成：（1）n-gram卷积层，（2）胶囊层，（3）卷积胶囊层，以及（4）完全连接的胶囊层。 作者尝试了三种策略来稳定动态路由过程，以减轻包含背景信息（例如停用词或与任何文档类别无关的词）的噪声包的干扰。 他们还探索了两种胶囊架构，如图7所示。分别为Capsule-A和Capsule-B。Capsule-A与[50]中的CapsNet类似。  Capsule-B使用三个并行网络，并在n-gram卷积层中使用具有不同窗口大小的过滤器，以学习更全面的文本表示形式。  CapsNet-B在实验中表现更好。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165324.png\" alt=\"\"></p>\n<p>Kim等人提出的基于CapsNet的模型[55]使用类似的架构。 该模型包括（1）一个输入层，该输入层将文档作为单词嵌入的序列；（2）卷积层，生成特征图并使用门控线性单元保留空间信息；（3）卷积胶囊层，通过聚合卷积层检测到的局部特征形成整体特征；（4）文本胶囊层以预测类标签。作者观察到，相比于图像，对象可以更自由地组合在文本中。 例如，即使某些句子的顺序改变了，文档的语义也可以保持不变，这与人脸上的眼睛和鼻子的位置不同。 因此，他们使用静态路由方案，该方案始终优于动态路由[50]进行文本分类。Aly等[56]提议使用CapsNets进行分层多标签分类（HMC），认为CapsNet编码子代关系的能力使其比传统方法更好地解决了HMC任务，在HMC任务中，文档被分配了一个或多个分类标签，这些标签被组织在一个层次结构。 他们模型的架构类似于[52，53，55]中的架构。任等人 [57]提出了CapsNets的另一种变体，它使用了胶囊之间的成分编码机制和基于k-means聚类的新路由算法。 首先，使用codebooks中的所有 codeword vectors 形成单词嵌入。 然后，通过k均值路由将下层胶囊捕获的特征汇总到高层胶囊中。</p>\n<h3 id=\"2-5-基于注意力机制模型\"><a href=\"#2-5-基于注意力机制模型\" class=\"headerlink\" title=\"2.5 基于注意力机制模型\"></a>2.5 基于注意力机制模型</h3><p>在开发用于NLP的深度学习模型时，注意力已成为越来越流行的概念和有用的工具[58，59]。 简而言之，语言模型中的注意力可被解释为重要权重的向量。 为了预测句子中的单词，我们使用注意力向量来估计它与其他单词的相关性或“与之相关”的程度，然后将注意力向量加权的值之和作为目标的近似值</p>\n<p>本节回顾了一些最突出的注意力模型，这些模型在发布时就在文本分类任务上取得了SOTA。</p>\n<p>杨等[60]提出了一种用于文本分类的分层注意力网络。 该模型具有两个鲜明的特征：（1）反映文档的层次结构的层次结构，（2）在单词和句子级别上应用的两个级别的注意力机制，使它能够在构建文档表示形式时以不同的方式参加重要或不重要的内容。 在六个文本分类任务上，该模型大大优于以前的方法。周等[61]将分层注意力模型扩展到跨语言情感分类。 在每种语言中，都使用LSTM网络对文档进行建模。 然后，通过使用分层注意机制实现分类，其中句子级别的注意模型了解文档的哪些句子对于确定总体情绪更重要。 而词级注意力模型则学习每个句子中哪些词具有决定性。</p>\n<p>沉等[62]提出了一种定向自我注意网络，用于无RNN / CNN语言理解，其中来自输入序列的元素之间的注意力是定向的和多维的。 轻量级神经网络仅基于所提出的注意力而无需任何RNN / CNN结构即可用于学习句子嵌入。刘等[63]提出了一个具有inner-attention的LSTM模型,用来做NLI任务。 该模型使用两阶段过程对句子进行编码。 首先，在词级Bi-LSTM上使用平均池化以生成第一阶段句子表示。 其次，采用注意力机制来代替同一句子的平均池，以获得更好的表示。 句子的第一阶段表示法用于出现在句子中的单词。</p>\n<p>注意模型也广泛应用于成对排序（pair-wise ranking）或匹配任务。  Santos等[64]提出了一种双向注意机制，称为Attentive Pooling（AP），用于成对排名。  AP可以使池化层知道当前的输入对（例如，问题-答案对），以使两个输入项的信息可以直接影响彼此表示的计算。 除了学习输入对的表示之外，AP联合学习该对投影段的相似性度量，然后为每个输入导出相应的注意力向量以指导池化。  AP是独立于底层表示学习的通用框架，并且可以应用于CNN和RNN，如图8（a）所示。Wang等[65]将文本分类视为标签-单词匹配问题：每个标签与单词向量一起嵌入相同的空间。作者介绍了一种注意力框架，该框架通过余弦相似性来度量文本序列和标签之间嵌入的兼容性，如图8（b）所示。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165837.png\" alt=\"\"></p>\n<p>Kim等[66]提出了一种使用密集连接的循环共同注意力网络的语义句子匹配方法。 类似于DenseNet [39]，该模型的每一层都使用所有先前的递归层的注意特征以及隐藏特征的级联信息。它可以保留从最底层单词嵌入层到最上层循环层的原始和共同注意特征信息。Yin等[67]提出了另一种基于注意力的CNN模型，用于句子对匹配。 他们提出了三种将句子之间的相互影响整合到CNN中的注意力方案，以便每个句子的表示都考虑到其成对的句子。 这些相互依赖的句子对表示形式比孤立的句子表示形式更为强大，这在包括答案选择，复述识别和文本蕴涵在内的多个分类任务中得到了验证。Tan等[68]在匹配聚合框架下采用了多种注意函数来匹配句子对。 杨等。  [69]介绍了一种基于注意力的神经匹配模型，用于对简短答案文本进行排名。 他们采用价值共享加权方案代替位置共享加权方案来组合不同的匹配信号，并使用问题关注网络将问题术语重要性学习纳入其中。 该模型在TREC QA数据集上取得了可喜的结果。</p>\n<p>还有其他有趣的注意力模型。  Lin等[70]使用自注意力来提取可解释的句子嵌入。  Wang等[71]提出了一种具有多尺度特征关注度的紧密连接的CNN，以产生可变的n-gram特征。  Yamada和Shindo [72] 使用neural attentive bag-of-entities 模型（使用知识库中的实体）进行文本分类。  Parikh等。  [73]使用注意力将问题分解为可以单独解决的子问题。  Chen等[74]探索了通用的池化方法来增强句子嵌入，并提出了一个基于向量的多头注意力模型。  Liu和Lane [75]提出了一种基于注意力的RNN模型，用于联合意图检测和空缺填充。</p>\n<h3 id=\"2-6-记忆增强网络\"><a href=\"#2-6-记忆增强网络\" class=\"headerlink\" title=\"2.6 记忆增强网络\"></a>2.6 记忆增强网络</h3><p>注意力模型在编码过程中存储的隐藏向量可以看作是模型内部记忆，而记忆增强网络则将神经网络与外部记忆结合在一起，模型可以对其进行读写。</p>\n<p>Munkhdalai和Yu[76]提出了一种记忆增强的神经网络，称为神经语义编码器（NSE），用于文本分类和QA。  NSE配备了一个可变大小的编码记忆，该编码记忆会随着时间的推移而发展，并通过读取，编写和写入操作保持对输入序列的理解，如图9所示。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200503113637.png\" alt=\"\"></p>\n<p>韦斯顿等[77]设计了一个用于综合QA任务的记忆网络，在该网络中，向模型提供了一系列语句（记忆记录），作为问题的支持事实。 该模型会根据问题和先前检索到的记忆来一次从记忆中检索一个条目。Sukhbaatar等[78]扩展了这项工作，并提出了端到端的记忆网络，在记忆网络中以柔和的方式利用注意力机制检索记忆条目，从而实现了端到端的训练。 他们表明，通过多次回合（跳数），该模型能够检索并推理几个支持事实，以回答特定问题。</p>\n<p>Kumar等[79]提出了一种动态记忆方法（DMN），它处理输入序列和问题，形成情节记忆，并产生相关的答案。 问题触发迭代注意进程，该进程允许模型将注意条件设置为输入和先前迭代的结果。 然后，在分层递归序列模型中对这些结果进行推理以生成答案。 对DMN进行端到端训练，并获得有关QA和POS标记的最新结果。 熊等[80]提出了DMN的详细分析，并改进了其就医和输入模块。</p>\n"},{"title":"个人可持续性发展","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2020-04-21T03:28:54.000Z","password":null,"summary":null,"img":null,"keywords":"可持续性发展","_content":"\n不要很拼 人们常说，身体是革命的本钱，年轻人最大的资本是什么，那就是年轻，年轻便意味着精力旺盛，身体好，可人终究是人，不可能像机器那样连轴转，机器坏了还能再换，人坏了可就再没有机会了。\n\n生活中有两个极端，一个是太拼，不把自己当人，为了工作也好，为了学习也好，总是使出十二分的力气，另一个是散漫，做什么都不积极，总是只出三分力气。\n\n可持续发展便是在两者之间寻找一个平衡点，既能把要做的做好，也能保护好自己。\n\n现如今的可持续发展领域涉及macro level、meso level，macro level 包括地球生态、气候变化等等，meso level 包括国家可持续性发展、社会可持续性发展等等，但在micro level方面涉及太少，包括个人和人际交往等等。\n\n在讨论可持续发展时，我们讨论的是环境，是我们身处的外界环境，到目前为止，可持续发展只针对基础设施，经济体系和社会的转型，而不是针对个人，不涉及个人的情感和心理发展以及身体健康等等。\n\n影响个人可持续性发展的原因主要有两个，一个是外部特征，例如身体健康，另一个是内部特征，包括情感、世界观、思想、价值观、需求以及愿望等等。 内部特征还存在着个体差异，支持个人可持续性的内在需求和品质可能会有所不同。 ","source":"_posts/个人可持续性发展.md","raw":"---\ntitle: 个人可持续性发展\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2020-04-21 11:28:54\npassword:\nsummary:\ntags: 可持续性发展\ncategories: 杂\nimg:\nkeywords: 可持续性发展\n---\n\n不要很拼 人们常说，身体是革命的本钱，年轻人最大的资本是什么，那就是年轻，年轻便意味着精力旺盛，身体好，可人终究是人，不可能像机器那样连轴转，机器坏了还能再换，人坏了可就再没有机会了。\n\n生活中有两个极端，一个是太拼，不把自己当人，为了工作也好，为了学习也好，总是使出十二分的力气，另一个是散漫，做什么都不积极，总是只出三分力气。\n\n可持续发展便是在两者之间寻找一个平衡点，既能把要做的做好，也能保护好自己。\n\n现如今的可持续发展领域涉及macro level、meso level，macro level 包括地球生态、气候变化等等，meso level 包括国家可持续性发展、社会可持续性发展等等，但在micro level方面涉及太少，包括个人和人际交往等等。\n\n在讨论可持续发展时，我们讨论的是环境，是我们身处的外界环境，到目前为止，可持续发展只针对基础设施，经济体系和社会的转型，而不是针对个人，不涉及个人的情感和心理发展以及身体健康等等。\n\n影响个人可持续性发展的原因主要有两个，一个是外部特征，例如身体健康，另一个是内部特征，包括情感、世界观、思想、价值观、需求以及愿望等等。 内部特征还存在着个体差异，支持个人可持续性的内在需求和品质可能会有所不同。 ","slug":"个人可持续性发展","published":1,"updated":"2020-08-20T08:41:44.649Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0man000uewse1x9fftgd","content":"<p>不要很拼 人们常说，身体是革命的本钱，年轻人最大的资本是什么，那就是年轻，年轻便意味着精力旺盛，身体好，可人终究是人，不可能像机器那样连轴转，机器坏了还能再换，人坏了可就再没有机会了。</p>\n<p>生活中有两个极端，一个是太拼，不把自己当人，为了工作也好，为了学习也好，总是使出十二分的力气，另一个是散漫，做什么都不积极，总是只出三分力气。</p>\n<p>可持续发展便是在两者之间寻找一个平衡点，既能把要做的做好，也能保护好自己。</p>\n<p>现如今的可持续发展领域涉及macro level、meso level，macro level 包括地球生态、气候变化等等，meso level 包括国家可持续性发展、社会可持续性发展等等，但在micro level方面涉及太少，包括个人和人际交往等等。</p>\n<p>在讨论可持续发展时，我们讨论的是环境，是我们身处的外界环境，到目前为止，可持续发展只针对基础设施，经济体系和社会的转型，而不是针对个人，不涉及个人的情感和心理发展以及身体健康等等。</p>\n<p>影响个人可持续性发展的原因主要有两个，一个是外部特征，例如身体健康，另一个是内部特征，包括情感、世界观、思想、价值观、需求以及愿望等等。 内部特征还存在着个体差异，支持个人可持续性的内在需求和品质可能会有所不同。 </p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>不要很拼 人们常说，身体是革命的本钱，年轻人最大的资本是什么，那就是年轻，年轻便意味着精力旺盛，身体好，可人终究是人，不可能像机器那样连轴转，机器坏了还能再换，人坏了可就再没有机会了。</p>\n<p>生活中有两个极端，一个是太拼，不把自己当人，为了工作也好，为了学习也好，总是使出十二分的力气，另一个是散漫，做什么都不积极，总是只出三分力气。</p>\n<p>可持续发展便是在两者之间寻找一个平衡点，既能把要做的做好，也能保护好自己。</p>\n<p>现如今的可持续发展领域涉及macro level、meso level，macro level 包括地球生态、气候变化等等，meso level 包括国家可持续性发展、社会可持续性发展等等，但在micro level方面涉及太少，包括个人和人际交往等等。</p>\n<p>在讨论可持续发展时，我们讨论的是环境，是我们身处的外界环境，到目前为止，可持续发展只针对基础设施，经济体系和社会的转型，而不是针对个人，不涉及个人的情感和心理发展以及身体健康等等。</p>\n<p>影响个人可持续性发展的原因主要有两个，一个是外部特征，例如身体健康，另一个是内部特征，包括情感、世界观、思想、价值观、需求以及愿望等等。 内部特征还存在着个体差异，支持个人可持续性的内在需求和品质可能会有所不同。 </p>\n"},{"title":"了不起的盖茨比","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-04-11T02:34:01.000Z","password":null,"summary":null,"img":"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/2020-04-11 11-53-25.jpg","keywords":"了不起的盖茨比","_content":"\n&emsp;&emsp;盖茨比——一个可怜、可爱、可敬勇士。\n\n&emsp;&emsp;盖茨比的了不起，千金散尽博红颜笑，纸醉金迷为心中念。盖茨比本人是极其单纯的，出生不好，却仰望星空，渴望走出大山走向世界；出生不好，却爱上豪门爱女，命运捉弄赶赴战场；出生不好，却怀有梦想，他未来的每一步都在向那个女孩靠近。尼克说，他们所有人加起来都比不上你，是的，一个如此痴情一步一步走来怀有美好梦想并一步一步践行的人，他们是比不上的，纵然尼克知道，盖茨比的梦已经渐行渐远。在城堡里盖茨比对尼克说“你对过去的理解是错的”，那时尼克笑了笑，没有说话，盖茨比苦涩一笑，重复了一句“是错的”。尼克或许从那时开始认识到眼前的这个人和其他人是不一样的。盖茨比一直追寻的或许从来不是什么黛西，他爱着黛西，追寻着黛西，却在这之上追寻着更多或许不切实际的东西。就像尼克说的，如果盖茨比要求的只是拥黛西入怀，或许一切都不一样。最终，盖茨比终究还是带着伟大的美好的梦想一起破灭了，他等的电话没等到，他的葬礼，黛西没有来，甚至没有一束花，名门望流不见踪影，只有尼克——他唯一的朋友。尼克很庆幸，庆幸自己当面称赞了盖茨比。\n\n> ​\t\tI offer you whatever insight my books may hold. \n>\n> ​\t\tI offer you the loyalty of a man who has never been loyal. \n>\n> ​\t\tI offer you that kernel of myself that I have saved somehow – the central heart that deals not in words, traffics not with dreams and is untouched by time, by joy, by adversities. \n>\n> ​\t\tI offer you the memory of a yellow rose seen at sunset, years before you were born. \n>\n> ​\t\tI offer you explanations of yourself, theories about yourself, authentic and surprising news of yourself. \n>\n> ​\t\tI can give you my loneliness my darkness, the hunger of my heart.\n>\n> ​\t\tI offer you everything I have, but you never came back.\n>\n> ​\t\tWhat can I hold you with?\n\n&emsp;&emsp;“美国梦”时代，纸醉金迷的享乐时代，尼克就是“美国梦”的一员，尼克帮人保守了两次秘密，第一次是汤姆邀请他参与的私人狂欢，在那儿他第一次尝试到了甜头，沉迷其中又置身事外，始终对其抱着一丝审视的心态，第二次是帮盖茨比保守秘密，不过稍有不同的是这次并没有沉迷其中，或者说这是盖茨比和汤姆的不同，就像汤姆对盖茨比说的“我们所有人都和你不一样，那是一种源自身体，源自血肉的不一样，我们流的血是不一样的”。的确，盖茨比所拥有的一切，所表现出来的种种行为迹象都是为了掩盖心里的伤疤，不过这并没有什么，他本就是如此，虽有欺骗，却也被大众认可，反观汤姆和黛西，一个花心出轨，为了自保漠视情人的遭遇，另一个也是出轨，肇事逃逸就罢了，最终盖茨比死后被盖上了原本是这两人的罪名，不求她为盖茨比做多大牺牲，盖茨比的葬礼，她一束花都没有，这太残忍了，是黄金时代造就了他们，还是本就如此？尼克生日那天，他说他失望透了，他当然失望了，他始终都是一个审视者，他怀抱梦想来到纽约，带着对这个城市的厌恶、恶心离去。\n\n&emsp;&emsp;文学的尽头都或多或少牵扯到虚无主义，《雪国》中驹子的遭遇也好，盖茨比的遭遇也好，都是理想与现实冲撞下的无可奈何的事实，梦，总是那么遥远，却又不可或缺，可远观不可亵玩。\n\n\n\n","source":"_posts/了不起的盖茨比.md","raw":"---\ntitle: 了不起的盖茨比\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-04-11 10:34:01\npassword:\nsummary:\ntags: 影评\ncategories: 影评\nimg: https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/2020-04-11 11-53-25.jpg\nkeywords: 了不起的盖茨比\n---\n\n&emsp;&emsp;盖茨比——一个可怜、可爱、可敬勇士。\n\n&emsp;&emsp;盖茨比的了不起，千金散尽博红颜笑，纸醉金迷为心中念。盖茨比本人是极其单纯的，出生不好，却仰望星空，渴望走出大山走向世界；出生不好，却爱上豪门爱女，命运捉弄赶赴战场；出生不好，却怀有梦想，他未来的每一步都在向那个女孩靠近。尼克说，他们所有人加起来都比不上你，是的，一个如此痴情一步一步走来怀有美好梦想并一步一步践行的人，他们是比不上的，纵然尼克知道，盖茨比的梦已经渐行渐远。在城堡里盖茨比对尼克说“你对过去的理解是错的”，那时尼克笑了笑，没有说话，盖茨比苦涩一笑，重复了一句“是错的”。尼克或许从那时开始认识到眼前的这个人和其他人是不一样的。盖茨比一直追寻的或许从来不是什么黛西，他爱着黛西，追寻着黛西，却在这之上追寻着更多或许不切实际的东西。就像尼克说的，如果盖茨比要求的只是拥黛西入怀，或许一切都不一样。最终，盖茨比终究还是带着伟大的美好的梦想一起破灭了，他等的电话没等到，他的葬礼，黛西没有来，甚至没有一束花，名门望流不见踪影，只有尼克——他唯一的朋友。尼克很庆幸，庆幸自己当面称赞了盖茨比。\n\n> ​\t\tI offer you whatever insight my books may hold. \n>\n> ​\t\tI offer you the loyalty of a man who has never been loyal. \n>\n> ​\t\tI offer you that kernel of myself that I have saved somehow – the central heart that deals not in words, traffics not with dreams and is untouched by time, by joy, by adversities. \n>\n> ​\t\tI offer you the memory of a yellow rose seen at sunset, years before you were born. \n>\n> ​\t\tI offer you explanations of yourself, theories about yourself, authentic and surprising news of yourself. \n>\n> ​\t\tI can give you my loneliness my darkness, the hunger of my heart.\n>\n> ​\t\tI offer you everything I have, but you never came back.\n>\n> ​\t\tWhat can I hold you with?\n\n&emsp;&emsp;“美国梦”时代，纸醉金迷的享乐时代，尼克就是“美国梦”的一员，尼克帮人保守了两次秘密，第一次是汤姆邀请他参与的私人狂欢，在那儿他第一次尝试到了甜头，沉迷其中又置身事外，始终对其抱着一丝审视的心态，第二次是帮盖茨比保守秘密，不过稍有不同的是这次并没有沉迷其中，或者说这是盖茨比和汤姆的不同，就像汤姆对盖茨比说的“我们所有人都和你不一样，那是一种源自身体，源自血肉的不一样，我们流的血是不一样的”。的确，盖茨比所拥有的一切，所表现出来的种种行为迹象都是为了掩盖心里的伤疤，不过这并没有什么，他本就是如此，虽有欺骗，却也被大众认可，反观汤姆和黛西，一个花心出轨，为了自保漠视情人的遭遇，另一个也是出轨，肇事逃逸就罢了，最终盖茨比死后被盖上了原本是这两人的罪名，不求她为盖茨比做多大牺牲，盖茨比的葬礼，她一束花都没有，这太残忍了，是黄金时代造就了他们，还是本就如此？尼克生日那天，他说他失望透了，他当然失望了，他始终都是一个审视者，他怀抱梦想来到纽约，带着对这个城市的厌恶、恶心离去。\n\n&emsp;&emsp;文学的尽头都或多或少牵扯到虚无主义，《雪国》中驹子的遭遇也好，盖茨比的遭遇也好，都是理想与现实冲撞下的无可奈何的事实，梦，总是那么遥远，却又不可或缺，可远观不可亵玩。\n\n\n\n","slug":"了不起的盖茨比","published":1,"updated":"2020-08-20T08:41:44.650Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mar000yewse6tcgauhj","content":"<p>  盖茨比——一个可怜、可爱、可敬勇士。</p>\n<p>  盖茨比的了不起，千金散尽博红颜笑，纸醉金迷为心中念。盖茨比本人是极其单纯的，出生不好，却仰望星空，渴望走出大山走向世界；出生不好，却爱上豪门爱女，命运捉弄赶赴战场；出生不好，却怀有梦想，他未来的每一步都在向那个女孩靠近。尼克说，他们所有人加起来都比不上你，是的，一个如此痴情一步一步走来怀有美好梦想并一步一步践行的人，他们是比不上的，纵然尼克知道，盖茨比的梦已经渐行渐远。在城堡里盖茨比对尼克说“你对过去的理解是错的”，那时尼克笑了笑，没有说话，盖茨比苦涩一笑，重复了一句“是错的”。尼克或许从那时开始认识到眼前的这个人和其他人是不一样的。盖茨比一直追寻的或许从来不是什么黛西，他爱着黛西，追寻着黛西，却在这之上追寻着更多或许不切实际的东西。就像尼克说的，如果盖茨比要求的只是拥黛西入怀，或许一切都不一样。最终，盖茨比终究还是带着伟大的美好的梦想一起破灭了，他等的电话没等到，他的葬礼，黛西没有来，甚至没有一束花，名门望流不见踪影，只有尼克——他唯一的朋友。尼克很庆幸，庆幸自己当面称赞了盖茨比。</p>\n<blockquote>\n<p>​        I offer you whatever insight my books may hold. </p>\n<p>​        I offer you the loyalty of a man who has never been loyal. </p>\n<p>​        I offer you that kernel of myself that I have saved somehow – the central heart that deals not in words, traffics not with dreams and is untouched by time, by joy, by adversities. </p>\n<p>​        I offer you the memory of a yellow rose seen at sunset, years before you were born. </p>\n<p>​        I offer you explanations of yourself, theories about yourself, authentic and surprising news of yourself. </p>\n<p>​        I can give you my loneliness my darkness, the hunger of my heart.</p>\n<p>​        I offer you everything I have, but you never came back.</p>\n<p>​        What can I hold you with?</p>\n</blockquote>\n<p>  “美国梦”时代，纸醉金迷的享乐时代，尼克就是“美国梦”的一员，尼克帮人保守了两次秘密，第一次是汤姆邀请他参与的私人狂欢，在那儿他第一次尝试到了甜头，沉迷其中又置身事外，始终对其抱着一丝审视的心态，第二次是帮盖茨比保守秘密，不过稍有不同的是这次并没有沉迷其中，或者说这是盖茨比和汤姆的不同，就像汤姆对盖茨比说的“我们所有人都和你不一样，那是一种源自身体，源自血肉的不一样，我们流的血是不一样的”。的确，盖茨比所拥有的一切，所表现出来的种种行为迹象都是为了掩盖心里的伤疤，不过这并没有什么，他本就是如此，虽有欺骗，却也被大众认可，反观汤姆和黛西，一个花心出轨，为了自保漠视情人的遭遇，另一个也是出轨，肇事逃逸就罢了，最终盖茨比死后被盖上了原本是这两人的罪名，不求她为盖茨比做多大牺牲，盖茨比的葬礼，她一束花都没有，这太残忍了，是黄金时代造就了他们，还是本就如此？尼克生日那天，他说他失望透了，他当然失望了，他始终都是一个审视者，他怀抱梦想来到纽约，带着对这个城市的厌恶、恶心离去。</p>\n<p>  文学的尽头都或多或少牵扯到虚无主义，《雪国》中驹子的遭遇也好，盖茨比的遭遇也好，都是理想与现实冲撞下的无可奈何的事实，梦，总是那么遥远，却又不可或缺，可远观不可亵玩。</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>&emsp;&emsp;盖茨比——一个可怜、可爱、可敬勇士。</p>\n<p>&emsp;&emsp;盖茨比的了不起，千金散尽博红颜笑，纸醉金迷为心中念。盖茨比本人是极其单纯的，出生不好，却仰望星空，渴望走出大山走向世界；出生不好，却爱上豪门爱女，命运捉弄赶赴战场；出生不好，却怀有梦想，他未来的每一步都在向那个女孩靠近。尼克说，他们所有人加起来都比不上你，是的，一个如此痴情一步一步走来怀有美好梦想并一步一步践行的人，他们是比不上的，纵然尼克知道，盖茨比的梦已经渐行渐远。在城堡里盖茨比对尼克说“你对过去的理解是错的”，那时尼克笑了笑，没有说话，盖茨比苦涩一笑，重复了一句“是错的”。尼克或许从那时开始认识到眼前的这个人和其他人是不一样的。盖茨比一直追寻的或许从来不是什么黛西，他爱着黛西，追寻着黛西，却在这之上追寻着更多或许不切实际的东西。就像尼克说的，如果盖茨比要求的只是拥黛西入怀，或许一切都不一样。最终，盖茨比终究还是带着伟大的美好的梦想一起破灭了，他等的电话没等到，他的葬礼，黛西没有来，甚至没有一束花，名门望流不见踪影，只有尼克——他唯一的朋友。尼克很庆幸，庆幸自己当面称赞了盖茨比。</p>\n<blockquote>\n<p>​        I offer you whatever insight my books may hold. </p>\n<p>​        I offer you the loyalty of a man who has never been loyal. </p>\n<p>​        I offer you that kernel of myself that I have saved somehow – the central heart that deals not in words, traffics not with dreams and is untouched by time, by joy, by adversities. </p>\n<p>​        I offer you the memory of a yellow rose seen at sunset, years before you were born. </p>\n<p>​        I offer you explanations of yourself, theories about yourself, authentic and surprising news of yourself. </p>\n<p>​        I can give you my loneliness my darkness, the hunger of my heart.</p>\n<p>​        I offer you everything I have, but you never came back.</p>\n<p>​        What can I hold you with?</p>\n</blockquote>\n<p>&emsp;&emsp;“美国梦”时代，纸醉金迷的享乐时代，尼克就是“美国梦”的一员，尼克帮人保守了两次秘密，第一次是汤姆邀请他参与的私人狂欢，在那儿他第一次尝试到了甜头，沉迷其中又置身事外，始终对其抱着一丝审视的心态，第二次是帮盖茨比保守秘密，不过稍有不同的是这次并没有沉迷其中，或者说这是盖茨比和汤姆的不同，就像汤姆对盖茨比说的“我们所有人都和你不一样，那是一种源自身体，源自血肉的不一样，我们流的血是不一样的”。的确，盖茨比所拥有的一切，所表现出来的种种行为迹象都是为了掩盖心里的伤疤，不过这并没有什么，他本就是如此，虽有欺骗，却也被大众认可，反观汤姆和黛西，一个花心出轨，为了自保漠视情人的遭遇，另一个也是出轨，肇事逃逸就罢了，最终盖茨比死后被盖上了原本是这两人的罪名，不求她为盖茨比做多大牺牲，盖茨比的葬礼，她一束花都没有，这太残忍了，是黄金时代造就了他们，还是本就如此？尼克生日那天，他说他失望透了，他当然失望了，他始终都是一个审视者，他怀抱梦想来到纽约，带着对这个城市的厌恶、恶心离去。</p>\n<p>&emsp;&emsp;文学的尽头都或多或少牵扯到虚无主义，《雪国》中驹子的遭遇也好，盖茨比的遭遇也好，都是理想与现实冲撞下的无可奈何的事实，梦，总是那么遥远，却又不可或缺，可远观不可亵玩。</p>\n"},{"title":"异构图神经网络","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-06-07T02:21:53.000Z","password":null,"summary":null,"img":null,"keywords":"异构图 图网络","_content":"\n转自https://www.jiqizhixin.com/articles/2019-10-14-5\n\n### **什么是异构图？**\n\n传统**同构图**（Homogeneous Graph）数据中只存在一种节点和边，因此在构建图神经网络时所有节点共享同样的模型参数并且拥有同样维度的特征空间。而**异构图**（Heterogeneous Graph）中可以存在不只一种节点和边，因此允许不同类型的节点拥有不同维度的特征或属性。\n\n这一特点使得异构图的应用十分广泛。事实上，如果用图来描述我们和周围事物的关系就会发现所产生的图都是天然异构的。比如我今天看了电影《流浪地球》，那“我”作为观众和电影《流浪地球》之间就建立了“看了”这一关系。异构图可以用来描述这种交互关系的集合。这个图分“观众”和“电影”两类节点，以及“看了”这一类边。“我”作为观众，和电影所具有的属性一定是不同的，需要用不同的模型或者不同的特征维度来表达。这张图就天然具有了异构性。\n\n再比如我去豆瓣上给《流浪地球》评了8分，那“我”和《流浪地球》之间就又建立了“评分”这一关系。“评分”和“看了”的属性也一定是不同的，如前者包含评分分数，后者则包含票价等。\n\n因此，很多用于机器学习的数据都可以用异构图来建模，而需要解决的任务也可以通过学习图上的点和边，或是整张图的表示来解决。这里举几个例子。\n\n**推荐系统![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021921144.png)**\n\n<center>图一：使用异构图建模推荐系统。<center>\n\n<center>（图源：https://peterxugo.github.io/2017/09/28/推荐系统/）<center>\n\n常见的推荐系统数据由用户和商品的交互信息组成。这些交互信息可以用图来表示（图一）。由于用户节点可能包含用户的年龄、职业等个人信息，而商品节点则包含内容、价格等商品特有的信息，所以该图是一个异构图。\n\n推荐系统可能感兴趣的内容包括：\n\n- 判断某个客户是否会观看某部电影，以及在什么样的促销下会购票。这个问题可以转化成**链接预测**（Link Prediction）问题：判断两个点之间是否会有连边。\n- 判断某个客户是否存在恶意刷分，或者某部电影是否遭到恶意刷分。根据标注是否存在，这个问题可以转化成**半监督节点分类**（Semi-supervised Node Classification）或者无监督的**异常检测**（Anomaly Detection）问题。\n\n**学术网络**\n\n学术网络的图结构更复杂一些。它可以包含“论文”、“作者”、“会议”和“机构”等节点类型。每个作者会隶属于某些机构，发表某些论文，而每篇论文又会被发表在某个会议上，也会引用其它的论文。每个节点同样可以有自己的特征——如论文可以用摘要当作特征等。同样，这些关系可以表示成一张异构图（图二）。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021950189.png)\n\n<center>图二：学术网络<center>\n\n<center>（图源：Dong et al., KDD 2017）<center>\n\n同样，**链接预测**和**节点分类**问题也适用于学术网络中。例如预测某个作者的研究领域或所属的研究团队、一篇论文或者一个作者的影响力等等。\n\n**知识图谱**\n\n知识图谱由于包含不同种类的节点和边，是一张天然的异构图。如下图，“人物”、“职业”和“荣誉”等都可以作为节点类型，而节点之间的关系也有“就职”、“获得荣誉”等多种类别。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021975987.png)\n\n<center>图三：Wikidata知识图谱示例<center>\n\n**图嵌入学习**在知识图谱上也是一个很重要的任务。它的目标是对每一个节点和每一类关系学习一个高维向量表示。此外，知识图谱补全、基于知识图谱的推理以及将知识图谱与其他系统结合（比如推荐系统）等也是重要的任务。\n\n### **有哪些针对异构图的图神经网络？**\n\n针对异构图设计图神经网络还是个非常新的方向。虽然论文不多，但已经在一些场景中取得了很不错的效果。这里简单介绍两个模型。\n\n**Relational Graph Convolutional Network (RGCN)**\n\n文章链接：https://arxiv.org/abs/1703.06103\n\nRGCN是一个在异构图上学习节点表示的早期工作。与Graph Convolutional Network（GCN）类似，它仍然基于**消息传递**（Message Passing）。但与GCN不同，每条边上的消息通过每类边独有的线性变换得到。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022001779.png)\n\nRGCN可以用于节点分类和链接预测等任务。\n\n**Graph Convolutional Matrix Completion (GCMC)**\n\n文章链接：https://arxiv.org/abs/1706.02263\n\nGCMC是在异构图上做产品推荐任务的一个尝试。它在一个由观众和电影组成的二分图上训练。观众和电影有各自的特征，而且每一条边上有对应的观众对电影的评分。GCMC将每一种评分视为一类边。\n\n图构建完成后，每个节点通过消息传递的形式，汇总从自己邻居传递来的消息，去更新自己的表示。与RGCN不同的是，GCMC只进行一层消息传递：\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022013579.png)\n\n消息同样是由源节点的原表示通过线性变换得到。与RGCN类似，每类边都有自己线性变换的参数矩阵。不同之处在于**多关系累和函数**accum有各种灵活选择。![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022024012.png)\n\nGCMC算出两类点的最终表示之后，再通过softmax得出某个交互下每种评分的概率：\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022036753.png)\n\n除了以上模型外，今年WWW还有诸如Heterogeneous Graph Attention Network (HAN)，Knowledge Graph Convolution Networks for Recommender Systems (KGCN) 等关于异构图的好工作。此外，在网络嵌入（network embedding）方向还有如metapath2vec等经典的工作。\n\n- Heterogeneous Graph Attention Network (WWW’19)\n- Knowledge Graph Convolution Networks for Recommender Systems (WWW’19)\n- HetGNN: Heterogeneous Graph Neural Network (KDD’19)\n- Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation (KDD’19)\n- KGAT: Knowledge Graph Attention Network for Recommendation (KDD’19)\n- metapath2vec: Scalable Representation Learning for Heterogeneous Networks (KDD’17)","source":"_posts/异构图神经网络.md","raw":"---\ntitle: 异构图神经网络\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-06-07 10:21:53\npassword:\nsummary:\ntags:\n\t- 异构图\n\t- 图网络\ncategories: 图网络\nimg:\nkeywords: 异构图 图网络\n---\n\n转自https://www.jiqizhixin.com/articles/2019-10-14-5\n\n### **什么是异构图？**\n\n传统**同构图**（Homogeneous Graph）数据中只存在一种节点和边，因此在构建图神经网络时所有节点共享同样的模型参数并且拥有同样维度的特征空间。而**异构图**（Heterogeneous Graph）中可以存在不只一种节点和边，因此允许不同类型的节点拥有不同维度的特征或属性。\n\n这一特点使得异构图的应用十分广泛。事实上，如果用图来描述我们和周围事物的关系就会发现所产生的图都是天然异构的。比如我今天看了电影《流浪地球》，那“我”作为观众和电影《流浪地球》之间就建立了“看了”这一关系。异构图可以用来描述这种交互关系的集合。这个图分“观众”和“电影”两类节点，以及“看了”这一类边。“我”作为观众，和电影所具有的属性一定是不同的，需要用不同的模型或者不同的特征维度来表达。这张图就天然具有了异构性。\n\n再比如我去豆瓣上给《流浪地球》评了8分，那“我”和《流浪地球》之间就又建立了“评分”这一关系。“评分”和“看了”的属性也一定是不同的，如前者包含评分分数，后者则包含票价等。\n\n因此，很多用于机器学习的数据都可以用异构图来建模，而需要解决的任务也可以通过学习图上的点和边，或是整张图的表示来解决。这里举几个例子。\n\n**推荐系统![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021921144.png)**\n\n<center>图一：使用异构图建模推荐系统。<center>\n\n<center>（图源：https://peterxugo.github.io/2017/09/28/推荐系统/）<center>\n\n常见的推荐系统数据由用户和商品的交互信息组成。这些交互信息可以用图来表示（图一）。由于用户节点可能包含用户的年龄、职业等个人信息，而商品节点则包含内容、价格等商品特有的信息，所以该图是一个异构图。\n\n推荐系统可能感兴趣的内容包括：\n\n- 判断某个客户是否会观看某部电影，以及在什么样的促销下会购票。这个问题可以转化成**链接预测**（Link Prediction）问题：判断两个点之间是否会有连边。\n- 判断某个客户是否存在恶意刷分，或者某部电影是否遭到恶意刷分。根据标注是否存在，这个问题可以转化成**半监督节点分类**（Semi-supervised Node Classification）或者无监督的**异常检测**（Anomaly Detection）问题。\n\n**学术网络**\n\n学术网络的图结构更复杂一些。它可以包含“论文”、“作者”、“会议”和“机构”等节点类型。每个作者会隶属于某些机构，发表某些论文，而每篇论文又会被发表在某个会议上，也会引用其它的论文。每个节点同样可以有自己的特征——如论文可以用摘要当作特征等。同样，这些关系可以表示成一张异构图（图二）。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021950189.png)\n\n<center>图二：学术网络<center>\n\n<center>（图源：Dong et al., KDD 2017）<center>\n\n同样，**链接预测**和**节点分类**问题也适用于学术网络中。例如预测某个作者的研究领域或所属的研究团队、一篇论文或者一个作者的影响力等等。\n\n**知识图谱**\n\n知识图谱由于包含不同种类的节点和边，是一张天然的异构图。如下图，“人物”、“职业”和“荣誉”等都可以作为节点类型，而节点之间的关系也有“就职”、“获得荣誉”等多种类别。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021975987.png)\n\n<center>图三：Wikidata知识图谱示例<center>\n\n**图嵌入学习**在知识图谱上也是一个很重要的任务。它的目标是对每一个节点和每一类关系学习一个高维向量表示。此外，知识图谱补全、基于知识图谱的推理以及将知识图谱与其他系统结合（比如推荐系统）等也是重要的任务。\n\n### **有哪些针对异构图的图神经网络？**\n\n针对异构图设计图神经网络还是个非常新的方向。虽然论文不多，但已经在一些场景中取得了很不错的效果。这里简单介绍两个模型。\n\n**Relational Graph Convolutional Network (RGCN)**\n\n文章链接：https://arxiv.org/abs/1703.06103\n\nRGCN是一个在异构图上学习节点表示的早期工作。与Graph Convolutional Network（GCN）类似，它仍然基于**消息传递**（Message Passing）。但与GCN不同，每条边上的消息通过每类边独有的线性变换得到。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022001779.png)\n\nRGCN可以用于节点分类和链接预测等任务。\n\n**Graph Convolutional Matrix Completion (GCMC)**\n\n文章链接：https://arxiv.org/abs/1706.02263\n\nGCMC是在异构图上做产品推荐任务的一个尝试。它在一个由观众和电影组成的二分图上训练。观众和电影有各自的特征，而且每一条边上有对应的观众对电影的评分。GCMC将每一种评分视为一类边。\n\n图构建完成后，每个节点通过消息传递的形式，汇总从自己邻居传递来的消息，去更新自己的表示。与RGCN不同的是，GCMC只进行一层消息传递：\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022013579.png)\n\n消息同样是由源节点的原表示通过线性变换得到。与RGCN类似，每类边都有自己线性变换的参数矩阵。不同之处在于**多关系累和函数**accum有各种灵活选择。![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022024012.png)\n\nGCMC算出两类点的最终表示之后，再通过softmax得出某个交互下每种评分的概率：\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022036753.png)\n\n除了以上模型外，今年WWW还有诸如Heterogeneous Graph Attention Network (HAN)，Knowledge Graph Convolution Networks for Recommender Systems (KGCN) 等关于异构图的好工作。此外，在网络嵌入（network embedding）方向还有如metapath2vec等经典的工作。\n\n- Heterogeneous Graph Attention Network (WWW’19)\n- Knowledge Graph Convolution Networks for Recommender Systems (WWW’19)\n- HetGNN: Heterogeneous Graph Neural Network (KDD’19)\n- Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation (KDD’19)\n- KGAT: Knowledge Graph Attention Network for Recommendation (KDD’19)\n- metapath2vec: Scalable Representation Learning for Heterogeneous Networks (KDD’17)","slug":"异构图神经网络","published":1,"updated":"2020-08-20T08:41:44.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mau000zewsecikha55f","content":"<p>转自<a href=\"https://www.jiqizhixin.com/articles/2019-10-14-5\" target=\"_blank\" rel=\"noopener\">https://www.jiqizhixin.com/articles/2019-10-14-5</a></p>\n<h3 id=\"什么是异构图？\"><a href=\"#什么是异构图？\" class=\"headerlink\" title=\"什么是异构图？\"></a><strong>什么是异构图？</strong></h3><p>传统<strong>同构图</strong>（Homogeneous Graph）数据中只存在一种节点和边，因此在构建图神经网络时所有节点共享同样的模型参数并且拥有同样维度的特征空间。而<strong>异构图</strong>（Heterogeneous Graph）中可以存在不只一种节点和边，因此允许不同类型的节点拥有不同维度的特征或属性。</p>\n<p>这一特点使得异构图的应用十分广泛。事实上，如果用图来描述我们和周围事物的关系就会发现所产生的图都是天然异构的。比如我今天看了电影《流浪地球》，那“我”作为观众和电影《流浪地球》之间就建立了“看了”这一关系。异构图可以用来描述这种交互关系的集合。这个图分“观众”和“电影”两类节点，以及“看了”这一类边。“我”作为观众，和电影所具有的属性一定是不同的，需要用不同的模型或者不同的特征维度来表达。这张图就天然具有了异构性。</p>\n<p>再比如我去豆瓣上给《流浪地球》评了8分，那“我”和《流浪地球》之间就又建立了“评分”这一关系。“评分”和“看了”的属性也一定是不同的，如前者包含评分分数，后者则包含票价等。</p>\n<p>因此，很多用于机器学习的数据都可以用异构图来建模，而需要解决的任务也可以通过学习图上的点和边，或是整张图的表示来解决。这里举几个例子。</p>\n<p><strong>推荐系统<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021921144.png\" alt=\"img\"></strong></p>\n<p></p><center>图一：使用异构图建模推荐系统。<center><p></p>\n<p></p><center>（图源：<a href=\"https://peterxugo.github.io/2017/09/28/推荐系统/）\" target=\"_blank\" rel=\"noopener\">https://peterxugo.github.io/2017/09/28/推荐系统/）</a><center><p></p>\n<p>常见的推荐系统数据由用户和商品的交互信息组成。这些交互信息可以用图来表示（图一）。由于用户节点可能包含用户的年龄、职业等个人信息，而商品节点则包含内容、价格等商品特有的信息，所以该图是一个异构图。</p>\n<p>推荐系统可能感兴趣的内容包括：</p>\n<ul>\n<li>判断某个客户是否会观看某部电影，以及在什么样的促销下会购票。这个问题可以转化成<strong>链接预测</strong>（Link Prediction）问题：判断两个点之间是否会有连边。</li>\n<li>判断某个客户是否存在恶意刷分，或者某部电影是否遭到恶意刷分。根据标注是否存在，这个问题可以转化成<strong>半监督节点分类</strong>（Semi-supervised Node Classification）或者无监督的<strong>异常检测</strong>（Anomaly Detection）问题。</li>\n</ul>\n<p><strong>学术网络</strong></p>\n<p>学术网络的图结构更复杂一些。它可以包含“论文”、“作者”、“会议”和“机构”等节点类型。每个作者会隶属于某些机构，发表某些论文，而每篇论文又会被发表在某个会议上，也会引用其它的论文。每个节点同样可以有自己的特征——如论文可以用摘要当作特征等。同样，这些关系可以表示成一张异构图（图二）。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021950189.png\" alt=\"img\"></p>\n<p></p><center>图二：学术网络<center><p></p>\n<p></p><center>（图源：Dong et al., KDD 2017）<center><p></p>\n<p>同样，<strong>链接预测</strong>和<strong>节点分类</strong>问题也适用于学术网络中。例如预测某个作者的研究领域或所属的研究团队、一篇论文或者一个作者的影响力等等。</p>\n<p><strong>知识图谱</strong></p>\n<p>知识图谱由于包含不同种类的节点和边，是一张天然的异构图。如下图，“人物”、“职业”和“荣誉”等都可以作为节点类型，而节点之间的关系也有“就职”、“获得荣誉”等多种类别。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021975987.png\" alt=\"img\"></p>\n<p></p><center>图三：Wikidata知识图谱示例<center><p></p>\n<p><strong>图嵌入学习</strong>在知识图谱上也是一个很重要的任务。它的目标是对每一个节点和每一类关系学习一个高维向量表示。此外，知识图谱补全、基于知识图谱的推理以及将知识图谱与其他系统结合（比如推荐系统）等也是重要的任务。</p>\n<h3 id=\"有哪些针对异构图的图神经网络？\"><a href=\"#有哪些针对异构图的图神经网络？\" class=\"headerlink\" title=\"有哪些针对异构图的图神经网络？\"></a><strong>有哪些针对异构图的图神经网络？</strong></h3><p>针对异构图设计图神经网络还是个非常新的方向。虽然论文不多，但已经在一些场景中取得了很不错的效果。这里简单介绍两个模型。</p>\n<p><strong>Relational Graph Convolutional Network (RGCN)</strong></p>\n<p>文章链接：<a href=\"https://arxiv.org/abs/1703.06103\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1703.06103</a></p>\n<p>RGCN是一个在异构图上学习节点表示的早期工作。与Graph Convolutional Network（GCN）类似，它仍然基于<strong>消息传递</strong>（Message Passing）。但与GCN不同，每条边上的消息通过每类边独有的线性变换得到。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022001779.png\" alt=\"img\"></p>\n<p>RGCN可以用于节点分类和链接预测等任务。</p>\n<p><strong>Graph Convolutional Matrix Completion (GCMC)</strong></p>\n<p>文章链接：<a href=\"https://arxiv.org/abs/1706.02263\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1706.02263</a></p>\n<p>GCMC是在异构图上做产品推荐任务的一个尝试。它在一个由观众和电影组成的二分图上训练。观众和电影有各自的特征，而且每一条边上有对应的观众对电影的评分。GCMC将每一种评分视为一类边。</p>\n<p>图构建完成后，每个节点通过消息传递的形式，汇总从自己邻居传递来的消息，去更新自己的表示。与RGCN不同的是，GCMC只进行一层消息传递：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022013579.png\" alt=\"img\"></p>\n<p>消息同样是由源节点的原表示通过线性变换得到。与RGCN类似，每类边都有自己线性变换的参数矩阵。不同之处在于<strong>多关系累和函数</strong>accum有各种灵活选择。<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022024012.png\" alt=\"img\"></p>\n<p>GCMC算出两类点的最终表示之后，再通过softmax得出某个交互下每种评分的概率：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022036753.png\" alt=\"img\"></p>\n<p>除了以上模型外，今年WWW还有诸如Heterogeneous Graph Attention Network (HAN)，Knowledge Graph Convolution Networks for Recommender Systems (KGCN) 等关于异构图的好工作。此外，在网络嵌入（network embedding）方向还有如metapath2vec等经典的工作。</p>\n<ul>\n<li>Heterogeneous Graph Attention Network (WWW’19)</li>\n<li>Knowledge Graph Convolution Networks for Recommender Systems (WWW’19)</li>\n<li>HetGNN: Heterogeneous Graph Neural Network (KDD’19)</li>\n<li>Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation (KDD’19)</li>\n<li>KGAT: Knowledge Graph Attention Network for Recommendation (KDD’19)</li>\n<li>metapath2vec: Scalable Representation Learning for Heterogeneous Networks (KDD’17)</li>\n</ul>\n</center></center></center></center></center></center></center></center></center></center><script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>转自<a href=\"https://www.jiqizhixin.com/articles/2019-10-14-5\" target=\"_blank\" rel=\"noopener\">https://www.jiqizhixin.com/articles/2019-10-14-5</a></p>\n<h3 id=\"什么是异构图？\"><a href=\"#什么是异构图？\" class=\"headerlink\" title=\"什么是异构图？\"></a><strong>什么是异构图？</strong></h3><p>传统<strong>同构图</strong>（Homogeneous Graph）数据中只存在一种节点和边，因此在构建图神经网络时所有节点共享同样的模型参数并且拥有同样维度的特征空间。而<strong>异构图</strong>（Heterogeneous Graph）中可以存在不只一种节点和边，因此允许不同类型的节点拥有不同维度的特征或属性。</p>\n<p>这一特点使得异构图的应用十分广泛。事实上，如果用图来描述我们和周围事物的关系就会发现所产生的图都是天然异构的。比如我今天看了电影《流浪地球》，那“我”作为观众和电影《流浪地球》之间就建立了“看了”这一关系。异构图可以用来描述这种交互关系的集合。这个图分“观众”和“电影”两类节点，以及“看了”这一类边。“我”作为观众，和电影所具有的属性一定是不同的，需要用不同的模型或者不同的特征维度来表达。这张图就天然具有了异构性。</p>\n<p>再比如我去豆瓣上给《流浪地球》评了8分，那“我”和《流浪地球》之间就又建立了“评分”这一关系。“评分”和“看了”的属性也一定是不同的，如前者包含评分分数，后者则包含票价等。</p>\n<p>因此，很多用于机器学习的数据都可以用异构图来建模，而需要解决的任务也可以通过学习图上的点和边，或是整张图的表示来解决。这里举几个例子。</p>\n<p><strong>推荐系统<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021921144.png\" alt=\"img\"></strong></p>\n<p><center>图一：使用异构图建模推荐系统。<center></p>\n<p><center>（图源：<a href=\"https://peterxugo.github.io/2017/09/28/推荐系统/）\" target=\"_blank\" rel=\"noopener\">https://peterxugo.github.io/2017/09/28/推荐系统/）</a><center></p>\n<p>常见的推荐系统数据由用户和商品的交互信息组成。这些交互信息可以用图来表示（图一）。由于用户节点可能包含用户的年龄、职业等个人信息，而商品节点则包含内容、价格等商品特有的信息，所以该图是一个异构图。</p>\n<p>推荐系统可能感兴趣的内容包括：</p>\n<ul>\n<li>判断某个客户是否会观看某部电影，以及在什么样的促销下会购票。这个问题可以转化成<strong>链接预测</strong>（Link Prediction）问题：判断两个点之间是否会有连边。</li>\n<li>判断某个客户是否存在恶意刷分，或者某部电影是否遭到恶意刷分。根据标注是否存在，这个问题可以转化成<strong>半监督节点分类</strong>（Semi-supervised Node Classification）或者无监督的<strong>异常检测</strong>（Anomaly Detection）问题。</li>\n</ul>\n<p><strong>学术网络</strong></p>\n<p>学术网络的图结构更复杂一些。它可以包含“论文”、“作者”、“会议”和“机构”等节点类型。每个作者会隶属于某些机构，发表某些论文，而每篇论文又会被发表在某个会议上，也会引用其它的论文。每个节点同样可以有自己的特征——如论文可以用摘要当作特征等。同样，这些关系可以表示成一张异构图（图二）。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021950189.png\" alt=\"img\"></p>\n<p><center>图二：学术网络<center></p>\n<p><center>（图源：Dong et al., KDD 2017）<center></p>\n<p>同样，<strong>链接预测</strong>和<strong>节点分类</strong>问题也适用于学术网络中。例如预测某个作者的研究领域或所属的研究团队、一篇论文或者一个作者的影响力等等。</p>\n<p><strong>知识图谱</strong></p>\n<p>知识图谱由于包含不同种类的节点和边，是一张天然的异构图。如下图，“人物”、“职业”和“荣誉”等都可以作为节点类型，而节点之间的关系也有“就职”、“获得荣誉”等多种类别。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571021975987.png\" alt=\"img\"></p>\n<p><center>图三：Wikidata知识图谱示例<center></p>\n<p><strong>图嵌入学习</strong>在知识图谱上也是一个很重要的任务。它的目标是对每一个节点和每一类关系学习一个高维向量表示。此外，知识图谱补全、基于知识图谱的推理以及将知识图谱与其他系统结合（比如推荐系统）等也是重要的任务。</p>\n<h3 id=\"有哪些针对异构图的图神经网络？\"><a href=\"#有哪些针对异构图的图神经网络？\" class=\"headerlink\" title=\"有哪些针对异构图的图神经网络？\"></a><strong>有哪些针对异构图的图神经网络？</strong></h3><p>针对异构图设计图神经网络还是个非常新的方向。虽然论文不多，但已经在一些场景中取得了很不错的效果。这里简单介绍两个模型。</p>\n<p><strong>Relational Graph Convolutional Network (RGCN)</strong></p>\n<p>文章链接：<a href=\"https://arxiv.org/abs/1703.06103\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1703.06103</a></p>\n<p>RGCN是一个在异构图上学习节点表示的早期工作。与Graph Convolutional Network（GCN）类似，它仍然基于<strong>消息传递</strong>（Message Passing）。但与GCN不同，每条边上的消息通过每类边独有的线性变换得到。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022001779.png\" alt=\"img\"></p>\n<p>RGCN可以用于节点分类和链接预测等任务。</p>\n<p><strong>Graph Convolutional Matrix Completion (GCMC)</strong></p>\n<p>文章链接：<a href=\"https://arxiv.org/abs/1706.02263\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1706.02263</a></p>\n<p>GCMC是在异构图上做产品推荐任务的一个尝试。它在一个由观众和电影组成的二分图上训练。观众和电影有各自的特征，而且每一条边上有对应的观众对电影的评分。GCMC将每一种评分视为一类边。</p>\n<p>图构建完成后，每个节点通过消息传递的形式，汇总从自己邻居传递来的消息，去更新自己的表示。与RGCN不同的是，GCMC只进行一层消息传递：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022013579.png\" alt=\"img\"></p>\n<p>消息同样是由源节点的原表示通过线性变换得到。与RGCN类似，每类边都有自己线性变换的参数矩阵。不同之处在于<strong>多关系累和函数</strong>accum有各种灵活选择。<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022024012.png\" alt=\"img\"></p>\n<p>GCMC算出两类点的最终表示之后，再通过softmax得出某个交互下每种评分的概率：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/1571022036753.png\" alt=\"img\"></p>\n<p>除了以上模型外，今年WWW还有诸如Heterogeneous Graph Attention Network (HAN)，Knowledge Graph Convolution Networks for Recommender Systems (KGCN) 等关于异构图的好工作。此外，在网络嵌入（network embedding）方向还有如metapath2vec等经典的工作。</p>\n<ul>\n<li>Heterogeneous Graph Attention Network (WWW’19)</li>\n<li>Knowledge Graph Convolution Networks for Recommender Systems (WWW’19)</li>\n<li>HetGNN: Heterogeneous Graph Neural Network (KDD’19)</li>\n<li>Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation (KDD’19)</li>\n<li>KGAT: Knowledge Graph Attention Network for Recommendation (KDD’19)</li>\n<li>metapath2vec: Scalable Representation Learning for Heterogeneous Networks (KDD’17)</li>\n</ul>\n"},{"title":"价值网与云服务平台技术","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-06-20T01:54:41.000Z","password":null,"summary":null,"img":null,"keywords":"课程 价值网与云服务平台技术","_content":"\n1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\n\n2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\n\n3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）。\n\n本课程重点考核学生的分析能力和实践能力，具体包括：\n\n1．发展趋势、商业模式、服务需求的综合分析（30分）。\n\n2．平台架构、技术特点（40分）。\n\n3．基于工业互联网，提出云平台发展建设（30分）。\n\n### **1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。**\n\n* 技术创新\n\n* 模式创新\n* 平台创新\n* 服务创新\n\n（1）从发展环境看，工业互联网平台发展动力将由政策驱动转向企业自发需求\n\n2019年，工业互联网平台、网络、安全等配套政策及行业政策体系 261 趋于完善，发展工业互联网已成为各龙头企业重塑产业竞争优势、推动 转型升级的共识。以海尔、阿里为例，海尔基于COSMOPlat平台打造了包 括工业组网解决方案、大数据解决方案、边缘层解决方案、智能制造解 决方案、工业安全解决方案等在内的170多个专业解决方案，赋能农业、 房车、机械、建陶等行业生态；阿里通过打造“1+N”工业互联网平台体 系，依托阿里的品牌价值和技术服务优势，聚合服务100余家中小信息化 服务商、大数据创新企业和信息工程服务企业，实现云端工业APP一站式 开发、托管、集成、运维和交易。\n\n![image-20200623160504217](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160504217.png)\n\n展望2020年，企业将“自下而上”推动工业互联网平台建设及推 广，针对不同的服务对象构建区域、行业、企业子平台，聚焦协议转 换、边缘计算、工业机理模型、生产线数字孪生等平台关键技术，形成 更多具有价值的行业解决方案，推动工业互联网平台在地方加速落地。\n\n（2）从技术创新看，新一代信息技术将加速与云平台技术融合\n\n2019年，大数据、人工智能、5G、区块链等新一代信息技术日趋成 熟，持续为领先制造企业和信息技术企业发展拓展新空间，涌现出更多 “平台+新技术”创新解决方案。如，富士康、商飞公司、紫光云引擎等 通过“平台+5G”融合应用实现高可靠、低时延、高通量的数据集成，催 生远程运动控制、全场景运营优化、智能巡检等模式；中国电信、杭州 汽轮等开展“平台+4K/8K高清视频”融合探索，实现高精度、异构化图 像与视频数据分析，催生智能产品检测、设备远程运维等模式；商飞公 司、华为、中兴通信、海尔等通过“平台+VR/AR”融合应用实现三维图 像快速生成与分析，催生远程辅助故障诊断、工业设计、多工种协作等 模式。\n\n![image-20200623160535443](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160535443.png)\n\n展望2020年，新一代信息技术与工业互联网平台的融合发展将从简 单到复杂、由单点聚焦到全面开花，衍生出更多新模式新业态，实现应 用创新，加速融合创新应用的落地，推动新一代信息技术与制造业的深 度融合\n\n（3）从平台应用看，面向特定场景的系统解决方案将加速涌现\n\n2019年，工业互联网平台产业链图谱更加完善，平台产业创新持续 活跃，在各行业中应用的深度和广度不断提升。在电子行业，中国电子 推出“中电云网”平台，打造SMT行业协同云、数字零售云，开发出新型 工业电商综合解决方案。在机械行业，中联重科聚焦工程机械、农用机 中国工业和信息化 发展形势展望系列 264 械等产品上云上平台，打造基于云谷工业互联网平台的设备全生命周期 智能服务模式。在工控行业，和利时打造HiaCloud工业互联网平台，在 底层设备协议解析与数据转换基础上，推出了面向化工、纺织、能源、 家电等行业解决方案。在汽车行业，北汽新能源打造了“北汽云”京津 冀地区产业协同工业互联网平台，形成汽车个性化定制、质量大数据分 析、车联网等解决方案。\n\n![image-20200623160602905](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160602905.png)\n\n展望2020年，产业整体发展将更加务实，更多产业资源加速进入工 业互联网领域，平台企业将聚焦行业痛点问题，将技术突破、模式创新 与产业实际需求相结合，涌现形成更多面向特定场景、具有更大价值的 行业解决方案。\n\n（4）从生态建设看，面向工业互联网平台的科产金服务体系将进一 步升级\n\n2019年，产业、科技、金融等各方积极探索和践行产融合作。在产 融对接交流方面，工业互联网产融推进论坛、工业互联网产融结合座谈 会分别于2月和8月召开，来自政产学研用资各方参会人员围绕工业互联 中国工业和信息化 发展形势展望系列 268 网发展面临的经济大势、投资机会及产融合作模式进行了深入交流，相 关平台企业与投资机构进行了对接。在产业界，寄云科技完成新一轮融 资，树根互联完成B轮5亿元融资，创联科技完成千万元A轮融资，工业互 联网领域的价值投资活跃越发活跃。同时，各平台企业积极探索产融结 合新模式，推动平台商业模式持续创新，形成功能订阅、金融服务等新 模式，如用友科技以功能订阅方式为近50万家中小企业提供平台化、通 用性服务并实现盈利，常州天正创新“生产力征信模型”，提供基于平 台的融资租赁、风险预警、客户标定等服务，帮助2000余家企业客户获 得授信30亿，实际放款超过22亿\n\n![image-20200623160951565](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160951565.png)\n\n展望2020年，面向工业互联网平台的科产金生态环境将持续向好。 随着各项产融结合政策的落地实施，更多产融对接平台搭建形成，产 业发展、科技创新、金融服务生态链更加完善，资本市场对以工业互联 网平台为代表的先进制造业企业将形成更多长期价值投资，形成产融结 合、良性互促的发展格局\n\n技术创新。工业互联网主要是数据驱动的工业智能，通过特定应用场景的工业APP体系，重点支撑网络化协同、智能化生产、个性化定制和服务化延伸。5G与工业互联网的融合，可以全面推动5G与垂直行业的研发设计、生产制造、管理服务等生产流程的深刻变革。典型的八大类5G+工业互联网融合应用，包括5G+超高清视频、5G+AR、5G+VR、5G+无人机、5G+云端机器人、5G+远程控制、5G+机器视觉以及5G+云化AGV（自动引导运输车）。\n\n模式创新。互联网平台主要面向消费者提供通用化服务，以用规模优势获取商业收益。工互联网平台侧重传统工业方式和企业用户（to B），更加强调面向特定场景的个性化服务。因此，不同于消费互联网以电子商务、广告竞价应用分成等为主流模式，工业互联网平台现阶段将以专业服务、功能订阅为最主要商业模式。\n\n* 专业服务：专业服务是当前平台企业的最主要盈利手段，基于平台的系统集成是最主要服务方式。绝大部分与设备管理、能耗优化、质量提升相关的大数据分析平台都以这种方式提供服务。\n* 功能订阅：功能订阅是现阶段平台盈利的重要补充，有可能成为未来平台商业模式的核心。IT资源及工业软件服务已普遍采用订阅服务方式。\n* 交易模式：交易模式中，工业产品交易相对成熟，制造能力交易与工业知识交易仍在探索。在工业产品交易方面，部分工业互联网平台依托其对产业链资源的集聚，提供工业产品交易服务。\n* 金融服务：金融服务模式显现巨大的价值潜力，是平台企业探索商业模式的新热点。推动产融结合是增强金融服务实体功能重要措施。工业企业及金融机构均可基于平台开展产融结合。目前从三条路径实现产融结合：一是数据+保险模式，二是数据+信贷模式，三是数据+租赁模式。\n* 应用商店：基于应用商店的分成模式刚刚起步。部分领先的工业互联网平台已经开始探索构建应用开发者商店，随着市场的成熟，这也可能成为平台一种新的盈利方式。\n* 平台销售：直接将平台作为一种软件产品进行销售，也是部分企业的盈利手段之一\n\n### **2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。**\n\n  云服务平台涉及了很多产品与技术，表面上看起来的确有点纷繁复杂，但是云服务平台还是有迹可循和有理可依的，其架构如图 2-1所示 。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161313484)\n\n上面这个云架构共分为服务和管理这两大部分。\n\n在服务方面，主要以提供用户基于云的各种服务为主，共包含三个层次：其一是**Software as a Service软件即服务**，简称SaaS，这层的作用是将应用主要以基于Web的方式提供给客户；其二是**Platform as a Service平台即服务**，简称PaaS，这层的作用是将一个应用的开发和部署平台作为服务提供给用户；其三是**Infrastructure as a Service基础架构即服务**，简称IaaS，这层的作用是将各种底层的计算（比如虚拟机）和存储等资源作为服务提供给用户。从用户角度而言，这三层服务，它们之间关系是独立的，因为它们提供的服务是完全不同的，而且面对的用户也不尽相同。但从技术角度而言，云服务这三层之间的关系并不是独立的，而是有一定依赖关系的，比如一个SaaS层的产品和服务不仅需要使用到SaaS层本身的技术，而且还依赖PaaS层所提供的开发和部署平台或者直接部署于IaaS层所提供的计算资源上，还有，PaaS层的产品和服务也很有可能构建于IaaS层服务之上。\n\n#### 软件即服务SaaS\n\n软件即服务（SaaS）为商用软件提供基于网络的访问。例如Netflix、Gmail、Google Docs、Office Web Apps、SaaS为企业提供一种降低软件使用成本的方法 — 按需使用软件而不是为每台计算机购买许可证。SaaS 给软件厂商提供了新的机会。尤其是，SaaS软件厂商可以通过四个因素提高 ROI（投资回报）：提高部署的速度、增加用户接受率、减少支持的需要、降低实现和升级的成本。\n\n由于SaaS层离普通用户非常接近，所以在SaaS层所使用到的技术，大多耳熟能详，下面是其中最主要的五种：\n\n* HTML\n* JavaScript\n* CSS\n* Flash\n* Silverlight\n\n在SaaS层的技术选型上，首先，由于通用性和较低的学习成本，大多数云计算产品都会比较倾向HTML 、JavaScript和CSS这对黄金组合，但是在HTML5被大家广泛接受之前，RIA技术在用户体验方面，还是具有一定的优势，所以Flash和Silverlight也将会有一定的用武之地，比如VMware vCloud就采用了基于Flash的Flex技术，而微软的云计算产品肯定会在今后大量使用Silverlight技术。\n\n#### 平台即服务PaaS\n\n平台即服务（Platform as a Service，PaaS）提供对操作系统和相关服务的访问。它让用户能够使用提供商支持的编程语言和工具把应用程序部署到云中。用户不必管理或控制底层基础架构，而是控制部署的应用程序并在一定程度上控制应用程序驻留环境的配置。PaaS的提供者包括Google App Engine、Windows Azure、Force.com、Heroku等。通过PaaS这种模式，用户可以在一个提供SDK（Software Development Kit，即软件开发工具包）、文档、测试环境和部署环境等在内的开发平台上非常方便地编写和部署应用，而且不论是在部署，还是在运行的时候，用户都无需为服务器、 操作系统、网络和存储等资源的运维而操心，这些繁琐的工作都由PaaS云供应商负责。\n\nPaaS 层的技术比较多样性，下面是常见的五种：\n\n* REST ：通过 REST（Representational State Transfer，表述性状态转移）技术，能够非常方便和优雅地将中间件层所支撑的部分服务提供给调用者\n* 多租户：就是能让一个单独的应用实例可以为多个组织服务，而且能保持良好的隔离性和安全性，并且通过这种技术，能有效地降低应用的购置和维护成本\n* 并行处理：为了处理海量的数据，需要利用庞大的x86集群进行规模巨大的并行处理，Google的MapReduce是这方面的代表之作\n* 应用服务器：在原有的应用服务器的基础上为云计算做了一定程度的优化，比如用于Google App Engine的Jetty应用服务器\n* 分布式缓存：通过分布式缓存技术，不仅能有效地降低对后台服务器的压力，而且还能加快相应的反应速度，最著名的分布式缓存例子莫过于Memcached\n\n对于很多PaaS平台，比如用于部署Ruby应用的Heroku云平台，应用服务器和分布式缓存都是必备的，同时REST技术也常用于对外的接口，多租户技术则主要用于SaaS应用的后台，比如用于支撑Salesforce 的CRM等应用的Force.com多租户内核，而并行处理技术常被作为单独的服务推出，比如Amazon的Elastic MapReduce \n\n#### 基础架构即服务IaaS\n\n 基础架构，或称基础设施（Infrastructure）是云的基础。它由服务器、网络设备、存储磁盘等物理资产组成。在使用IaaS时，用户并不实际控制底层基础架构，而是控制操作系统、存储和部署应用程序，还在有限的程度上控制网络组件的选择。  在IaaS所采用的技术方面，都是一些比较底层的技术，其中有四种技术是比较常用的：\n\n* 虚拟化：也可以理解它为基础设施层的“多租户”，因为通过虚拟化技术，能够在一个物理服务器上生成多个虚拟机，并且能在这些虚拟机之间能实现全面的隔离， 这样不仅能减低服务器的购置成本，而且还能同时降低服务器的运维成本，成熟的x86虚拟化技术有VMware的ESX和开源的Xen 。\n* 分布式存储：为了承载海量的数据，同时也要保证这些数据的可管理性，所以需要一整套分布式的存储系统，在这方面， Google 的GFS是典范之作。\n* 关系型数据库：基本是在原有的关系型数据库的基础上做了扩展和管理等方面的优化，使其在云中更适应。\n* NoSQL：为了满足一些关系数据库所无法满足的目标，比如支撑海量的数据等，一些公司特地设计一批不是基于关系模型的数据库，比如Google的BigTable和Facebook的Cassandra等\n\n现在大多数的IaaS服务都是基于Xen的，比如Amazon的EC2等，但VMware也推出了基于ESX技术的vCloud ，同时业界也有几个基于关系型数据库的云服务，比如Amazon 的RDS（Relational Database Service，关系型数据库服务）和Windows Azure SDS（SQL Data Services， SQL数据库服务）等。关于分布式存储和NoSQL，它们已经被广泛用于云平台的后端，比如Google App Engine的Datastore就是基于BigTable和GFS这两个技术之上的，而Amazon则推出基于NoSQL技术的Simple DB 。\n\n#### 架构示例：Salesforce CRM\n\n 首先，从用户角度而言，Salesforce CRM属于SaaS层服务，主要通过在云中部署可定制化的CRM应用，来让企业用户在初始投入很低的情况下使用CRM，并且可根据自身的流程来灵活地定制，而且用户只需接入互联网就能使用。从技术角度而言，Salesforce CRM像很多SaaS产品一样，不仅用到SaaS层的技术，而且还用到PaaS层、IaaS层和云管理层的技术。图2-3为Salesforce CRM在技术层面上大致的架构。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161728828)\n\n  Salesforce CRM采用的主要技术包括以下几种。\n\n* SaaS层，基于HTML、JavaScript和CSS这个黄金组合。\n* PaaS层，在此层，Salesforce引入了多租户内核和为支撑此内核运行而定制的应用服务器\n* IaaS层，虽然在后端还是使用在企业环境中很常见的Oracle数据库，但是它为了支撑上层的多租户内核作了很多优化\n\n### **3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）**\n\n需要关注的几个问题 \n\n（一）工业互联网平台商业化路径尚不明确 当前，我国工业互联网平台仍然处于发展初期，平台技术研发投入 成本较高，平台在垂直行业的商业应用和推广仍处于探索阶段，优势互 补、协同发展的平台产业生态亟待完善。同时，产业统计口径中也缺乏 有效反映数字经济时代工业互联网价值的创新统计方法，无法精确统计 平台投入产出效益，阻碍了创新解决方案培育、推广与普及。 \n\n（二）解决方案在中小企业规模化推广困难 与传统消费互联网相比，工业互联网平台解决方案在落地过程中涉 及大量数字化改造、二次开发和系统集成等工作，推广难度和成本超过 企业预期。其中以工业企业数字化试点建设为例，整个建设周期大概需 要持续1-2年甚至更长时间，而对于数量众多的中小企业仅能实现局部改 造，全面的工业互联网解决方案应用推广任务艰巨。 中国工业和信息化 发展形势展望系列 270 \n\n（三）工业信息安全保障薄弱制约平台应用 随着工业互联网的快速发展，制造环境走向开放、跨域、互联，工 业信息安全问题日益突出。由于目前工业互联网平台在数据产权确认、 交易、保护、跨境流转等等方面的标准或规范尚不健全，工业信息安全 防护手段和机制尚不完善，导致企业普遍对于工业数据上云后的数据资 产流失、数据安全风险存在顾虑，亟待相关完善安全保障体系，确保平 台健康持续发展。\n\n针对存在的问题，提出以下建议：\n\n（一）夯实制造业数字化转型基础，着力突破工业互联网平台核技术 依托工业互联网创新发展战略，着力突破工业机理模型、算法、信 息物理系统等关键技术和核心产品，超前布局数字孪生、云化仿真设计 与运营管理软件等，支持建设平台开源社区，提升平台安全可靠发展能 力。发挥政产学研用合力，加强关键核心技术攻关突破。推动工业互联 网、大数据、人工智能与制造业深度融合，深化生产制造、经营管理、 市场服务等环节的数字化应用，加快传统行业数字化转型进程。 \n\n（二）加强标准示范引领作用，培育一批平台解决方案和典型应案例 强化示范引领作用，实施更大规模、更深层次的平台应用示范，围 绕“平台+5G”、“平台+区块链”等新技术融合趋势，遴选一批平台创 271 新解决方案和应用标杆。持续开展平台绩效评价，不断完善平台发展指 数评价框架，强化平台应用的价值导向，释放平台赋能数字经济效益。 持续推进一批工业互联网平台应用创新中心建设，整合地方工业互联网 平台创新资源与行业需求，搭建面向平台解决方案供需对接、成果推广 的公共服务平台，构建平台赋能产业发展的生态体系。 \n\n（三）聚焦块状经济和带状经济区域，系统性推进工业互联网平台由 点到面落地 以区域发展总体战略为基础，着力促进工业互联网向更多垂直领域 城延伸，积极推动工业互联网创新发展战略与京津冀一体化、长三角一 体化、粤港澳大湾区、振兴东北老工业基地、西部大开发等区域战略， “一带一路”合作倡议等统筹实施，打造工业互联网平台应用先导示范 区，加快平台由点及线到面应用普及，带动区域产业协同发展。推动建 立工业互联网生态发展基金，鼓励社会资本参与工业互联网平台建设， 推动产融结合创新发展。 \n\n（四）筑牢工业信息安全保障基础，加快构建覆盖国家、地方、企业 的三级安全技术防控体系 制定工业数据安全规范，研制工业互联网大数据分级分类、工业 APP管理、工业互联网平台建设评价等关键标准，围绕平台数据收集、存 中国工业和信息化 发展形势展望系列 272 储、传输、共享等各环节，明确差异化安全机制和策略。建设工业互联 网大数据中心，建立中央、地方、行业企业多层次数据管理机制，打破 数据孤岛和基础设施捆绑。强化平台安全监测预警能力，搭建国家、地 方、企业多级协同联动的态势感知网络，实现对重要和关键平台接入设 备、控制系统、运行数据的风险实时监测，感知边缘层、IaaS层、PaaS 层和SaaS层等的安全状态，切实提升工业互联网平台安全防护水平。\n","source":"_posts/价值网与云服务平台技术.md","raw":"---\ntitle: 价值网与云服务平台技术\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-06-20 09:54:41\npassword:\nsummary:\ntags:\n\t- 课程\n\t- 价值网与云服务平台技术\ncategories: 课程\nimg:\nkeywords: 课程 价值网与云服务平台技术\n---\n\n1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\n\n2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\n\n3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）。\n\n本课程重点考核学生的分析能力和实践能力，具体包括：\n\n1．发展趋势、商业模式、服务需求的综合分析（30分）。\n\n2．平台架构、技术特点（40分）。\n\n3．基于工业互联网，提出云平台发展建设（30分）。\n\n### **1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。**\n\n* 技术创新\n\n* 模式创新\n* 平台创新\n* 服务创新\n\n（1）从发展环境看，工业互联网平台发展动力将由政策驱动转向企业自发需求\n\n2019年，工业互联网平台、网络、安全等配套政策及行业政策体系 261 趋于完善，发展工业互联网已成为各龙头企业重塑产业竞争优势、推动 转型升级的共识。以海尔、阿里为例，海尔基于COSMOPlat平台打造了包 括工业组网解决方案、大数据解决方案、边缘层解决方案、智能制造解 决方案、工业安全解决方案等在内的170多个专业解决方案，赋能农业、 房车、机械、建陶等行业生态；阿里通过打造“1+N”工业互联网平台体 系，依托阿里的品牌价值和技术服务优势，聚合服务100余家中小信息化 服务商、大数据创新企业和信息工程服务企业，实现云端工业APP一站式 开发、托管、集成、运维和交易。\n\n![image-20200623160504217](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160504217.png)\n\n展望2020年，企业将“自下而上”推动工业互联网平台建设及推 广，针对不同的服务对象构建区域、行业、企业子平台，聚焦协议转 换、边缘计算、工业机理模型、生产线数字孪生等平台关键技术，形成 更多具有价值的行业解决方案，推动工业互联网平台在地方加速落地。\n\n（2）从技术创新看，新一代信息技术将加速与云平台技术融合\n\n2019年，大数据、人工智能、5G、区块链等新一代信息技术日趋成 熟，持续为领先制造企业和信息技术企业发展拓展新空间，涌现出更多 “平台+新技术”创新解决方案。如，富士康、商飞公司、紫光云引擎等 通过“平台+5G”融合应用实现高可靠、低时延、高通量的数据集成，催 生远程运动控制、全场景运营优化、智能巡检等模式；中国电信、杭州 汽轮等开展“平台+4K/8K高清视频”融合探索，实现高精度、异构化图 像与视频数据分析，催生智能产品检测、设备远程运维等模式；商飞公 司、华为、中兴通信、海尔等通过“平台+VR/AR”融合应用实现三维图 像快速生成与分析，催生远程辅助故障诊断、工业设计、多工种协作等 模式。\n\n![image-20200623160535443](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160535443.png)\n\n展望2020年，新一代信息技术与工业互联网平台的融合发展将从简 单到复杂、由单点聚焦到全面开花，衍生出更多新模式新业态，实现应 用创新，加速融合创新应用的落地，推动新一代信息技术与制造业的深 度融合\n\n（3）从平台应用看，面向特定场景的系统解决方案将加速涌现\n\n2019年，工业互联网平台产业链图谱更加完善，平台产业创新持续 活跃，在各行业中应用的深度和广度不断提升。在电子行业，中国电子 推出“中电云网”平台，打造SMT行业协同云、数字零售云，开发出新型 工业电商综合解决方案。在机械行业，中联重科聚焦工程机械、农用机 中国工业和信息化 发展形势展望系列 264 械等产品上云上平台，打造基于云谷工业互联网平台的设备全生命周期 智能服务模式。在工控行业，和利时打造HiaCloud工业互联网平台，在 底层设备协议解析与数据转换基础上，推出了面向化工、纺织、能源、 家电等行业解决方案。在汽车行业，北汽新能源打造了“北汽云”京津 冀地区产业协同工业互联网平台，形成汽车个性化定制、质量大数据分 析、车联网等解决方案。\n\n![image-20200623160602905](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160602905.png)\n\n展望2020年，产业整体发展将更加务实，更多产业资源加速进入工 业互联网领域，平台企业将聚焦行业痛点问题，将技术突破、模式创新 与产业实际需求相结合，涌现形成更多面向特定场景、具有更大价值的 行业解决方案。\n\n（4）从生态建设看，面向工业互联网平台的科产金服务体系将进一 步升级\n\n2019年，产业、科技、金融等各方积极探索和践行产融合作。在产 融对接交流方面，工业互联网产融推进论坛、工业互联网产融结合座谈 会分别于2月和8月召开，来自政产学研用资各方参会人员围绕工业互联 中国工业和信息化 发展形势展望系列 268 网发展面临的经济大势、投资机会及产融合作模式进行了深入交流，相 关平台企业与投资机构进行了对接。在产业界，寄云科技完成新一轮融 资，树根互联完成B轮5亿元融资，创联科技完成千万元A轮融资，工业互 联网领域的价值投资活跃越发活跃。同时，各平台企业积极探索产融结 合新模式，推动平台商业模式持续创新，形成功能订阅、金融服务等新 模式，如用友科技以功能订阅方式为近50万家中小企业提供平台化、通 用性服务并实现盈利，常州天正创新“生产力征信模型”，提供基于平 台的融资租赁、风险预警、客户标定等服务，帮助2000余家企业客户获 得授信30亿，实际放款超过22亿\n\n![image-20200623160951565](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160951565.png)\n\n展望2020年，面向工业互联网平台的科产金生态环境将持续向好。 随着各项产融结合政策的落地实施，更多产融对接平台搭建形成，产 业发展、科技创新、金融服务生态链更加完善，资本市场对以工业互联 网平台为代表的先进制造业企业将形成更多长期价值投资，形成产融结 合、良性互促的发展格局\n\n技术创新。工业互联网主要是数据驱动的工业智能，通过特定应用场景的工业APP体系，重点支撑网络化协同、智能化生产、个性化定制和服务化延伸。5G与工业互联网的融合，可以全面推动5G与垂直行业的研发设计、生产制造、管理服务等生产流程的深刻变革。典型的八大类5G+工业互联网融合应用，包括5G+超高清视频、5G+AR、5G+VR、5G+无人机、5G+云端机器人、5G+远程控制、5G+机器视觉以及5G+云化AGV（自动引导运输车）。\n\n模式创新。互联网平台主要面向消费者提供通用化服务，以用规模优势获取商业收益。工互联网平台侧重传统工业方式和企业用户（to B），更加强调面向特定场景的个性化服务。因此，不同于消费互联网以电子商务、广告竞价应用分成等为主流模式，工业互联网平台现阶段将以专业服务、功能订阅为最主要商业模式。\n\n* 专业服务：专业服务是当前平台企业的最主要盈利手段，基于平台的系统集成是最主要服务方式。绝大部分与设备管理、能耗优化、质量提升相关的大数据分析平台都以这种方式提供服务。\n* 功能订阅：功能订阅是现阶段平台盈利的重要补充，有可能成为未来平台商业模式的核心。IT资源及工业软件服务已普遍采用订阅服务方式。\n* 交易模式：交易模式中，工业产品交易相对成熟，制造能力交易与工业知识交易仍在探索。在工业产品交易方面，部分工业互联网平台依托其对产业链资源的集聚，提供工业产品交易服务。\n* 金融服务：金融服务模式显现巨大的价值潜力，是平台企业探索商业模式的新热点。推动产融结合是增强金融服务实体功能重要措施。工业企业及金融机构均可基于平台开展产融结合。目前从三条路径实现产融结合：一是数据+保险模式，二是数据+信贷模式，三是数据+租赁模式。\n* 应用商店：基于应用商店的分成模式刚刚起步。部分领先的工业互联网平台已经开始探索构建应用开发者商店，随着市场的成熟，这也可能成为平台一种新的盈利方式。\n* 平台销售：直接将平台作为一种软件产品进行销售，也是部分企业的盈利手段之一\n\n### **2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。**\n\n  云服务平台涉及了很多产品与技术，表面上看起来的确有点纷繁复杂，但是云服务平台还是有迹可循和有理可依的，其架构如图 2-1所示 。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161313484)\n\n上面这个云架构共分为服务和管理这两大部分。\n\n在服务方面，主要以提供用户基于云的各种服务为主，共包含三个层次：其一是**Software as a Service软件即服务**，简称SaaS，这层的作用是将应用主要以基于Web的方式提供给客户；其二是**Platform as a Service平台即服务**，简称PaaS，这层的作用是将一个应用的开发和部署平台作为服务提供给用户；其三是**Infrastructure as a Service基础架构即服务**，简称IaaS，这层的作用是将各种底层的计算（比如虚拟机）和存储等资源作为服务提供给用户。从用户角度而言，这三层服务，它们之间关系是独立的，因为它们提供的服务是完全不同的，而且面对的用户也不尽相同。但从技术角度而言，云服务这三层之间的关系并不是独立的，而是有一定依赖关系的，比如一个SaaS层的产品和服务不仅需要使用到SaaS层本身的技术，而且还依赖PaaS层所提供的开发和部署平台或者直接部署于IaaS层所提供的计算资源上，还有，PaaS层的产品和服务也很有可能构建于IaaS层服务之上。\n\n#### 软件即服务SaaS\n\n软件即服务（SaaS）为商用软件提供基于网络的访问。例如Netflix、Gmail、Google Docs、Office Web Apps、SaaS为企业提供一种降低软件使用成本的方法 — 按需使用软件而不是为每台计算机购买许可证。SaaS 给软件厂商提供了新的机会。尤其是，SaaS软件厂商可以通过四个因素提高 ROI（投资回报）：提高部署的速度、增加用户接受率、减少支持的需要、降低实现和升级的成本。\n\n由于SaaS层离普通用户非常接近，所以在SaaS层所使用到的技术，大多耳熟能详，下面是其中最主要的五种：\n\n* HTML\n* JavaScript\n* CSS\n* Flash\n* Silverlight\n\n在SaaS层的技术选型上，首先，由于通用性和较低的学习成本，大多数云计算产品都会比较倾向HTML 、JavaScript和CSS这对黄金组合，但是在HTML5被大家广泛接受之前，RIA技术在用户体验方面，还是具有一定的优势，所以Flash和Silverlight也将会有一定的用武之地，比如VMware vCloud就采用了基于Flash的Flex技术，而微软的云计算产品肯定会在今后大量使用Silverlight技术。\n\n#### 平台即服务PaaS\n\n平台即服务（Platform as a Service，PaaS）提供对操作系统和相关服务的访问。它让用户能够使用提供商支持的编程语言和工具把应用程序部署到云中。用户不必管理或控制底层基础架构，而是控制部署的应用程序并在一定程度上控制应用程序驻留环境的配置。PaaS的提供者包括Google App Engine、Windows Azure、Force.com、Heroku等。通过PaaS这种模式，用户可以在一个提供SDK（Software Development Kit，即软件开发工具包）、文档、测试环境和部署环境等在内的开发平台上非常方便地编写和部署应用，而且不论是在部署，还是在运行的时候，用户都无需为服务器、 操作系统、网络和存储等资源的运维而操心，这些繁琐的工作都由PaaS云供应商负责。\n\nPaaS 层的技术比较多样性，下面是常见的五种：\n\n* REST ：通过 REST（Representational State Transfer，表述性状态转移）技术，能够非常方便和优雅地将中间件层所支撑的部分服务提供给调用者\n* 多租户：就是能让一个单独的应用实例可以为多个组织服务，而且能保持良好的隔离性和安全性，并且通过这种技术，能有效地降低应用的购置和维护成本\n* 并行处理：为了处理海量的数据，需要利用庞大的x86集群进行规模巨大的并行处理，Google的MapReduce是这方面的代表之作\n* 应用服务器：在原有的应用服务器的基础上为云计算做了一定程度的优化，比如用于Google App Engine的Jetty应用服务器\n* 分布式缓存：通过分布式缓存技术，不仅能有效地降低对后台服务器的压力，而且还能加快相应的反应速度，最著名的分布式缓存例子莫过于Memcached\n\n对于很多PaaS平台，比如用于部署Ruby应用的Heroku云平台，应用服务器和分布式缓存都是必备的，同时REST技术也常用于对外的接口，多租户技术则主要用于SaaS应用的后台，比如用于支撑Salesforce 的CRM等应用的Force.com多租户内核，而并行处理技术常被作为单独的服务推出，比如Amazon的Elastic MapReduce \n\n#### 基础架构即服务IaaS\n\n 基础架构，或称基础设施（Infrastructure）是云的基础。它由服务器、网络设备、存储磁盘等物理资产组成。在使用IaaS时，用户并不实际控制底层基础架构，而是控制操作系统、存储和部署应用程序，还在有限的程度上控制网络组件的选择。  在IaaS所采用的技术方面，都是一些比较底层的技术，其中有四种技术是比较常用的：\n\n* 虚拟化：也可以理解它为基础设施层的“多租户”，因为通过虚拟化技术，能够在一个物理服务器上生成多个虚拟机，并且能在这些虚拟机之间能实现全面的隔离， 这样不仅能减低服务器的购置成本，而且还能同时降低服务器的运维成本，成熟的x86虚拟化技术有VMware的ESX和开源的Xen 。\n* 分布式存储：为了承载海量的数据，同时也要保证这些数据的可管理性，所以需要一整套分布式的存储系统，在这方面， Google 的GFS是典范之作。\n* 关系型数据库：基本是在原有的关系型数据库的基础上做了扩展和管理等方面的优化，使其在云中更适应。\n* NoSQL：为了满足一些关系数据库所无法满足的目标，比如支撑海量的数据等，一些公司特地设计一批不是基于关系模型的数据库，比如Google的BigTable和Facebook的Cassandra等\n\n现在大多数的IaaS服务都是基于Xen的，比如Amazon的EC2等，但VMware也推出了基于ESX技术的vCloud ，同时业界也有几个基于关系型数据库的云服务，比如Amazon 的RDS（Relational Database Service，关系型数据库服务）和Windows Azure SDS（SQL Data Services， SQL数据库服务）等。关于分布式存储和NoSQL，它们已经被广泛用于云平台的后端，比如Google App Engine的Datastore就是基于BigTable和GFS这两个技术之上的，而Amazon则推出基于NoSQL技术的Simple DB 。\n\n#### 架构示例：Salesforce CRM\n\n 首先，从用户角度而言，Salesforce CRM属于SaaS层服务，主要通过在云中部署可定制化的CRM应用，来让企业用户在初始投入很低的情况下使用CRM，并且可根据自身的流程来灵活地定制，而且用户只需接入互联网就能使用。从技术角度而言，Salesforce CRM像很多SaaS产品一样，不仅用到SaaS层的技术，而且还用到PaaS层、IaaS层和云管理层的技术。图2-3为Salesforce CRM在技术层面上大致的架构。\n\n![img](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161728828)\n\n  Salesforce CRM采用的主要技术包括以下几种。\n\n* SaaS层，基于HTML、JavaScript和CSS这个黄金组合。\n* PaaS层，在此层，Salesforce引入了多租户内核和为支撑此内核运行而定制的应用服务器\n* IaaS层，虽然在后端还是使用在企业环境中很常见的Oracle数据库，但是它为了支撑上层的多租户内核作了很多优化\n\n### **3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）**\n\n需要关注的几个问题 \n\n（一）工业互联网平台商业化路径尚不明确 当前，我国工业互联网平台仍然处于发展初期，平台技术研发投入 成本较高，平台在垂直行业的商业应用和推广仍处于探索阶段，优势互 补、协同发展的平台产业生态亟待完善。同时，产业统计口径中也缺乏 有效反映数字经济时代工业互联网价值的创新统计方法，无法精确统计 平台投入产出效益，阻碍了创新解决方案培育、推广与普及。 \n\n（二）解决方案在中小企业规模化推广困难 与传统消费互联网相比，工业互联网平台解决方案在落地过程中涉 及大量数字化改造、二次开发和系统集成等工作，推广难度和成本超过 企业预期。其中以工业企业数字化试点建设为例，整个建设周期大概需 要持续1-2年甚至更长时间，而对于数量众多的中小企业仅能实现局部改 造，全面的工业互联网解决方案应用推广任务艰巨。 中国工业和信息化 发展形势展望系列 270 \n\n（三）工业信息安全保障薄弱制约平台应用 随着工业互联网的快速发展，制造环境走向开放、跨域、互联，工 业信息安全问题日益突出。由于目前工业互联网平台在数据产权确认、 交易、保护、跨境流转等等方面的标准或规范尚不健全，工业信息安全 防护手段和机制尚不完善，导致企业普遍对于工业数据上云后的数据资 产流失、数据安全风险存在顾虑，亟待相关完善安全保障体系，确保平 台健康持续发展。\n\n针对存在的问题，提出以下建议：\n\n（一）夯实制造业数字化转型基础，着力突破工业互联网平台核技术 依托工业互联网创新发展战略，着力突破工业机理模型、算法、信 息物理系统等关键技术和核心产品，超前布局数字孪生、云化仿真设计 与运营管理软件等，支持建设平台开源社区，提升平台安全可靠发展能 力。发挥政产学研用合力，加强关键核心技术攻关突破。推动工业互联 网、大数据、人工智能与制造业深度融合，深化生产制造、经营管理、 市场服务等环节的数字化应用，加快传统行业数字化转型进程。 \n\n（二）加强标准示范引领作用，培育一批平台解决方案和典型应案例 强化示范引领作用，实施更大规模、更深层次的平台应用示范，围 绕“平台+5G”、“平台+区块链”等新技术融合趋势，遴选一批平台创 271 新解决方案和应用标杆。持续开展平台绩效评价，不断完善平台发展指 数评价框架，强化平台应用的价值导向，释放平台赋能数字经济效益。 持续推进一批工业互联网平台应用创新中心建设，整合地方工业互联网 平台创新资源与行业需求，搭建面向平台解决方案供需对接、成果推广 的公共服务平台，构建平台赋能产业发展的生态体系。 \n\n（三）聚焦块状经济和带状经济区域，系统性推进工业互联网平台由 点到面落地 以区域发展总体战略为基础，着力促进工业互联网向更多垂直领域 城延伸，积极推动工业互联网创新发展战略与京津冀一体化、长三角一 体化、粤港澳大湾区、振兴东北老工业基地、西部大开发等区域战略， “一带一路”合作倡议等统筹实施，打造工业互联网平台应用先导示范 区，加快平台由点及线到面应用普及，带动区域产业协同发展。推动建 立工业互联网生态发展基金，鼓励社会资本参与工业互联网平台建设， 推动产融结合创新发展。 \n\n（四）筑牢工业信息安全保障基础，加快构建覆盖国家、地方、企业 的三级安全技术防控体系 制定工业数据安全规范，研制工业互联网大数据分级分类、工业 APP管理、工业互联网平台建设评价等关键标准，围绕平台数据收集、存 中国工业和信息化 发展形势展望系列 272 储、传输、共享等各环节，明确差异化安全机制和策略。建设工业互联 网大数据中心，建立中央、地方、行业企业多层次数据管理机制，打破 数据孤岛和基础设施捆绑。强化平台安全监测预警能力，搭建国家、地 方、企业多级协同联动的态势感知网络，实现对重要和关键平台接入设 备、控制系统、运行数据的风险实时监测，感知边缘层、IaaS层、PaaS 层和SaaS层等的安全状态，切实提升工业互联网平台安全防护水平。\n","slug":"价值网与云服务平台技术","published":1,"updated":"2020-08-20T08:41:44.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0max0012ewse39pub8qw","content":"<p>1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。</p>\n<p>2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。</p>\n<p>3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）。</p>\n<p>本课程重点考核学生的分析能力和实践能力，具体包括：</p>\n<p>1．发展趋势、商业模式、服务需求的综合分析（30分）。</p>\n<p>2．平台架构、技术特点（40分）。</p>\n<p>3．基于工业互联网，提出云平台发展建设（30分）。</p>\n<h3 id=\"1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\"><a href=\"#1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\" class=\"headerlink\" title=\"1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\"></a><strong>1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。</strong></h3><ul>\n<li><p>技术创新</p>\n</li>\n<li><p>模式创新</p>\n</li>\n<li>平台创新</li>\n<li>服务创新</li>\n</ul>\n<p>（1）从发展环境看，工业互联网平台发展动力将由政策驱动转向企业自发需求</p>\n<p>2019年，工业互联网平台、网络、安全等配套政策及行业政策体系 261 趋于完善，发展工业互联网已成为各龙头企业重塑产业竞争优势、推动 转型升级的共识。以海尔、阿里为例，海尔基于COSMOPlat平台打造了包 括工业组网解决方案、大数据解决方案、边缘层解决方案、智能制造解 决方案、工业安全解决方案等在内的170多个专业解决方案，赋能农业、 房车、机械、建陶等行业生态；阿里通过打造“1+N”工业互联网平台体 系，依托阿里的品牌价值和技术服务优势，聚合服务100余家中小信息化 服务商、大数据创新企业和信息工程服务企业，实现云端工业APP一站式 开发、托管、集成、运维和交易。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160504217.png\" alt=\"image-20200623160504217\"></p>\n<p>展望2020年，企业将“自下而上”推动工业互联网平台建设及推 广，针对不同的服务对象构建区域、行业、企业子平台，聚焦协议转 换、边缘计算、工业机理模型、生产线数字孪生等平台关键技术，形成 更多具有价值的行业解决方案，推动工业互联网平台在地方加速落地。</p>\n<p>（2）从技术创新看，新一代信息技术将加速与云平台技术融合</p>\n<p>2019年，大数据、人工智能、5G、区块链等新一代信息技术日趋成 熟，持续为领先制造企业和信息技术企业发展拓展新空间，涌现出更多 “平台+新技术”创新解决方案。如，富士康、商飞公司、紫光云引擎等 通过“平台+5G”融合应用实现高可靠、低时延、高通量的数据集成，催 生远程运动控制、全场景运营优化、智能巡检等模式；中国电信、杭州 汽轮等开展“平台+4K/8K高清视频”融合探索，实现高精度、异构化图 像与视频数据分析，催生智能产品检测、设备远程运维等模式；商飞公 司、华为、中兴通信、海尔等通过“平台+VR/AR”融合应用实现三维图 像快速生成与分析，催生远程辅助故障诊断、工业设计、多工种协作等 模式。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160535443.png\" alt=\"image-20200623160535443\"></p>\n<p>展望2020年，新一代信息技术与工业互联网平台的融合发展将从简 单到复杂、由单点聚焦到全面开花，衍生出更多新模式新业态，实现应 用创新，加速融合创新应用的落地，推动新一代信息技术与制造业的深 度融合</p>\n<p>（3）从平台应用看，面向特定场景的系统解决方案将加速涌现</p>\n<p>2019年，工业互联网平台产业链图谱更加完善，平台产业创新持续 活跃，在各行业中应用的深度和广度不断提升。在电子行业，中国电子 推出“中电云网”平台，打造SMT行业协同云、数字零售云，开发出新型 工业电商综合解决方案。在机械行业，中联重科聚焦工程机械、农用机 中国工业和信息化 发展形势展望系列 264 械等产品上云上平台，打造基于云谷工业互联网平台的设备全生命周期 智能服务模式。在工控行业，和利时打造HiaCloud工业互联网平台，在 底层设备协议解析与数据转换基础上，推出了面向化工、纺织、能源、 家电等行业解决方案。在汽车行业，北汽新能源打造了“北汽云”京津 冀地区产业协同工业互联网平台，形成汽车个性化定制、质量大数据分 析、车联网等解决方案。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160602905.png\" alt=\"image-20200623160602905\"></p>\n<p>展望2020年，产业整体发展将更加务实，更多产业资源加速进入工 业互联网领域，平台企业将聚焦行业痛点问题，将技术突破、模式创新 与产业实际需求相结合，涌现形成更多面向特定场景、具有更大价值的 行业解决方案。</p>\n<p>（4）从生态建设看，面向工业互联网平台的科产金服务体系将进一 步升级</p>\n<p>2019年，产业、科技、金融等各方积极探索和践行产融合作。在产 融对接交流方面，工业互联网产融推进论坛、工业互联网产融结合座谈 会分别于2月和8月召开，来自政产学研用资各方参会人员围绕工业互联 中国工业和信息化 发展形势展望系列 268 网发展面临的经济大势、投资机会及产融合作模式进行了深入交流，相 关平台企业与投资机构进行了对接。在产业界，寄云科技完成新一轮融 资，树根互联完成B轮5亿元融资，创联科技完成千万元A轮融资，工业互 联网领域的价值投资活跃越发活跃。同时，各平台企业积极探索产融结 合新模式，推动平台商业模式持续创新，形成功能订阅、金融服务等新 模式，如用友科技以功能订阅方式为近50万家中小企业提供平台化、通 用性服务并实现盈利，常州天正创新“生产力征信模型”，提供基于平 台的融资租赁、风险预警、客户标定等服务，帮助2000余家企业客户获 得授信30亿，实际放款超过22亿</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160951565.png\" alt=\"image-20200623160951565\"></p>\n<p>展望2020年，面向工业互联网平台的科产金生态环境将持续向好。 随着各项产融结合政策的落地实施，更多产融对接平台搭建形成，产 业发展、科技创新、金融服务生态链更加完善，资本市场对以工业互联 网平台为代表的先进制造业企业将形成更多长期价值投资，形成产融结 合、良性互促的发展格局</p>\n<p>技术创新。工业互联网主要是数据驱动的工业智能，通过特定应用场景的工业APP体系，重点支撑网络化协同、智能化生产、个性化定制和服务化延伸。5G与工业互联网的融合，可以全面推动5G与垂直行业的研发设计、生产制造、管理服务等生产流程的深刻变革。典型的八大类5G+工业互联网融合应用，包括5G+超高清视频、5G+AR、5G+VR、5G+无人机、5G+云端机器人、5G+远程控制、5G+机器视觉以及5G+云化AGV（自动引导运输车）。</p>\n<p>模式创新。互联网平台主要面向消费者提供通用化服务，以用规模优势获取商业收益。工互联网平台侧重传统工业方式和企业用户（to B），更加强调面向特定场景的个性化服务。因此，不同于消费互联网以电子商务、广告竞价应用分成等为主流模式，工业互联网平台现阶段将以专业服务、功能订阅为最主要商业模式。</p>\n<ul>\n<li>专业服务：专业服务是当前平台企业的最主要盈利手段，基于平台的系统集成是最主要服务方式。绝大部分与设备管理、能耗优化、质量提升相关的大数据分析平台都以这种方式提供服务。</li>\n<li>功能订阅：功能订阅是现阶段平台盈利的重要补充，有可能成为未来平台商业模式的核心。IT资源及工业软件服务已普遍采用订阅服务方式。</li>\n<li>交易模式：交易模式中，工业产品交易相对成熟，制造能力交易与工业知识交易仍在探索。在工业产品交易方面，部分工业互联网平台依托其对产业链资源的集聚，提供工业产品交易服务。</li>\n<li>金融服务：金融服务模式显现巨大的价值潜力，是平台企业探索商业模式的新热点。推动产融结合是增强金融服务实体功能重要措施。工业企业及金融机构均可基于平台开展产融结合。目前从三条路径实现产融结合：一是数据+保险模式，二是数据+信贷模式，三是数据+租赁模式。</li>\n<li>应用商店：基于应用商店的分成模式刚刚起步。部分领先的工业互联网平台已经开始探索构建应用开发者商店，随着市场的成熟，这也可能成为平台一种新的盈利方式。</li>\n<li>平台销售：直接将平台作为一种软件产品进行销售，也是部分企业的盈利手段之一</li>\n</ul>\n<h3 id=\"2．论述平台的技术架构。包括“互联网-”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\"><a href=\"#2．论述平台的技术架构。包括“互联网-”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\" class=\"headerlink\" title=\"2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\"></a><strong>2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。</strong></h3><p>  云服务平台涉及了很多产品与技术，表面上看起来的确有点纷繁复杂，但是云服务平台还是有迹可循和有理可依的，其架构如图 2-1所示 。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161313484\" alt=\"img\"></p>\n<p>上面这个云架构共分为服务和管理这两大部分。</p>\n<p>在服务方面，主要以提供用户基于云的各种服务为主，共包含三个层次：其一是<strong>Software as a Service软件即服务</strong>，简称SaaS，这层的作用是将应用主要以基于Web的方式提供给客户；其二是<strong>Platform as a Service平台即服务</strong>，简称PaaS，这层的作用是将一个应用的开发和部署平台作为服务提供给用户；其三是<strong>Infrastructure as a Service基础架构即服务</strong>，简称IaaS，这层的作用是将各种底层的计算（比如虚拟机）和存储等资源作为服务提供给用户。从用户角度而言，这三层服务，它们之间关系是独立的，因为它们提供的服务是完全不同的，而且面对的用户也不尽相同。但从技术角度而言，云服务这三层之间的关系并不是独立的，而是有一定依赖关系的，比如一个SaaS层的产品和服务不仅需要使用到SaaS层本身的技术，而且还依赖PaaS层所提供的开发和部署平台或者直接部署于IaaS层所提供的计算资源上，还有，PaaS层的产品和服务也很有可能构建于IaaS层服务之上。</p>\n<h4 id=\"软件即服务SaaS\"><a href=\"#软件即服务SaaS\" class=\"headerlink\" title=\"软件即服务SaaS\"></a>软件即服务SaaS</h4><p>软件即服务（SaaS）为商用软件提供基于网络的访问。例如Netflix、Gmail、Google Docs、Office Web Apps、SaaS为企业提供一种降低软件使用成本的方法 — 按需使用软件而不是为每台计算机购买许可证。SaaS 给软件厂商提供了新的机会。尤其是，SaaS软件厂商可以通过四个因素提高 ROI（投资回报）：提高部署的速度、增加用户接受率、减少支持的需要、降低实现和升级的成本。</p>\n<p>由于SaaS层离普通用户非常接近，所以在SaaS层所使用到的技术，大多耳熟能详，下面是其中最主要的五种：</p>\n<ul>\n<li>HTML</li>\n<li>JavaScript</li>\n<li>CSS</li>\n<li>Flash</li>\n<li>Silverlight</li>\n</ul>\n<p>在SaaS层的技术选型上，首先，由于通用性和较低的学习成本，大多数云计算产品都会比较倾向HTML 、JavaScript和CSS这对黄金组合，但是在HTML5被大家广泛接受之前，RIA技术在用户体验方面，还是具有一定的优势，所以Flash和Silverlight也将会有一定的用武之地，比如VMware vCloud就采用了基于Flash的Flex技术，而微软的云计算产品肯定会在今后大量使用Silverlight技术。</p>\n<h4 id=\"平台即服务PaaS\"><a href=\"#平台即服务PaaS\" class=\"headerlink\" title=\"平台即服务PaaS\"></a>平台即服务PaaS</h4><p>平台即服务（Platform as a Service，PaaS）提供对操作系统和相关服务的访问。它让用户能够使用提供商支持的编程语言和工具把应用程序部署到云中。用户不必管理或控制底层基础架构，而是控制部署的应用程序并在一定程度上控制应用程序驻留环境的配置。PaaS的提供者包括Google App Engine、Windows Azure、Force.com、Heroku等。通过PaaS这种模式，用户可以在一个提供SDK（Software Development Kit，即软件开发工具包）、文档、测试环境和部署环境等在内的开发平台上非常方便地编写和部署应用，而且不论是在部署，还是在运行的时候，用户都无需为服务器、 操作系统、网络和存储等资源的运维而操心，这些繁琐的工作都由PaaS云供应商负责。</p>\n<p>PaaS 层的技术比较多样性，下面是常见的五种：</p>\n<ul>\n<li>REST ：通过 REST（Representational State Transfer，表述性状态转移）技术，能够非常方便和优雅地将中间件层所支撑的部分服务提供给调用者</li>\n<li>多租户：就是能让一个单独的应用实例可以为多个组织服务，而且能保持良好的隔离性和安全性，并且通过这种技术，能有效地降低应用的购置和维护成本</li>\n<li>并行处理：为了处理海量的数据，需要利用庞大的x86集群进行规模巨大的并行处理，Google的MapReduce是这方面的代表之作</li>\n<li>应用服务器：在原有的应用服务器的基础上为云计算做了一定程度的优化，比如用于Google App Engine的Jetty应用服务器</li>\n<li>分布式缓存：通过分布式缓存技术，不仅能有效地降低对后台服务器的压力，而且还能加快相应的反应速度，最著名的分布式缓存例子莫过于Memcached</li>\n</ul>\n<p>对于很多PaaS平台，比如用于部署Ruby应用的Heroku云平台，应用服务器和分布式缓存都是必备的，同时REST技术也常用于对外的接口，多租户技术则主要用于SaaS应用的后台，比如用于支撑Salesforce 的CRM等应用的Force.com多租户内核，而并行处理技术常被作为单独的服务推出，比如Amazon的Elastic MapReduce </p>\n<h4 id=\"基础架构即服务IaaS\"><a href=\"#基础架构即服务IaaS\" class=\"headerlink\" title=\"基础架构即服务IaaS\"></a>基础架构即服务IaaS</h4><p> 基础架构，或称基础设施（Infrastructure）是云的基础。它由服务器、网络设备、存储磁盘等物理资产组成。在使用IaaS时，用户并不实际控制底层基础架构，而是控制操作系统、存储和部署应用程序，还在有限的程度上控制网络组件的选择。  在IaaS所采用的技术方面，都是一些比较底层的技术，其中有四种技术是比较常用的：</p>\n<ul>\n<li>虚拟化：也可以理解它为基础设施层的“多租户”，因为通过虚拟化技术，能够在一个物理服务器上生成多个虚拟机，并且能在这些虚拟机之间能实现全面的隔离， 这样不仅能减低服务器的购置成本，而且还能同时降低服务器的运维成本，成熟的x86虚拟化技术有VMware的ESX和开源的Xen 。</li>\n<li>分布式存储：为了承载海量的数据，同时也要保证这些数据的可管理性，所以需要一整套分布式的存储系统，在这方面， Google 的GFS是典范之作。</li>\n<li>关系型数据库：基本是在原有的关系型数据库的基础上做了扩展和管理等方面的优化，使其在云中更适应。</li>\n<li>NoSQL：为了满足一些关系数据库所无法满足的目标，比如支撑海量的数据等，一些公司特地设计一批不是基于关系模型的数据库，比如Google的BigTable和Facebook的Cassandra等</li>\n</ul>\n<p>现在大多数的IaaS服务都是基于Xen的，比如Amazon的EC2等，但VMware也推出了基于ESX技术的vCloud ，同时业界也有几个基于关系型数据库的云服务，比如Amazon 的RDS（Relational Database Service，关系型数据库服务）和Windows Azure SDS（SQL Data Services， SQL数据库服务）等。关于分布式存储和NoSQL，它们已经被广泛用于云平台的后端，比如Google App Engine的Datastore就是基于BigTable和GFS这两个技术之上的，而Amazon则推出基于NoSQL技术的Simple DB 。</p>\n<h4 id=\"架构示例：Salesforce-CRM\"><a href=\"#架构示例：Salesforce-CRM\" class=\"headerlink\" title=\"架构示例：Salesforce CRM\"></a>架构示例：Salesforce CRM</h4><p> 首先，从用户角度而言，Salesforce CRM属于SaaS层服务，主要通过在云中部署可定制化的CRM应用，来让企业用户在初始投入很低的情况下使用CRM，并且可根据自身的流程来灵活地定制，而且用户只需接入互联网就能使用。从技术角度而言，Salesforce CRM像很多SaaS产品一样，不仅用到SaaS层的技术，而且还用到PaaS层、IaaS层和云管理层的技术。图2-3为Salesforce CRM在技术层面上大致的架构。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161728828\" alt=\"img\"></p>\n<p>  Salesforce CRM采用的主要技术包括以下几种。</p>\n<ul>\n<li>SaaS层，基于HTML、JavaScript和CSS这个黄金组合。</li>\n<li>PaaS层，在此层，Salesforce引入了多租户内核和为支撑此内核运行而定制的应用服务器</li>\n<li>IaaS层，虽然在后端还是使用在企业环境中很常见的Oracle数据库，但是它为了支撑上层的多租户内核作了很多优化</li>\n</ul>\n<h3 id=\"3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）\"><a href=\"#3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）\" class=\"headerlink\" title=\"3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）\"></a><strong>3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）</strong></h3><p>需要关注的几个问题 </p>\n<p>（一）工业互联网平台商业化路径尚不明确 当前，我国工业互联网平台仍然处于发展初期，平台技术研发投入 成本较高，平台在垂直行业的商业应用和推广仍处于探索阶段，优势互 补、协同发展的平台产业生态亟待完善。同时，产业统计口径中也缺乏 有效反映数字经济时代工业互联网价值的创新统计方法，无法精确统计 平台投入产出效益，阻碍了创新解决方案培育、推广与普及。 </p>\n<p>（二）解决方案在中小企业规模化推广困难 与传统消费互联网相比，工业互联网平台解决方案在落地过程中涉 及大量数字化改造、二次开发和系统集成等工作，推广难度和成本超过 企业预期。其中以工业企业数字化试点建设为例，整个建设周期大概需 要持续1-2年甚至更长时间，而对于数量众多的中小企业仅能实现局部改 造，全面的工业互联网解决方案应用推广任务艰巨。 中国工业和信息化 发展形势展望系列 270 </p>\n<p>（三）工业信息安全保障薄弱制约平台应用 随着工业互联网的快速发展，制造环境走向开放、跨域、互联，工 业信息安全问题日益突出。由于目前工业互联网平台在数据产权确认、 交易、保护、跨境流转等等方面的标准或规范尚不健全，工业信息安全 防护手段和机制尚不完善，导致企业普遍对于工业数据上云后的数据资 产流失、数据安全风险存在顾虑，亟待相关完善安全保障体系，确保平 台健康持续发展。</p>\n<p>针对存在的问题，提出以下建议：</p>\n<p>（一）夯实制造业数字化转型基础，着力突破工业互联网平台核技术 依托工业互联网创新发展战略，着力突破工业机理模型、算法、信 息物理系统等关键技术和核心产品，超前布局数字孪生、云化仿真设计 与运营管理软件等，支持建设平台开源社区，提升平台安全可靠发展能 力。发挥政产学研用合力，加强关键核心技术攻关突破。推动工业互联 网、大数据、人工智能与制造业深度融合，深化生产制造、经营管理、 市场服务等环节的数字化应用，加快传统行业数字化转型进程。 </p>\n<p>（二）加强标准示范引领作用，培育一批平台解决方案和典型应案例 强化示范引领作用，实施更大规模、更深层次的平台应用示范，围 绕“平台+5G”、“平台+区块链”等新技术融合趋势，遴选一批平台创 271 新解决方案和应用标杆。持续开展平台绩效评价，不断完善平台发展指 数评价框架，强化平台应用的价值导向，释放平台赋能数字经济效益。 持续推进一批工业互联网平台应用创新中心建设，整合地方工业互联网 平台创新资源与行业需求，搭建面向平台解决方案供需对接、成果推广 的公共服务平台，构建平台赋能产业发展的生态体系。 </p>\n<p>（三）聚焦块状经济和带状经济区域，系统性推进工业互联网平台由 点到面落地 以区域发展总体战略为基础，着力促进工业互联网向更多垂直领域 城延伸，积极推动工业互联网创新发展战略与京津冀一体化、长三角一 体化、粤港澳大湾区、振兴东北老工业基地、西部大开发等区域战略， “一带一路”合作倡议等统筹实施，打造工业互联网平台应用先导示范 区，加快平台由点及线到面应用普及，带动区域产业协同发展。推动建 立工业互联网生态发展基金，鼓励社会资本参与工业互联网平台建设， 推动产融结合创新发展。 </p>\n<p>（四）筑牢工业信息安全保障基础，加快构建覆盖国家、地方、企业 的三级安全技术防控体系 制定工业数据安全规范，研制工业互联网大数据分级分类、工业 APP管理、工业互联网平台建设评价等关键标准，围绕平台数据收集、存 中国工业和信息化 发展形势展望系列 272 储、传输、共享等各环节，明确差异化安全机制和策略。建设工业互联 网大数据中心，建立中央、地方、行业企业多层次数据管理机制，打破 数据孤岛和基础设施捆绑。强化平台安全监测预警能力，搭建国家、地 方、企业多级协同联动的态势感知网络，实现对重要和关键平台接入设 备、控制系统、运行数据的风险实时监测，感知边缘层、IaaS层、PaaS 层和SaaS层等的安全状态，切实提升工业互联网平台安全防护水平。</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。</p>\n<p>2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。</p>\n<p>3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）。</p>\n<p>本课程重点考核学生的分析能力和实践能力，具体包括：</p>\n<p>1．发展趋势、商业模式、服务需求的综合分析（30分）。</p>\n<p>2．平台架构、技术特点（40分）。</p>\n<p>3．基于工业互联网，提出云平台发展建设（30分）。</p>\n<h3 id=\"1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\"><a href=\"#1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\" class=\"headerlink\" title=\"1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。\"></a><strong>1．论述云平台的发展趋势、商业模式与企业需求。应从技术创新、模式创新、平台创新与服务创新等方面进行论述。注意论述与传统模式的差异（包括价值链构建的差异），开展需求分析。</strong></h3><ul>\n<li><p>技术创新</p>\n</li>\n<li><p>模式创新</p>\n</li>\n<li>平台创新</li>\n<li>服务创新</li>\n</ul>\n<p>（1）从发展环境看，工业互联网平台发展动力将由政策驱动转向企业自发需求</p>\n<p>2019年，工业互联网平台、网络、安全等配套政策及行业政策体系 261 趋于完善，发展工业互联网已成为各龙头企业重塑产业竞争优势、推动 转型升级的共识。以海尔、阿里为例，海尔基于COSMOPlat平台打造了包 括工业组网解决方案、大数据解决方案、边缘层解决方案、智能制造解 决方案、工业安全解决方案等在内的170多个专业解决方案，赋能农业、 房车、机械、建陶等行业生态；阿里通过打造“1+N”工业互联网平台体 系，依托阿里的品牌价值和技术服务优势，聚合服务100余家中小信息化 服务商、大数据创新企业和信息工程服务企业，实现云端工业APP一站式 开发、托管、集成、运维和交易。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160504217.png\" alt=\"image-20200623160504217\"></p>\n<p>展望2020年，企业将“自下而上”推动工业互联网平台建设及推 广，针对不同的服务对象构建区域、行业、企业子平台，聚焦协议转 换、边缘计算、工业机理模型、生产线数字孪生等平台关键技术，形成 更多具有价值的行业解决方案，推动工业互联网平台在地方加速落地。</p>\n<p>（2）从技术创新看，新一代信息技术将加速与云平台技术融合</p>\n<p>2019年，大数据、人工智能、5G、区块链等新一代信息技术日趋成 熟，持续为领先制造企业和信息技术企业发展拓展新空间，涌现出更多 “平台+新技术”创新解决方案。如，富士康、商飞公司、紫光云引擎等 通过“平台+5G”融合应用实现高可靠、低时延、高通量的数据集成，催 生远程运动控制、全场景运营优化、智能巡检等模式；中国电信、杭州 汽轮等开展“平台+4K/8K高清视频”融合探索，实现高精度、异构化图 像与视频数据分析，催生智能产品检测、设备远程运维等模式；商飞公 司、华为、中兴通信、海尔等通过“平台+VR/AR”融合应用实现三维图 像快速生成与分析，催生远程辅助故障诊断、工业设计、多工种协作等 模式。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160535443.png\" alt=\"image-20200623160535443\"></p>\n<p>展望2020年，新一代信息技术与工业互联网平台的融合发展将从简 单到复杂、由单点聚焦到全面开花，衍生出更多新模式新业态，实现应 用创新，加速融合创新应用的落地，推动新一代信息技术与制造业的深 度融合</p>\n<p>（3）从平台应用看，面向特定场景的系统解决方案将加速涌现</p>\n<p>2019年，工业互联网平台产业链图谱更加完善，平台产业创新持续 活跃，在各行业中应用的深度和广度不断提升。在电子行业，中国电子 推出“中电云网”平台，打造SMT行业协同云、数字零售云，开发出新型 工业电商综合解决方案。在机械行业，中联重科聚焦工程机械、农用机 中国工业和信息化 发展形势展望系列 264 械等产品上云上平台，打造基于云谷工业互联网平台的设备全生命周期 智能服务模式。在工控行业，和利时打造HiaCloud工业互联网平台，在 底层设备协议解析与数据转换基础上，推出了面向化工、纺织、能源、 家电等行业解决方案。在汽车行业，北汽新能源打造了“北汽云”京津 冀地区产业协同工业互联网平台，形成汽车个性化定制、质量大数据分 析、车联网等解决方案。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160602905.png\" alt=\"image-20200623160602905\"></p>\n<p>展望2020年，产业整体发展将更加务实，更多产业资源加速进入工 业互联网领域，平台企业将聚焦行业痛点问题，将技术突破、模式创新 与产业实际需求相结合，涌现形成更多面向特定场景、具有更大价值的 行业解决方案。</p>\n<p>（4）从生态建设看，面向工业互联网平台的科产金服务体系将进一 步升级</p>\n<p>2019年，产业、科技、金融等各方积极探索和践行产融合作。在产 融对接交流方面，工业互联网产融推进论坛、工业互联网产融结合座谈 会分别于2月和8月召开，来自政产学研用资各方参会人员围绕工业互联 中国工业和信息化 发展形势展望系列 268 网发展面临的经济大势、投资机会及产融合作模式进行了深入交流，相 关平台企业与投资机构进行了对接。在产业界，寄云科技完成新一轮融 资，树根互联完成B轮5亿元融资，创联科技完成千万元A轮融资，工业互 联网领域的价值投资活跃越发活跃。同时，各平台企业积极探索产融结 合新模式，推动平台商业模式持续创新，形成功能订阅、金融服务等新 模式，如用友科技以功能订阅方式为近50万家中小企业提供平台化、通 用性服务并实现盈利，常州天正创新“生产力征信模型”，提供基于平 台的融资租赁、风险预警、客户标定等服务，帮助2000余家企业客户获 得授信30亿，实际放款超过22亿</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/image-20200623160951565.png\" alt=\"image-20200623160951565\"></p>\n<p>展望2020年，面向工业互联网平台的科产金生态环境将持续向好。 随着各项产融结合政策的落地实施，更多产融对接平台搭建形成，产 业发展、科技创新、金融服务生态链更加完善，资本市场对以工业互联 网平台为代表的先进制造业企业将形成更多长期价值投资，形成产融结 合、良性互促的发展格局</p>\n<p>技术创新。工业互联网主要是数据驱动的工业智能，通过特定应用场景的工业APP体系，重点支撑网络化协同、智能化生产、个性化定制和服务化延伸。5G与工业互联网的融合，可以全面推动5G与垂直行业的研发设计、生产制造、管理服务等生产流程的深刻变革。典型的八大类5G+工业互联网融合应用，包括5G+超高清视频、5G+AR、5G+VR、5G+无人机、5G+云端机器人、5G+远程控制、5G+机器视觉以及5G+云化AGV（自动引导运输车）。</p>\n<p>模式创新。互联网平台主要面向消费者提供通用化服务，以用规模优势获取商业收益。工互联网平台侧重传统工业方式和企业用户（to B），更加强调面向特定场景的个性化服务。因此，不同于消费互联网以电子商务、广告竞价应用分成等为主流模式，工业互联网平台现阶段将以专业服务、功能订阅为最主要商业模式。</p>\n<ul>\n<li>专业服务：专业服务是当前平台企业的最主要盈利手段，基于平台的系统集成是最主要服务方式。绝大部分与设备管理、能耗优化、质量提升相关的大数据分析平台都以这种方式提供服务。</li>\n<li>功能订阅：功能订阅是现阶段平台盈利的重要补充，有可能成为未来平台商业模式的核心。IT资源及工业软件服务已普遍采用订阅服务方式。</li>\n<li>交易模式：交易模式中，工业产品交易相对成熟，制造能力交易与工业知识交易仍在探索。在工业产品交易方面，部分工业互联网平台依托其对产业链资源的集聚，提供工业产品交易服务。</li>\n<li>金融服务：金融服务模式显现巨大的价值潜力，是平台企业探索商业模式的新热点。推动产融结合是增强金融服务实体功能重要措施。工业企业及金融机构均可基于平台开展产融结合。目前从三条路径实现产融结合：一是数据+保险模式，二是数据+信贷模式，三是数据+租赁模式。</li>\n<li>应用商店：基于应用商店的分成模式刚刚起步。部分领先的工业互联网平台已经开始探索构建应用开发者商店，随着市场的成熟，这也可能成为平台一种新的盈利方式。</li>\n<li>平台销售：直接将平台作为一种软件产品进行销售，也是部分企业的盈利手段之一</li>\n</ul>\n<h3 id=\"2．论述平台的技术架构。包括“互联网-”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\"><a href=\"#2．论述平台的技术架构。包括“互联网-”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\" class=\"headerlink\" title=\"2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。\"></a><strong>2．论述平台的技术架构。包括“互联网+”及变革性创新技术趋势下，国内外技术发展及平台架构的特点分析。</strong></h3><p>  云服务平台涉及了很多产品与技术，表面上看起来的确有点纷繁复杂，但是云服务平台还是有迹可循和有理可依的，其架构如图 2-1所示 。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161313484\" alt=\"img\"></p>\n<p>上面这个云架构共分为服务和管理这两大部分。</p>\n<p>在服务方面，主要以提供用户基于云的各种服务为主，共包含三个层次：其一是<strong>Software as a Service软件即服务</strong>，简称SaaS，这层的作用是将应用主要以基于Web的方式提供给客户；其二是<strong>Platform as a Service平台即服务</strong>，简称PaaS，这层的作用是将一个应用的开发和部署平台作为服务提供给用户；其三是<strong>Infrastructure as a Service基础架构即服务</strong>，简称IaaS，这层的作用是将各种底层的计算（比如虚拟机）和存储等资源作为服务提供给用户。从用户角度而言，这三层服务，它们之间关系是独立的，因为它们提供的服务是完全不同的，而且面对的用户也不尽相同。但从技术角度而言，云服务这三层之间的关系并不是独立的，而是有一定依赖关系的，比如一个SaaS层的产品和服务不仅需要使用到SaaS层本身的技术，而且还依赖PaaS层所提供的开发和部署平台或者直接部署于IaaS层所提供的计算资源上，还有，PaaS层的产品和服务也很有可能构建于IaaS层服务之上。</p>\n<h4 id=\"软件即服务SaaS\"><a href=\"#软件即服务SaaS\" class=\"headerlink\" title=\"软件即服务SaaS\"></a>软件即服务SaaS</h4><p>软件即服务（SaaS）为商用软件提供基于网络的访问。例如Netflix、Gmail、Google Docs、Office Web Apps、SaaS为企业提供一种降低软件使用成本的方法 — 按需使用软件而不是为每台计算机购买许可证。SaaS 给软件厂商提供了新的机会。尤其是，SaaS软件厂商可以通过四个因素提高 ROI（投资回报）：提高部署的速度、增加用户接受率、减少支持的需要、降低实现和升级的成本。</p>\n<p>由于SaaS层离普通用户非常接近，所以在SaaS层所使用到的技术，大多耳熟能详，下面是其中最主要的五种：</p>\n<ul>\n<li>HTML</li>\n<li>JavaScript</li>\n<li>CSS</li>\n<li>Flash</li>\n<li>Silverlight</li>\n</ul>\n<p>在SaaS层的技术选型上，首先，由于通用性和较低的学习成本，大多数云计算产品都会比较倾向HTML 、JavaScript和CSS这对黄金组合，但是在HTML5被大家广泛接受之前，RIA技术在用户体验方面，还是具有一定的优势，所以Flash和Silverlight也将会有一定的用武之地，比如VMware vCloud就采用了基于Flash的Flex技术，而微软的云计算产品肯定会在今后大量使用Silverlight技术。</p>\n<h4 id=\"平台即服务PaaS\"><a href=\"#平台即服务PaaS\" class=\"headerlink\" title=\"平台即服务PaaS\"></a>平台即服务PaaS</h4><p>平台即服务（Platform as a Service，PaaS）提供对操作系统和相关服务的访问。它让用户能够使用提供商支持的编程语言和工具把应用程序部署到云中。用户不必管理或控制底层基础架构，而是控制部署的应用程序并在一定程度上控制应用程序驻留环境的配置。PaaS的提供者包括Google App Engine、Windows Azure、Force.com、Heroku等。通过PaaS这种模式，用户可以在一个提供SDK（Software Development Kit，即软件开发工具包）、文档、测试环境和部署环境等在内的开发平台上非常方便地编写和部署应用，而且不论是在部署，还是在运行的时候，用户都无需为服务器、 操作系统、网络和存储等资源的运维而操心，这些繁琐的工作都由PaaS云供应商负责。</p>\n<p>PaaS 层的技术比较多样性，下面是常见的五种：</p>\n<ul>\n<li>REST ：通过 REST（Representational State Transfer，表述性状态转移）技术，能够非常方便和优雅地将中间件层所支撑的部分服务提供给调用者</li>\n<li>多租户：就是能让一个单独的应用实例可以为多个组织服务，而且能保持良好的隔离性和安全性，并且通过这种技术，能有效地降低应用的购置和维护成本</li>\n<li>并行处理：为了处理海量的数据，需要利用庞大的x86集群进行规模巨大的并行处理，Google的MapReduce是这方面的代表之作</li>\n<li>应用服务器：在原有的应用服务器的基础上为云计算做了一定程度的优化，比如用于Google App Engine的Jetty应用服务器</li>\n<li>分布式缓存：通过分布式缓存技术，不仅能有效地降低对后台服务器的压力，而且还能加快相应的反应速度，最著名的分布式缓存例子莫过于Memcached</li>\n</ul>\n<p>对于很多PaaS平台，比如用于部署Ruby应用的Heroku云平台，应用服务器和分布式缓存都是必备的，同时REST技术也常用于对外的接口，多租户技术则主要用于SaaS应用的后台，比如用于支撑Salesforce 的CRM等应用的Force.com多租户内核，而并行处理技术常被作为单独的服务推出，比如Amazon的Elastic MapReduce </p>\n<h4 id=\"基础架构即服务IaaS\"><a href=\"#基础架构即服务IaaS\" class=\"headerlink\" title=\"基础架构即服务IaaS\"></a>基础架构即服务IaaS</h4><p> 基础架构，或称基础设施（Infrastructure）是云的基础。它由服务器、网络设备、存储磁盘等物理资产组成。在使用IaaS时，用户并不实际控制底层基础架构，而是控制操作系统、存储和部署应用程序，还在有限的程度上控制网络组件的选择。  在IaaS所采用的技术方面，都是一些比较底层的技术，其中有四种技术是比较常用的：</p>\n<ul>\n<li>虚拟化：也可以理解它为基础设施层的“多租户”，因为通过虚拟化技术，能够在一个物理服务器上生成多个虚拟机，并且能在这些虚拟机之间能实现全面的隔离， 这样不仅能减低服务器的购置成本，而且还能同时降低服务器的运维成本，成熟的x86虚拟化技术有VMware的ESX和开源的Xen 。</li>\n<li>分布式存储：为了承载海量的数据，同时也要保证这些数据的可管理性，所以需要一整套分布式的存储系统，在这方面， Google 的GFS是典范之作。</li>\n<li>关系型数据库：基本是在原有的关系型数据库的基础上做了扩展和管理等方面的优化，使其在云中更适应。</li>\n<li>NoSQL：为了满足一些关系数据库所无法满足的目标，比如支撑海量的数据等，一些公司特地设计一批不是基于关系模型的数据库，比如Google的BigTable和Facebook的Cassandra等</li>\n</ul>\n<p>现在大多数的IaaS服务都是基于Xen的，比如Amazon的EC2等，但VMware也推出了基于ESX技术的vCloud ，同时业界也有几个基于关系型数据库的云服务，比如Amazon 的RDS（Relational Database Service，关系型数据库服务）和Windows Azure SDS（SQL Data Services， SQL数据库服务）等。关于分布式存储和NoSQL，它们已经被广泛用于云平台的后端，比如Google App Engine的Datastore就是基于BigTable和GFS这两个技术之上的，而Amazon则推出基于NoSQL技术的Simple DB 。</p>\n<h4 id=\"架构示例：Salesforce-CRM\"><a href=\"#架构示例：Salesforce-CRM\" class=\"headerlink\" title=\"架构示例：Salesforce CRM\"></a>架构示例：Salesforce CRM</h4><p> 首先，从用户角度而言，Salesforce CRM属于SaaS层服务，主要通过在云中部署可定制化的CRM应用，来让企业用户在初始投入很低的情况下使用CRM，并且可根据自身的流程来灵活地定制，而且用户只需接入互联网就能使用。从技术角度而言，Salesforce CRM像很多SaaS产品一样，不仅用到SaaS层的技术，而且还用到PaaS层、IaaS层和云管理层的技术。图2-3为Salesforce CRM在技术层面上大致的架构。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20130605161728828\" alt=\"img\"></p>\n<p>  Salesforce CRM采用的主要技术包括以下几种。</p>\n<ul>\n<li>SaaS层，基于HTML、JavaScript和CSS这个黄金组合。</li>\n<li>PaaS层，在此层，Salesforce引入了多租户内核和为支撑此内核运行而定制的应用服务器</li>\n<li>IaaS层，虽然在后端还是使用在企业环境中很常见的Oracle数据库，但是它为了支撑上层的多租户内核作了很多优化</li>\n</ul>\n<h3 id=\"3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）\"><a href=\"#3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）\" class=\"headerlink\" title=\"3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）\"></a><strong>3．我国发展工业互联网平台的建议。包括需求、问题、特点等方面的分析，提出工业互联网平台发展建设（模式、平台、架构、应用等方面）</strong></h3><p>需要关注的几个问题 </p>\n<p>（一）工业互联网平台商业化路径尚不明确 当前，我国工业互联网平台仍然处于发展初期，平台技术研发投入 成本较高，平台在垂直行业的商业应用和推广仍处于探索阶段，优势互 补、协同发展的平台产业生态亟待完善。同时，产业统计口径中也缺乏 有效反映数字经济时代工业互联网价值的创新统计方法，无法精确统计 平台投入产出效益，阻碍了创新解决方案培育、推广与普及。 </p>\n<p>（二）解决方案在中小企业规模化推广困难 与传统消费互联网相比，工业互联网平台解决方案在落地过程中涉 及大量数字化改造、二次开发和系统集成等工作，推广难度和成本超过 企业预期。其中以工业企业数字化试点建设为例，整个建设周期大概需 要持续1-2年甚至更长时间，而对于数量众多的中小企业仅能实现局部改 造，全面的工业互联网解决方案应用推广任务艰巨。 中国工业和信息化 发展形势展望系列 270 </p>\n<p>（三）工业信息安全保障薄弱制约平台应用 随着工业互联网的快速发展，制造环境走向开放、跨域、互联，工 业信息安全问题日益突出。由于目前工业互联网平台在数据产权确认、 交易、保护、跨境流转等等方面的标准或规范尚不健全，工业信息安全 防护手段和机制尚不完善，导致企业普遍对于工业数据上云后的数据资 产流失、数据安全风险存在顾虑，亟待相关完善安全保障体系，确保平 台健康持续发展。</p>\n<p>针对存在的问题，提出以下建议：</p>\n<p>（一）夯实制造业数字化转型基础，着力突破工业互联网平台核技术 依托工业互联网创新发展战略，着力突破工业机理模型、算法、信 息物理系统等关键技术和核心产品，超前布局数字孪生、云化仿真设计 与运营管理软件等，支持建设平台开源社区，提升平台安全可靠发展能 力。发挥政产学研用合力，加强关键核心技术攻关突破。推动工业互联 网、大数据、人工智能与制造业深度融合，深化生产制造、经营管理、 市场服务等环节的数字化应用，加快传统行业数字化转型进程。 </p>\n<p>（二）加强标准示范引领作用，培育一批平台解决方案和典型应案例 强化示范引领作用，实施更大规模、更深层次的平台应用示范，围 绕“平台+5G”、“平台+区块链”等新技术融合趋势，遴选一批平台创 271 新解决方案和应用标杆。持续开展平台绩效评价，不断完善平台发展指 数评价框架，强化平台应用的价值导向，释放平台赋能数字经济效益。 持续推进一批工业互联网平台应用创新中心建设，整合地方工业互联网 平台创新资源与行业需求，搭建面向平台解决方案供需对接、成果推广 的公共服务平台，构建平台赋能产业发展的生态体系。 </p>\n<p>（三）聚焦块状经济和带状经济区域，系统性推进工业互联网平台由 点到面落地 以区域发展总体战略为基础，着力促进工业互联网向更多垂直领域 城延伸，积极推动工业互联网创新发展战略与京津冀一体化、长三角一 体化、粤港澳大湾区、振兴东北老工业基地、西部大开发等区域战略， “一带一路”合作倡议等统筹实施，打造工业互联网平台应用先导示范 区，加快平台由点及线到面应用普及，带动区域产业协同发展。推动建 立工业互联网生态发展基金，鼓励社会资本参与工业互联网平台建设， 推动产融结合创新发展。 </p>\n<p>（四）筑牢工业信息安全保障基础，加快构建覆盖国家、地方、企业 的三级安全技术防控体系 制定工业数据安全规范，研制工业互联网大数据分级分类、工业 APP管理、工业互联网平台建设评价等关键标准，围绕平台数据收集、存 中国工业和信息化 发展形势展望系列 272 储、传输、共享等各环节，明确差异化安全机制和策略。建设工业互联 网大数据中心，建立中央、地方、行业企业多层次数据管理机制，打破 数据孤岛和基础设施捆绑。强化平台安全监测预警能力，搭建国家、地 方、企业多级协同联动的态势感知网络，实现对重要和关键平台接入设 备、控制系统、运行数据的风险实时监测，感知边缘层、IaaS层、PaaS 层和SaaS层等的安全状态，切实提升工业互联网平台安全防护水平。</p>\n"},{"title":"我与小可爱","top":true,"cover":true,"toc":true,"mathjax":false,"reprintPolicy":"cc_by","date":"2020-06-17T14:42:22.000Z","author":null,"password":"002303677257ad71f55fb0f743fbed3c29c424d63280582e15168c98a088be29","img":"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200619104910.jpg","summary":"我与小可爱的点点滴滴","keywords":"小可爱","_content":"\n6.17 哈哈哈，晋级饲养员啦，我的表白信：\n\n>小可爱你好呀\n>\n>这应该算是一封情书吧，也算是告白信吧，抱歉啦，明明说过要再多了解了解的，可是反过来一想，到底要了解到什么程度才算了解了呢？我没有答案，我也不知道后面会发生什么，我就想啊，这缘分要是不抓住，会不会就跑了呢？为什么不趁着现在表白呢?我呢，有一丢丢害羞啦，只能先这样表白啦，回学校我一定给你补上玫瑰，再当面亲口对你说啦。嗯，遇上小可爱你呀，真是三生有幸，有句话叫做“一切的错过，都是为了更好的相遇”，我觉得缘分使我们相遇，就是上天对我们最好的安排（是你自己选的我的毕设任务吗？）初识时呢，是12月22日，还记得吗？那一天聊了之后就是一月份了再聊的了呢。\n>\n>后来在例会上听见了小可爱的声音，就很突然，感觉，哇，这声音真好听，可以说我是因为声音开始对小可爱有一些想法的，后来3月份稍加频繁联系了些，四月份我主动加了小可爱的QQ，想着能多了解一下呢，四月底五月初开始了频繁的聊天。5月4号开始呢，每天有意无意的找你聊天，维持小火花不灭，慢慢的发现和小可爱聊天真快乐，一发不可收拾，后来关注了我的知乎，我第一时间就去把小可爱的知乎瞅了个遍，发现还有点蠢萌蠢萌的呢，在相处中呢，我有发现小可爱似乎对我也有点意思，只是不太清楚到了哪种程度，第一次听到小可爱把我安利的东西全都收藏了起来，超感动的，完全超出了我的预料诶，超级开心啦，后面还发现小可爱有点小机灵鬼，就很有趣，性格也很好，好期待见面的呢，小可爱说请我吃饭的时候，我有一丢丢紧张，就是期待又紧张，因为如果这次见面搞砸了的话，会对后续有超级大的影响啦，有点害怕，不过还是鼓起勇气去啦，见了面发现真棒，是我喜欢的类型，就喜欢小可爱这种留着刘海，笑容迷人，眼睛也漂亮的可可爱爱的女孩子，嗯，168也还是可爱的，真是完美身高，身材看起来也不错诶（害羞），至于小可爱昨晚上说的哪些都没有很care的点啦。\n>\n>嗯，虽然我们现在对对方还没有非常了解，不过我觉着我们也可以在一起后再慢慢了解啦（没错，我忍不住啦），毕竟没有一对情侣是完全了解对方后再决定是否在一起的啦，所以嘞，小可爱，我喜欢你，超级喜欢你，可以和我交往吗？我想用余生慢慢了解你，照顾你，宠着你呀。\n>\n>嗯，我超级宠女朋友的哟。\n>\n>我超级羡慕盖茨比对爱情的梦想和追求，盖茨比想纠正错误，回到从前，他后悔了，而我不想留下遗憾，所以你愿意跟我一起吗？\n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t饶志双  \n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   2020.6.16\n>\n> \n\n小可爱给我的回信，心心念念的，半夜两点多突然醒了，然后悄悄看了（含羞啦）：\n\n>​\t我愿意和你在一起，做你的女朋友\n>\n>​\t我喜欢和你聊天，我能从中获得快乐，感受到幸福\n>\n>​\t我喜欢和你听音乐，你喜欢的歌我也很喜欢\n>\n>​\t我喜欢看你用Github插件(我没记错的话)做的个人主页，我说的崇拜是认真的\n>\n>​\t我喜欢你在机场把背包放到我身上的右手，让我觉得很温暖\n>\n>​\t我喜欢和你在机场的最后一个再见，我从安检的地方伸出头时你还在\n>\n> \n>\n>​\t对于爱情我渴望又畏惧\n>\n>​\t我谈过恋爱，也没谈过恋爱\n>\n>​\t除了爸爸和哥哥，我没有真正的在肢体上亲近过任何一个男生\n>\n>​\t对于谈恋爱，我内心是忐忑的\n>\n>​\t我希望自己成为你的小可爱，但又对这个身份有着些许陌生\n>\n>​\t我其实是个小孩子，在爱的包围下长大，内心渴望被保护\n>\n>​\t我习惯了爸爸的疼爱和哥哥的关心，但是还不太习惯男朋友的存在\n>\n>​\t我说不出肉麻的情话\n>\n>​\t我希望你可以慢慢靠近我，我担心自己会因为害怕而逃跑\n>\n>​\t唉，我22了，听起来很矫情哈，你就当我还是小孩子吧\n>\n> \n>\n>​\t偷偷告诉你\n>\n>​\t和你的聊天记录我看了好多遍了\n>\n>​\t刚刚在楼下一个人鬼鬼祟祟的给你打电话，像在做啥见不得人的事\n>\n>​\t我会尽快适应我的新身份\n>\n>​\t虽然表达上有点障碍，但是喜欢你是认真的\n>\n>​\t还有啊，我的毕设题目确实是我自己选的，这确实是缘分噢\n>\n>​\t我虽然不缺爱，但是缺乏安全感\n>\n>​\t你要好好对我啊\n>\n> \n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t毛慧慧\n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   2020.6.17\n>\n> \n\n今天暂时到这儿啦～\n\n6.18 小可爱 属虎 超凶的那种（感觉明明属猪嘛，最幸福的品种，毕竟有我宠O(∩_∩)O哈哈哈~）\n\n6.18 小可爱要是真秃了，我一定给小可爱买假发(要不要考虑买点枸杞呢)\n\n- [x] 6.18 想看小可爱戴帽子的样子\n\n- [ ] 6.18 小可爱什么时候给我拍新疆的日出呀==\n\n6.18 喜欢听小可爱喘的声音（有点变态的样子，但真的好听）\n\n6.18 小可爱生气了 会不想说话\n\n6.18 小可爱戏精真好玩\n\n6.18 小可爱说以后领钱了就要投喂我呢，真幸福，让我来做家庭主男吧，嘻嘻\n\n6.18 现在要好好饲养小可爱，那么可爱，多招人喜欢呀，还省钱（（））\n\n- [ ] 给小可爱穿自己的衣服\n- [ ] 小可爱7.21生日策划\n\n6.18 今天和小可爱打了好久的电话呀，特别想跟小可爱聊天\n\n- [ ] 给小可爱准备愿望卡（一个月一张，不能过分哦，比如女装什么的）\n\n**6.18 一直喜欢慧慧小可爱**\n\n**6.18 一直喜欢慧慧小可爱**\n\n**6.18 一直喜欢慧慧小可爱**\n\n6.19 小可爱不怎么吃零食，怕长胖，以后我的零食给她吃两成就行了\n\n- [ ] 红烧排骨\n\n- [ ] ‌衣服 双双的小可爱 慧慧的大保镖\n\n6.19 小可爱骗我妈妈要看我，太吓人了\n\n6.19 小可爱给我看了戴帽子的样子，嘻嘻\n\n6.19 今天和小可爱打了一整天电话呢，晚上连着打了四个多小时电话 嘿嘿嘿\n\n- [ ] 和小可爱一起拍好看的情侣照\n\n6.19 小可爱喜欢哭鼻子，要好好宠着呢\n\n6.19 小可爱虽然做的不对，可是小可爱不渣哦，有清清楚楚明明白白说清楚就做的很棒啦\n\n6.20 小可爱想公主抱，害羞 52公斤\n\n- [x] 项链，对戒\n\n6.20 小可爱控制不了自己啦，沉迷恋爱，无心学习\n\n- [ ] 想枕着膝枕，听小可爱唱情歌，嘿嘿嘿\n\n6.20 小可爱可能只适合一两面\n\n6.21 永远不凶毛慧慧小可爱\n\n- [ ] 带小慧慧去看辛夷花（3月，4月）\n- [ ] 带小慧慧去看电影\n\n6.21 忙的时候要提前告诉小可爱(๑• . •๑)\n\n6.21 看到小可爱的消息要及时回，忙不开也要说一声\n\n6.21 小可爱呀，我好喜欢你呀，特别特别喜欢你，希望余生，有你一起陪我\n\n6.21 包吃包住哦，小可爱快过来呀，超期待的\n\n6.21 小慧慧小可爱记得养我呀\n\n6.21 小可爱哭了，以后可要等我在身边再哭呀\n\n- [ ] 见面的时候给小可爱一个大大的抱抱\n\n6.21 小可爱，我不会再爱上其他任何人了，以后慧慧小可爱是我的，我是慧慧小可爱的\n\n6.22 慧慧小可爱怕冷，冬天的时候要用脖子帮慧慧小可爱暖手\n\n6.22 慧慧小可爱讨厌冷暴力，不可以生气太久，要及时找慧慧小可爱说话\n\n6.22 要把慧慧小可爱的指纹录入手机\n\n6.22 手机的主题壁纸要和慧慧小可爱一样，开学见面的时候慧慧小可爱要看\n\n6.22 慧慧小可爱生气的时候要哄，要多和慧慧小可爱说话\n\n6.22 不可以说对慧慧小可爱没感觉，慧慧小可爱会伤心的\n\n6.23 起来工作啦\n\n6.23 小可爱越来越流氓了（哈哈哈）\n\n6.23 选好了戒指和项链\n\n6.24 小可爱腿腿看起来蛮修长的诶\n\n6.24 今天一天都在和小可爱打电话，真的打了一天电话哦\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090518.png\" style=\"zoom: 25%;\" />\n\n6.25 小可爱给爸爸妈妈说了我，超开心啦，还晚上偷偷表白，我都有记在心里，真好，世界上最幸福的事就是我喜欢的人刚好喜欢我，比这更幸福的是我喜欢的人比我想象的更喜欢我，爱你噢，小慧慧\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090859.png\" style=\"zoom: 25%;\" />\n\n6.25 今天和室友聚餐了，说起来都在成都，但真的五个人在一起聚都快一年没有了，晚上小可爱吃的烤羊肉串很好吃的样子，晚上十点多了到家，小可爱还出去顶着被蚊子叮咬和我打电话，就是个小傻蛋（给小可爱亲亲安慰）\n\n6.25 超级无敌可爱温柔爱我比爱自己多的慧慧小可爱，人又乖，笑起来又好看，声音又好听，我最喜欢了，小可爱别惩罚我了（今天打牌没有及时回小可爱消息，小可爱可难过了）\n\n6.26 小可爱小小的愿望：一个甜甜的有惊喜的初吻\n\n6.26 小可爱就是个小傻蛋，做一些奇奇怪怪的梦，哼，都说梦是反的，我要一直一直和小可爱在一起。\n\n6.26 戒指到了，戴起来真好看，戴着就感觉承担起了一个家庭的责任（有点怪怪的样子，第一次戴诶）\n\n6.26 慧慧大傻蛋，说不给我写，我可伤心了，就不想理小可爱了，虽然知道是开玩笑的，可还是会伤心，就还是会难过，我也是一个小公主呢，哼。\n\n6.26 小可爱伤心难过，我也会伤心难过，小可爱哭，我就会手足无措，我不想小可爱再伤心难过了，小可爱，我想你了，想见到你，超级超级想，超级超级超级想，看见室友和他女朋友，就很羡慕，好想你也在我身边呀，你伤心难过的时候我可以抱抱你，你哭的时候我可以陪着你，可以擦去你眼角的泪水，而不是像现在这样什么也做不了，真的好想你呀。\n\n6.27 对待小可爱我是认真的，超级认真的，小可爱说缺少安全感，所以我可以让小可爱看我的ＱＱ和电脑，我说想和小可爱一直走下去也是认真的，或许会遇到比小可爱更优秀的女生，但那不是我的，我认定了小可爱，小可爱就是我的唯一，我是个很专一的人，你若不离不弃，我便陪你到老，这不是我说说而已，我就是这么固执的一个人，有了你，我就会屏蔽掉其他人，保持应有的距离，小可爱，余生很长，我们慢慢变老，和小可爱在一起我才发现，我可以这么喜欢一个人，就很幸福，头一次发现原来我也会这么粘人。\n\n- [ ] 6.27 小可爱给我煮粥、煎饼\n\n6.27 小可爱大懒虫，睡了一下午\n\n6.28 喜欢一直一直和小可爱在一起，做小可爱的避湾港\n\n6.29 陪小可爱看《十日游戏》\n\n6.30 超喜欢小可爱唱歌，期待小可爱的《喜欢你》\n\n7.9 宝宝下雨时会难过，接送宝宝\n\n7.9 宝宝在身边的时候告诉答应告诉宝宝的事\n\n7.20宝宝要亲亲抱抱我（（））\n\n7.20宝宝QQ换了和我一样的字体气泡，宝宝想我亲脚脚\n\n7.20和宝宝5200分了，纪念一下\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722144821.jpg\" style=\"zoom:25%;\" />\n\n7.21小宝宝生日，没有我陪着太可怜了，臭宝宝呀，好想你哦，小宝宝真的喜欢粉色的哇，宝宝呀，以后过生在我身边过吧，我来宠着宝宝。宝宝发朋友圈了，开心\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722142949.jpg\" style=\"zoom: 33%;\" />\n\n7.22宝宝什么时候看《死神来了》勒，宝宝要越来越忙了，难受==\n\n7.22想宝宝的第N天，宝宝喜欢看我的博客，肯定是喜欢被在乎的感觉，宝宝呀，即使没更，还是很在乎宝宝的哦\n\n- [ ] 7.22等着验证臭宝宝会不会睁着眼睡觉\n\n7.22和宝宝的聊天记录好多好多哦\n\n7.24宝宝脖子疼，多休息休息啦\n\n7.24宝宝说过来了就照顾我，我可记下来了\n\n7.24宝宝给我做好吃的\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200724191906.jpg\" style=\"zoom: 33%;\" />\n\n7.25宝宝情感需求大，就像一个刚出生的小宝宝，需要细心呵护\n\n7.25宝宝说两个人走到最后靠的是“相信”、“依赖”，我觉得两个人走到最后靠的是“包容”和“沟通”，包容缺点也好，包容性格也好，包容就像粘合剂，能让两个人贴的更近，沟通是一种方法，能有效传达双方情感心意的方法，也是有效解决双方矛盾误解方法，深层次的沟通能让我们更加了解彼此，更加懂得对方。相信和依赖都是一种状态，一种情感上的交流，代表着我百分之百的心意，全身心的投入。\n\n7.25我希望宝宝有什么都能和我讲，我喜欢共享宝宝的所有情绪\n\n7.25我以后可能是个醋王，连宝宝弟弟的醋都吃（（））\n\n7.25深深演唱会还行，喜欢《随风》、《不想睡》，晚安，明天见\n\n7.27臭宝宝，问了四次爱我不，都说不爱，可难过了，昨晚上失眠了，睡眠质量很差，精神不咋好，本来是想找宝宝充电的，哼。\n\n7.27臭宝宝，认错态度良好，还很积极，我真的很开心，我挺害怕把宝宝宠坏，害怕宝宝持宠而娇，那样会非常难受的，因为我也挺喜欢宝宝宠我的，看到宝宝认错发的消息，眼泪哗哗的就掉下来了，真的好喜欢宝宝呀。\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200728100236.jpg\" style=\"zoom: 25%;\" />\n\n7.27宝宝要记得回家了手写哦，要给我看（（））爱你，宝宝\n\n7.28今天去买虾回来做\n\n8.8宝宝哭的很厉害，手脚发麻，一直抽泣不能控制，对不起宝宝，让宝宝遭受了这么大痛苦，宝宝真的吓到我了。\n\n8.9小宝宝情感比我细腻很多很多，情绪积累很快，宝宝真的真的好爱我呀，我的一些举动对宝宝的影响很大，宝宝在我身上寄托了很深的情感，深到我会不经意间让宝宝伤心难过\n\n8.9宝宝，我们好相似呀，对对方需求大，生气了不想说话，不开心了想对方找自己，哄自己。\n\n8.9宝宝不开心了要记得把宝宝哄开心，不能自己也不开心了就不管宝宝，把宝宝丢下，对不起宝宝，我知道错啦，以后多主动找找宝宝，宝宝也需要多多的宠爱，爱你 ，宝宝\n\n8.9宝宝开心的样子真的很好看，宝宝开心我也开心，宝宝穿漏肩长裙真的又性感又漂亮，以后要多穿穿\n\n8.9早上神智不清晰，最容易受宝宝撩拨了，宝宝可得负责，我现在可想亲宝宝肩了，既想靠在肩上又想亲亲的咬它，我不纯洁了，宝宝（雾）\n\n8.10早起，体检，去学校，打扫寝室，羞羞\n\n8.11爱宝宝，也爱学习","source":"_posts/我与小可爱.md","raw":"---\ntitle: 我与小可爱\ntop: true\ncover: true\ntoc: true\nmathjax: false\nreprintPolicy: cc_by\ndate: 2020-06-17 22:42:22\nauthor:\npassword: 002303677257ad71f55fb0f743fbed3c29c424d63280582e15168c98a088be29\nimg: https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200619104910.jpg\nsummary: 我与小可爱的点点滴滴\ncategories: 小可爱\nkeywords: 小可爱\ntags: 小可爱\n---\n\n6.17 哈哈哈，晋级饲养员啦，我的表白信：\n\n>小可爱你好呀\n>\n>这应该算是一封情书吧，也算是告白信吧，抱歉啦，明明说过要再多了解了解的，可是反过来一想，到底要了解到什么程度才算了解了呢？我没有答案，我也不知道后面会发生什么，我就想啊，这缘分要是不抓住，会不会就跑了呢？为什么不趁着现在表白呢?我呢，有一丢丢害羞啦，只能先这样表白啦，回学校我一定给你补上玫瑰，再当面亲口对你说啦。嗯，遇上小可爱你呀，真是三生有幸，有句话叫做“一切的错过，都是为了更好的相遇”，我觉得缘分使我们相遇，就是上天对我们最好的安排（是你自己选的我的毕设任务吗？）初识时呢，是12月22日，还记得吗？那一天聊了之后就是一月份了再聊的了呢。\n>\n>后来在例会上听见了小可爱的声音，就很突然，感觉，哇，这声音真好听，可以说我是因为声音开始对小可爱有一些想法的，后来3月份稍加频繁联系了些，四月份我主动加了小可爱的QQ，想着能多了解一下呢，四月底五月初开始了频繁的聊天。5月4号开始呢，每天有意无意的找你聊天，维持小火花不灭，慢慢的发现和小可爱聊天真快乐，一发不可收拾，后来关注了我的知乎，我第一时间就去把小可爱的知乎瞅了个遍，发现还有点蠢萌蠢萌的呢，在相处中呢，我有发现小可爱似乎对我也有点意思，只是不太清楚到了哪种程度，第一次听到小可爱把我安利的东西全都收藏了起来，超感动的，完全超出了我的预料诶，超级开心啦，后面还发现小可爱有点小机灵鬼，就很有趣，性格也很好，好期待见面的呢，小可爱说请我吃饭的时候，我有一丢丢紧张，就是期待又紧张，因为如果这次见面搞砸了的话，会对后续有超级大的影响啦，有点害怕，不过还是鼓起勇气去啦，见了面发现真棒，是我喜欢的类型，就喜欢小可爱这种留着刘海，笑容迷人，眼睛也漂亮的可可爱爱的女孩子，嗯，168也还是可爱的，真是完美身高，身材看起来也不错诶（害羞），至于小可爱昨晚上说的哪些都没有很care的点啦。\n>\n>嗯，虽然我们现在对对方还没有非常了解，不过我觉着我们也可以在一起后再慢慢了解啦（没错，我忍不住啦），毕竟没有一对情侣是完全了解对方后再决定是否在一起的啦，所以嘞，小可爱，我喜欢你，超级喜欢你，可以和我交往吗？我想用余生慢慢了解你，照顾你，宠着你呀。\n>\n>嗯，我超级宠女朋友的哟。\n>\n>我超级羡慕盖茨比对爱情的梦想和追求，盖茨比想纠正错误，回到从前，他后悔了，而我不想留下遗憾，所以你愿意跟我一起吗？\n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t饶志双  \n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   2020.6.16\n>\n> \n\n小可爱给我的回信，心心念念的，半夜两点多突然醒了，然后悄悄看了（含羞啦）：\n\n>​\t我愿意和你在一起，做你的女朋友\n>\n>​\t我喜欢和你聊天，我能从中获得快乐，感受到幸福\n>\n>​\t我喜欢和你听音乐，你喜欢的歌我也很喜欢\n>\n>​\t我喜欢看你用Github插件(我没记错的话)做的个人主页，我说的崇拜是认真的\n>\n>​\t我喜欢你在机场把背包放到我身上的右手，让我觉得很温暖\n>\n>​\t我喜欢和你在机场的最后一个再见，我从安检的地方伸出头时你还在\n>\n> \n>\n>​\t对于爱情我渴望又畏惧\n>\n>​\t我谈过恋爱，也没谈过恋爱\n>\n>​\t除了爸爸和哥哥，我没有真正的在肢体上亲近过任何一个男生\n>\n>​\t对于谈恋爱，我内心是忐忑的\n>\n>​\t我希望自己成为你的小可爱，但又对这个身份有着些许陌生\n>\n>​\t我其实是个小孩子，在爱的包围下长大，内心渴望被保护\n>\n>​\t我习惯了爸爸的疼爱和哥哥的关心，但是还不太习惯男朋友的存在\n>\n>​\t我说不出肉麻的情话\n>\n>​\t我希望你可以慢慢靠近我，我担心自己会因为害怕而逃跑\n>\n>​\t唉，我22了，听起来很矫情哈，你就当我还是小孩子吧\n>\n> \n>\n>​\t偷偷告诉你\n>\n>​\t和你的聊天记录我看了好多遍了\n>\n>​\t刚刚在楼下一个人鬼鬼祟祟的给你打电话，像在做啥见不得人的事\n>\n>​\t我会尽快适应我的新身份\n>\n>​\t虽然表达上有点障碍，但是喜欢你是认真的\n>\n>​\t还有啊，我的毕设题目确实是我自己选的，这确实是缘分噢\n>\n>​\t我虽然不缺爱，但是缺乏安全感\n>\n>​\t你要好好对我啊\n>\n> \n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t毛慧慧\n>\n>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   2020.6.17\n>\n> \n\n今天暂时到这儿啦～\n\n6.18 小可爱 属虎 超凶的那种（感觉明明属猪嘛，最幸福的品种，毕竟有我宠O(∩_∩)O哈哈哈~）\n\n6.18 小可爱要是真秃了，我一定给小可爱买假发(要不要考虑买点枸杞呢)\n\n- [x] 6.18 想看小可爱戴帽子的样子\n\n- [ ] 6.18 小可爱什么时候给我拍新疆的日出呀==\n\n6.18 喜欢听小可爱喘的声音（有点变态的样子，但真的好听）\n\n6.18 小可爱生气了 会不想说话\n\n6.18 小可爱戏精真好玩\n\n6.18 小可爱说以后领钱了就要投喂我呢，真幸福，让我来做家庭主男吧，嘻嘻\n\n6.18 现在要好好饲养小可爱，那么可爱，多招人喜欢呀，还省钱（（））\n\n- [ ] 给小可爱穿自己的衣服\n- [ ] 小可爱7.21生日策划\n\n6.18 今天和小可爱打了好久的电话呀，特别想跟小可爱聊天\n\n- [ ] 给小可爱准备愿望卡（一个月一张，不能过分哦，比如女装什么的）\n\n**6.18 一直喜欢慧慧小可爱**\n\n**6.18 一直喜欢慧慧小可爱**\n\n**6.18 一直喜欢慧慧小可爱**\n\n6.19 小可爱不怎么吃零食，怕长胖，以后我的零食给她吃两成就行了\n\n- [ ] 红烧排骨\n\n- [ ] ‌衣服 双双的小可爱 慧慧的大保镖\n\n6.19 小可爱骗我妈妈要看我，太吓人了\n\n6.19 小可爱给我看了戴帽子的样子，嘻嘻\n\n6.19 今天和小可爱打了一整天电话呢，晚上连着打了四个多小时电话 嘿嘿嘿\n\n- [ ] 和小可爱一起拍好看的情侣照\n\n6.19 小可爱喜欢哭鼻子，要好好宠着呢\n\n6.19 小可爱虽然做的不对，可是小可爱不渣哦，有清清楚楚明明白白说清楚就做的很棒啦\n\n6.20 小可爱想公主抱，害羞 52公斤\n\n- [x] 项链，对戒\n\n6.20 小可爱控制不了自己啦，沉迷恋爱，无心学习\n\n- [ ] 想枕着膝枕，听小可爱唱情歌，嘿嘿嘿\n\n6.20 小可爱可能只适合一两面\n\n6.21 永远不凶毛慧慧小可爱\n\n- [ ] 带小慧慧去看辛夷花（3月，4月）\n- [ ] 带小慧慧去看电影\n\n6.21 忙的时候要提前告诉小可爱(๑• . •๑)\n\n6.21 看到小可爱的消息要及时回，忙不开也要说一声\n\n6.21 小可爱呀，我好喜欢你呀，特别特别喜欢你，希望余生，有你一起陪我\n\n6.21 包吃包住哦，小可爱快过来呀，超期待的\n\n6.21 小慧慧小可爱记得养我呀\n\n6.21 小可爱哭了，以后可要等我在身边再哭呀\n\n- [ ] 见面的时候给小可爱一个大大的抱抱\n\n6.21 小可爱，我不会再爱上其他任何人了，以后慧慧小可爱是我的，我是慧慧小可爱的\n\n6.22 慧慧小可爱怕冷，冬天的时候要用脖子帮慧慧小可爱暖手\n\n6.22 慧慧小可爱讨厌冷暴力，不可以生气太久，要及时找慧慧小可爱说话\n\n6.22 要把慧慧小可爱的指纹录入手机\n\n6.22 手机的主题壁纸要和慧慧小可爱一样，开学见面的时候慧慧小可爱要看\n\n6.22 慧慧小可爱生气的时候要哄，要多和慧慧小可爱说话\n\n6.22 不可以说对慧慧小可爱没感觉，慧慧小可爱会伤心的\n\n6.23 起来工作啦\n\n6.23 小可爱越来越流氓了（哈哈哈）\n\n6.23 选好了戒指和项链\n\n6.24 小可爱腿腿看起来蛮修长的诶\n\n6.24 今天一天都在和小可爱打电话，真的打了一天电话哦\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090518.png\" style=\"zoom: 25%;\" />\n\n6.25 小可爱给爸爸妈妈说了我，超开心啦，还晚上偷偷表白，我都有记在心里，真好，世界上最幸福的事就是我喜欢的人刚好喜欢我，比这更幸福的是我喜欢的人比我想象的更喜欢我，爱你噢，小慧慧\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090859.png\" style=\"zoom: 25%;\" />\n\n6.25 今天和室友聚餐了，说起来都在成都，但真的五个人在一起聚都快一年没有了，晚上小可爱吃的烤羊肉串很好吃的样子，晚上十点多了到家，小可爱还出去顶着被蚊子叮咬和我打电话，就是个小傻蛋（给小可爱亲亲安慰）\n\n6.25 超级无敌可爱温柔爱我比爱自己多的慧慧小可爱，人又乖，笑起来又好看，声音又好听，我最喜欢了，小可爱别惩罚我了（今天打牌没有及时回小可爱消息，小可爱可难过了）\n\n6.26 小可爱小小的愿望：一个甜甜的有惊喜的初吻\n\n6.26 小可爱就是个小傻蛋，做一些奇奇怪怪的梦，哼，都说梦是反的，我要一直一直和小可爱在一起。\n\n6.26 戒指到了，戴起来真好看，戴着就感觉承担起了一个家庭的责任（有点怪怪的样子，第一次戴诶）\n\n6.26 慧慧大傻蛋，说不给我写，我可伤心了，就不想理小可爱了，虽然知道是开玩笑的，可还是会伤心，就还是会难过，我也是一个小公主呢，哼。\n\n6.26 小可爱伤心难过，我也会伤心难过，小可爱哭，我就会手足无措，我不想小可爱再伤心难过了，小可爱，我想你了，想见到你，超级超级想，超级超级超级想，看见室友和他女朋友，就很羡慕，好想你也在我身边呀，你伤心难过的时候我可以抱抱你，你哭的时候我可以陪着你，可以擦去你眼角的泪水，而不是像现在这样什么也做不了，真的好想你呀。\n\n6.27 对待小可爱我是认真的，超级认真的，小可爱说缺少安全感，所以我可以让小可爱看我的ＱＱ和电脑，我说想和小可爱一直走下去也是认真的，或许会遇到比小可爱更优秀的女生，但那不是我的，我认定了小可爱，小可爱就是我的唯一，我是个很专一的人，你若不离不弃，我便陪你到老，这不是我说说而已，我就是这么固执的一个人，有了你，我就会屏蔽掉其他人，保持应有的距离，小可爱，余生很长，我们慢慢变老，和小可爱在一起我才发现，我可以这么喜欢一个人，就很幸福，头一次发现原来我也会这么粘人。\n\n- [ ] 6.27 小可爱给我煮粥、煎饼\n\n6.27 小可爱大懒虫，睡了一下午\n\n6.28 喜欢一直一直和小可爱在一起，做小可爱的避湾港\n\n6.29 陪小可爱看《十日游戏》\n\n6.30 超喜欢小可爱唱歌，期待小可爱的《喜欢你》\n\n7.9 宝宝下雨时会难过，接送宝宝\n\n7.9 宝宝在身边的时候告诉答应告诉宝宝的事\n\n7.20宝宝要亲亲抱抱我（（））\n\n7.20宝宝QQ换了和我一样的字体气泡，宝宝想我亲脚脚\n\n7.20和宝宝5200分了，纪念一下\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722144821.jpg\" style=\"zoom:25%;\" />\n\n7.21小宝宝生日，没有我陪着太可怜了，臭宝宝呀，好想你哦，小宝宝真的喜欢粉色的哇，宝宝呀，以后过生在我身边过吧，我来宠着宝宝。宝宝发朋友圈了，开心\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722142949.jpg\" style=\"zoom: 33%;\" />\n\n7.22宝宝什么时候看《死神来了》勒，宝宝要越来越忙了，难受==\n\n7.22想宝宝的第N天，宝宝喜欢看我的博客，肯定是喜欢被在乎的感觉，宝宝呀，即使没更，还是很在乎宝宝的哦\n\n- [ ] 7.22等着验证臭宝宝会不会睁着眼睡觉\n\n7.22和宝宝的聊天记录好多好多哦\n\n7.24宝宝脖子疼，多休息休息啦\n\n7.24宝宝说过来了就照顾我，我可记下来了\n\n7.24宝宝给我做好吃的\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200724191906.jpg\" style=\"zoom: 33%;\" />\n\n7.25宝宝情感需求大，就像一个刚出生的小宝宝，需要细心呵护\n\n7.25宝宝说两个人走到最后靠的是“相信”、“依赖”，我觉得两个人走到最后靠的是“包容”和“沟通”，包容缺点也好，包容性格也好，包容就像粘合剂，能让两个人贴的更近，沟通是一种方法，能有效传达双方情感心意的方法，也是有效解决双方矛盾误解方法，深层次的沟通能让我们更加了解彼此，更加懂得对方。相信和依赖都是一种状态，一种情感上的交流，代表着我百分之百的心意，全身心的投入。\n\n7.25我希望宝宝有什么都能和我讲，我喜欢共享宝宝的所有情绪\n\n7.25我以后可能是个醋王，连宝宝弟弟的醋都吃（（））\n\n7.25深深演唱会还行，喜欢《随风》、《不想睡》，晚安，明天见\n\n7.27臭宝宝，问了四次爱我不，都说不爱，可难过了，昨晚上失眠了，睡眠质量很差，精神不咋好，本来是想找宝宝充电的，哼。\n\n7.27臭宝宝，认错态度良好，还很积极，我真的很开心，我挺害怕把宝宝宠坏，害怕宝宝持宠而娇，那样会非常难受的，因为我也挺喜欢宝宝宠我的，看到宝宝认错发的消息，眼泪哗哗的就掉下来了，真的好喜欢宝宝呀。\n\n<img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200728100236.jpg\" style=\"zoom: 25%;\" />\n\n7.27宝宝要记得回家了手写哦，要给我看（（））爱你，宝宝\n\n7.28今天去买虾回来做\n\n8.8宝宝哭的很厉害，手脚发麻，一直抽泣不能控制，对不起宝宝，让宝宝遭受了这么大痛苦，宝宝真的吓到我了。\n\n8.9小宝宝情感比我细腻很多很多，情绪积累很快，宝宝真的真的好爱我呀，我的一些举动对宝宝的影响很大，宝宝在我身上寄托了很深的情感，深到我会不经意间让宝宝伤心难过\n\n8.9宝宝，我们好相似呀，对对方需求大，生气了不想说话，不开心了想对方找自己，哄自己。\n\n8.9宝宝不开心了要记得把宝宝哄开心，不能自己也不开心了就不管宝宝，把宝宝丢下，对不起宝宝，我知道错啦，以后多主动找找宝宝，宝宝也需要多多的宠爱，爱你 ，宝宝\n\n8.9宝宝开心的样子真的很好看，宝宝开心我也开心，宝宝穿漏肩长裙真的又性感又漂亮，以后要多穿穿\n\n8.9早上神智不清晰，最容易受宝宝撩拨了，宝宝可得负责，我现在可想亲宝宝肩了，既想靠在肩上又想亲亲的咬它，我不纯洁了，宝宝（雾）\n\n8.10早起，体检，去学校，打扫寝室，羞羞\n\n8.11爱宝宝，也爱学习","slug":"我与小可爱","published":1,"updated":"2020-08-20T08:41:44.652Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mb10014ewsegmtfaa0r","content":"<p>6.17 哈哈哈，晋级饲养员啦，我的表白信：</p>\n<blockquote>\n<p>小可爱你好呀</p>\n<p>这应该算是一封情书吧，也算是告白信吧，抱歉啦，明明说过要再多了解了解的，可是反过来一想，到底要了解到什么程度才算了解了呢？我没有答案，我也不知道后面会发生什么，我就想啊，这缘分要是不抓住，会不会就跑了呢？为什么不趁着现在表白呢?我呢，有一丢丢害羞啦，只能先这样表白啦，回学校我一定给你补上玫瑰，再当面亲口对你说啦。嗯，遇上小可爱你呀，真是三生有幸，有句话叫做“一切的错过，都是为了更好的相遇”，我觉得缘分使我们相遇，就是上天对我们最好的安排（是你自己选的我的毕设任务吗？）初识时呢，是12月22日，还记得吗？那一天聊了之后就是一月份了再聊的了呢。</p>\n<p>后来在例会上听见了小可爱的声音，就很突然，感觉，哇，这声音真好听，可以说我是因为声音开始对小可爱有一些想法的，后来3月份稍加频繁联系了些，四月份我主动加了小可爱的QQ，想着能多了解一下呢，四月底五月初开始了频繁的聊天。5月4号开始呢，每天有意无意的找你聊天，维持小火花不灭，慢慢的发现和小可爱聊天真快乐，一发不可收拾，后来关注了我的知乎，我第一时间就去把小可爱的知乎瞅了个遍，发现还有点蠢萌蠢萌的呢，在相处中呢，我有发现小可爱似乎对我也有点意思，只是不太清楚到了哪种程度，第一次听到小可爱把我安利的东西全都收藏了起来，超感动的，完全超出了我的预料诶，超级开心啦，后面还发现小可爱有点小机灵鬼，就很有趣，性格也很好，好期待见面的呢，小可爱说请我吃饭的时候，我有一丢丢紧张，就是期待又紧张，因为如果这次见面搞砸了的话，会对后续有超级大的影响啦，有点害怕，不过还是鼓起勇气去啦，见了面发现真棒，是我喜欢的类型，就喜欢小可爱这种留着刘海，笑容迷人，眼睛也漂亮的可可爱爱的女孩子，嗯，168也还是可爱的，真是完美身高，身材看起来也不错诶（害羞），至于小可爱昨晚上说的哪些都没有很care的点啦。</p>\n<p>嗯，虽然我们现在对对方还没有非常了解，不过我觉着我们也可以在一起后再慢慢了解啦（没错，我忍不住啦），毕竟没有一对情侣是完全了解对方后再决定是否在一起的啦，所以嘞，小可爱，我喜欢你，超级喜欢你，可以和我交往吗？我想用余生慢慢了解你，照顾你，宠着你呀。</p>\n<p>嗯，我超级宠女朋友的哟。</p>\n<p>我超级羡慕盖茨比对爱情的梦想和追求，盖茨比想纠正错误，回到从前，他后悔了，而我不想留下遗憾，所以你愿意跟我一起吗？</p>\n<p>​                                                                                                                                                            饶志双  </p>\n<p>​                                                                                                                                                           2020.6.16</p>\n</blockquote>\n<p>小可爱给我的回信，心心念念的，半夜两点多突然醒了，然后悄悄看了（含羞啦）：</p>\n<blockquote>\n<p>​    我愿意和你在一起，做你的女朋友</p>\n<p>​    我喜欢和你聊天，我能从中获得快乐，感受到幸福</p>\n<p>​    我喜欢和你听音乐，你喜欢的歌我也很喜欢</p>\n<p>​    我喜欢看你用Github插件(我没记错的话)做的个人主页，我说的崇拜是认真的</p>\n<p>​    我喜欢你在机场把背包放到我身上的右手，让我觉得很温暖</p>\n<p>​    我喜欢和你在机场的最后一个再见，我从安检的地方伸出头时你还在</p>\n<p>​    对于爱情我渴望又畏惧</p>\n<p>​    我谈过恋爱，也没谈过恋爱</p>\n<p>​    除了爸爸和哥哥，我没有真正的在肢体上亲近过任何一个男生</p>\n<p>​    对于谈恋爱，我内心是忐忑的</p>\n<p>​    我希望自己成为你的小可爱，但又对这个身份有着些许陌生</p>\n<p>​    我其实是个小孩子，在爱的包围下长大，内心渴望被保护</p>\n<p>​    我习惯了爸爸的疼爱和哥哥的关心，但是还不太习惯男朋友的存在</p>\n<p>​    我说不出肉麻的情话</p>\n<p>​    我希望你可以慢慢靠近我，我担心自己会因为害怕而逃跑</p>\n<p>​    唉，我22了，听起来很矫情哈，你就当我还是小孩子吧</p>\n<p>​    偷偷告诉你</p>\n<p>​    和你的聊天记录我看了好多遍了</p>\n<p>​    刚刚在楼下一个人鬼鬼祟祟的给你打电话，像在做啥见不得人的事</p>\n<p>​    我会尽快适应我的新身份</p>\n<p>​    虽然表达上有点障碍，但是喜欢你是认真的</p>\n<p>​    还有啊，我的毕设题目确实是我自己选的，这确实是缘分噢</p>\n<p>​    我虽然不缺爱，但是缺乏安全感</p>\n<p>​    你要好好对我啊</p>\n<p>​                                                                                                                                                                毛慧慧</p>\n<p>​                                                                                                                                                               2020.6.17</p>\n</blockquote>\n<p>今天暂时到这儿啦～</p>\n<p>6.18 小可爱 属虎 超凶的那种（感觉明明属猪嘛，最幸福的品种，毕竟有我宠O(∩_∩)O哈哈哈~）</p>\n<p>6.18 小可爱要是真秃了，我一定给小可爱买假发(要不要考虑买点枸杞呢)</p>\n<ul>\n<li><p>[x] 6.18 想看小可爱戴帽子的样子</p>\n</li>\n<li><p>[ ] 6.18 小可爱什么时候给我拍新疆的日出呀==</p>\n</li>\n</ul>\n<p>6.18 喜欢听小可爱喘的声音（有点变态的样子，但真的好听）</p>\n<p>6.18 小可爱生气了 会不想说话</p>\n<p>6.18 小可爱戏精真好玩</p>\n<p>6.18 小可爱说以后领钱了就要投喂我呢，真幸福，让我来做家庭主男吧，嘻嘻</p>\n<p>6.18 现在要好好饲养小可爱，那么可爱，多招人喜欢呀，还省钱（（））</p>\n<ul>\n<li>[ ] 给小可爱穿自己的衣服</li>\n<li>[ ] 小可爱7.21生日策划</li>\n</ul>\n<p>6.18 今天和小可爱打了好久的电话呀，特别想跟小可爱聊天</p>\n<ul>\n<li>[ ] 给小可爱准备愿望卡（一个月一张，不能过分哦，比如女装什么的）</li>\n</ul>\n<p><strong>6.18 一直喜欢慧慧小可爱</strong></p>\n<p><strong>6.18 一直喜欢慧慧小可爱</strong></p>\n<p><strong>6.18 一直喜欢慧慧小可爱</strong></p>\n<p>6.19 小可爱不怎么吃零食，怕长胖，以后我的零食给她吃两成就行了</p>\n<ul>\n<li><p>[ ] 红烧排骨</p>\n</li>\n<li><p>[ ] ‌衣服 双双的小可爱 慧慧的大保镖</p>\n</li>\n</ul>\n<p>6.19 小可爱骗我妈妈要看我，太吓人了</p>\n<p>6.19 小可爱给我看了戴帽子的样子，嘻嘻</p>\n<p>6.19 今天和小可爱打了一整天电话呢，晚上连着打了四个多小时电话 嘿嘿嘿</p>\n<ul>\n<li>[ ] 和小可爱一起拍好看的情侣照</li>\n</ul>\n<p>6.19 小可爱喜欢哭鼻子，要好好宠着呢</p>\n<p>6.19 小可爱虽然做的不对，可是小可爱不渣哦，有清清楚楚明明白白说清楚就做的很棒啦</p>\n<p>6.20 小可爱想公主抱，害羞 52公斤</p>\n<ul>\n<li>[x] 项链，对戒</li>\n</ul>\n<p>6.20 小可爱控制不了自己啦，沉迷恋爱，无心学习</p>\n<ul>\n<li>[ ] 想枕着膝枕，听小可爱唱情歌，嘿嘿嘿</li>\n</ul>\n<p>6.20 小可爱可能只适合一两面</p>\n<p>6.21 永远不凶毛慧慧小可爱</p>\n<ul>\n<li>[ ] 带小慧慧去看辛夷花（3月，4月）</li>\n<li>[ ] 带小慧慧去看电影</li>\n</ul>\n<p>6.21 忙的时候要提前告诉小可爱(๑• . •๑)</p>\n<p>6.21 看到小可爱的消息要及时回，忙不开也要说一声</p>\n<p>6.21 小可爱呀，我好喜欢你呀，特别特别喜欢你，希望余生，有你一起陪我</p>\n<p>6.21 包吃包住哦，小可爱快过来呀，超期待的</p>\n<p>6.21 小慧慧小可爱记得养我呀</p>\n<p>6.21 小可爱哭了，以后可要等我在身边再哭呀</p>\n<ul>\n<li>[ ] 见面的时候给小可爱一个大大的抱抱</li>\n</ul>\n<p>6.21 小可爱，我不会再爱上其他任何人了，以后慧慧小可爱是我的，我是慧慧小可爱的</p>\n<p>6.22 慧慧小可爱怕冷，冬天的时候要用脖子帮慧慧小可爱暖手</p>\n<p>6.22 慧慧小可爱讨厌冷暴力，不可以生气太久，要及时找慧慧小可爱说话</p>\n<p>6.22 要把慧慧小可爱的指纹录入手机</p>\n<p>6.22 手机的主题壁纸要和慧慧小可爱一样，开学见面的时候慧慧小可爱要看</p>\n<p>6.22 慧慧小可爱生气的时候要哄，要多和慧慧小可爱说话</p>\n<p>6.22 不可以说对慧慧小可爱没感觉，慧慧小可爱会伤心的</p>\n<p>6.23 起来工作啦</p>\n<p>6.23 小可爱越来越流氓了（哈哈哈）</p>\n<p>6.23 选好了戒指和项链</p>\n<p>6.24 小可爱腿腿看起来蛮修长的诶</p>\n<p>6.24 今天一天都在和小可爱打电话，真的打了一天电话哦</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090518.png\" style=\"zoom: 25%;\"></p>\n<p>6.25 小可爱给爸爸妈妈说了我，超开心啦，还晚上偷偷表白，我都有记在心里，真好，世界上最幸福的事就是我喜欢的人刚好喜欢我，比这更幸福的是我喜欢的人比我想象的更喜欢我，爱你噢，小慧慧</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090859.png\" style=\"zoom: 25%;\"></p>\n<p>6.25 今天和室友聚餐了，说起来都在成都，但真的五个人在一起聚都快一年没有了，晚上小可爱吃的烤羊肉串很好吃的样子，晚上十点多了到家，小可爱还出去顶着被蚊子叮咬和我打电话，就是个小傻蛋（给小可爱亲亲安慰）</p>\n<p>6.25 超级无敌可爱温柔爱我比爱自己多的慧慧小可爱，人又乖，笑起来又好看，声音又好听，我最喜欢了，小可爱别惩罚我了（今天打牌没有及时回小可爱消息，小可爱可难过了）</p>\n<p>6.26 小可爱小小的愿望：一个甜甜的有惊喜的初吻</p>\n<p>6.26 小可爱就是个小傻蛋，做一些奇奇怪怪的梦，哼，都说梦是反的，我要一直一直和小可爱在一起。</p>\n<p>6.26 戒指到了，戴起来真好看，戴着就感觉承担起了一个家庭的责任（有点怪怪的样子，第一次戴诶）</p>\n<p>6.26 慧慧大傻蛋，说不给我写，我可伤心了，就不想理小可爱了，虽然知道是开玩笑的，可还是会伤心，就还是会难过，我也是一个小公主呢，哼。</p>\n<p>6.26 小可爱伤心难过，我也会伤心难过，小可爱哭，我就会手足无措，我不想小可爱再伤心难过了，小可爱，我想你了，想见到你，超级超级想，超级超级超级想，看见室友和他女朋友，就很羡慕，好想你也在我身边呀，你伤心难过的时候我可以抱抱你，你哭的时候我可以陪着你，可以擦去你眼角的泪水，而不是像现在这样什么也做不了，真的好想你呀。</p>\n<p>6.27 对待小可爱我是认真的，超级认真的，小可爱说缺少安全感，所以我可以让小可爱看我的ＱＱ和电脑，我说想和小可爱一直走下去也是认真的，或许会遇到比小可爱更优秀的女生，但那不是我的，我认定了小可爱，小可爱就是我的唯一，我是个很专一的人，你若不离不弃，我便陪你到老，这不是我说说而已，我就是这么固执的一个人，有了你，我就会屏蔽掉其他人，保持应有的距离，小可爱，余生很长，我们慢慢变老，和小可爱在一起我才发现，我可以这么喜欢一个人，就很幸福，头一次发现原来我也会这么粘人。</p>\n<ul>\n<li>[ ] 6.27 小可爱给我煮粥、煎饼</li>\n</ul>\n<p>6.27 小可爱大懒虫，睡了一下午</p>\n<p>6.28 喜欢一直一直和小可爱在一起，做小可爱的避湾港</p>\n<p>6.29 陪小可爱看《十日游戏》</p>\n<p>6.30 超喜欢小可爱唱歌，期待小可爱的《喜欢你》</p>\n<p>7.9 宝宝下雨时会难过，接送宝宝</p>\n<p>7.9 宝宝在身边的时候告诉答应告诉宝宝的事</p>\n<p>7.20宝宝要亲亲抱抱我（（））</p>\n<p>7.20宝宝QQ换了和我一样的字体气泡，宝宝想我亲脚脚</p>\n<p>7.20和宝宝5200分了，纪念一下</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722144821.jpg\" style=\"zoom:25%;\"></p>\n<p>7.21小宝宝生日，没有我陪着太可怜了，臭宝宝呀，好想你哦，小宝宝真的喜欢粉色的哇，宝宝呀，以后过生在我身边过吧，我来宠着宝宝。宝宝发朋友圈了，开心</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722142949.jpg\" style=\"zoom: 33%;\"></p>\n<p>7.22宝宝什么时候看《死神来了》勒，宝宝要越来越忙了，难受==</p>\n<p>7.22想宝宝的第N天，宝宝喜欢看我的博客，肯定是喜欢被在乎的感觉，宝宝呀，即使没更，还是很在乎宝宝的哦</p>\n<ul>\n<li>[ ] 7.22等着验证臭宝宝会不会睁着眼睡觉</li>\n</ul>\n<p>7.22和宝宝的聊天记录好多好多哦</p>\n<p>7.24宝宝脖子疼，多休息休息啦</p>\n<p>7.24宝宝说过来了就照顾我，我可记下来了</p>\n<p>7.24宝宝给我做好吃的</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200724191906.jpg\" style=\"zoom: 33%;\"></p>\n<p>7.25宝宝情感需求大，就像一个刚出生的小宝宝，需要细心呵护</p>\n<p>7.25宝宝说两个人走到最后靠的是“相信”、“依赖”，我觉得两个人走到最后靠的是“包容”和“沟通”，包容缺点也好，包容性格也好，包容就像粘合剂，能让两个人贴的更近，沟通是一种方法，能有效传达双方情感心意的方法，也是有效解决双方矛盾误解方法，深层次的沟通能让我们更加了解彼此，更加懂得对方。相信和依赖都是一种状态，一种情感上的交流，代表着我百分之百的心意，全身心的投入。</p>\n<p>7.25我希望宝宝有什么都能和我讲，我喜欢共享宝宝的所有情绪</p>\n<p>7.25我以后可能是个醋王，连宝宝弟弟的醋都吃（（））</p>\n<p>7.25深深演唱会还行，喜欢《随风》、《不想睡》，晚安，明天见</p>\n<p>7.27臭宝宝，问了四次爱我不，都说不爱，可难过了，昨晚上失眠了，睡眠质量很差，精神不咋好，本来是想找宝宝充电的，哼。</p>\n<p>7.27臭宝宝，认错态度良好，还很积极，我真的很开心，我挺害怕把宝宝宠坏，害怕宝宝持宠而娇，那样会非常难受的，因为我也挺喜欢宝宝宠我的，看到宝宝认错发的消息，眼泪哗哗的就掉下来了，真的好喜欢宝宝呀。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200728100236.jpg\" style=\"zoom: 25%;\"></p>\n<p>7.27宝宝要记得回家了手写哦，要给我看（（））爱你，宝宝</p>\n<p>7.28今天去买虾回来做</p>\n<p>8.8宝宝哭的很厉害，手脚发麻，一直抽泣不能控制，对不起宝宝，让宝宝遭受了这么大痛苦，宝宝真的吓到我了。</p>\n<p>8.9小宝宝情感比我细腻很多很多，情绪积累很快，宝宝真的真的好爱我呀，我的一些举动对宝宝的影响很大，宝宝在我身上寄托了很深的情感，深到我会不经意间让宝宝伤心难过</p>\n<p>8.9宝宝，我们好相似呀，对对方需求大，生气了不想说话，不开心了想对方找自己，哄自己。</p>\n<p>8.9宝宝不开心了要记得把宝宝哄开心，不能自己也不开心了就不管宝宝，把宝宝丢下，对不起宝宝，我知道错啦，以后多主动找找宝宝，宝宝也需要多多的宠爱，爱你 ，宝宝</p>\n<p>8.9宝宝开心的样子真的很好看，宝宝开心我也开心，宝宝穿漏肩长裙真的又性感又漂亮，以后要多穿穿</p>\n<p>8.9早上神智不清晰，最容易受宝宝撩拨了，宝宝可得负责，我现在可想亲宝宝肩了，既想靠在肩上又想亲亲的咬它，我不纯洁了，宝宝（雾）</p>\n<p>8.10早起，体检，去学校，打扫寝室，羞羞</p>\n<p>8.11爱宝宝，也爱学习</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>6.17 哈哈哈，晋级饲养员啦，我的表白信：</p>\n<blockquote>\n<p>小可爱你好呀</p>\n<p>这应该算是一封情书吧，也算是告白信吧，抱歉啦，明明说过要再多了解了解的，可是反过来一想，到底要了解到什么程度才算了解了呢？我没有答案，我也不知道后面会发生什么，我就想啊，这缘分要是不抓住，会不会就跑了呢？为什么不趁着现在表白呢?我呢，有一丢丢害羞啦，只能先这样表白啦，回学校我一定给你补上玫瑰，再当面亲口对你说啦。嗯，遇上小可爱你呀，真是三生有幸，有句话叫做“一切的错过，都是为了更好的相遇”，我觉得缘分使我们相遇，就是上天对我们最好的安排（是你自己选的我的毕设任务吗？）初识时呢，是12月22日，还记得吗？那一天聊了之后就是一月份了再聊的了呢。</p>\n<p>后来在例会上听见了小可爱的声音，就很突然，感觉，哇，这声音真好听，可以说我是因为声音开始对小可爱有一些想法的，后来3月份稍加频繁联系了些，四月份我主动加了小可爱的QQ，想着能多了解一下呢，四月底五月初开始了频繁的聊天。5月4号开始呢，每天有意无意的找你聊天，维持小火花不灭，慢慢的发现和小可爱聊天真快乐，一发不可收拾，后来关注了我的知乎，我第一时间就去把小可爱的知乎瞅了个遍，发现还有点蠢萌蠢萌的呢，在相处中呢，我有发现小可爱似乎对我也有点意思，只是不太清楚到了哪种程度，第一次听到小可爱把我安利的东西全都收藏了起来，超感动的，完全超出了我的预料诶，超级开心啦，后面还发现小可爱有点小机灵鬼，就很有趣，性格也很好，好期待见面的呢，小可爱说请我吃饭的时候，我有一丢丢紧张，就是期待又紧张，因为如果这次见面搞砸了的话，会对后续有超级大的影响啦，有点害怕，不过还是鼓起勇气去啦，见了面发现真棒，是我喜欢的类型，就喜欢小可爱这种留着刘海，笑容迷人，眼睛也漂亮的可可爱爱的女孩子，嗯，168也还是可爱的，真是完美身高，身材看起来也不错诶（害羞），至于小可爱昨晚上说的哪些都没有很care的点啦。</p>\n<p>嗯，虽然我们现在对对方还没有非常了解，不过我觉着我们也可以在一起后再慢慢了解啦（没错，我忍不住啦），毕竟没有一对情侣是完全了解对方后再决定是否在一起的啦，所以嘞，小可爱，我喜欢你，超级喜欢你，可以和我交往吗？我想用余生慢慢了解你，照顾你，宠着你呀。</p>\n<p>嗯，我超级宠女朋友的哟。</p>\n<p>我超级羡慕盖茨比对爱情的梦想和追求，盖茨比想纠正错误，回到从前，他后悔了，而我不想留下遗憾，所以你愿意跟我一起吗？</p>\n<p>​                                                                                                                                                            饶志双  </p>\n<p>​                                                                                                                                                           2020.6.16</p>\n</blockquote>\n<p>小可爱给我的回信，心心念念的，半夜两点多突然醒了，然后悄悄看了（含羞啦）：</p>\n<blockquote>\n<p>​    我愿意和你在一起，做你的女朋友</p>\n<p>​    我喜欢和你聊天，我能从中获得快乐，感受到幸福</p>\n<p>​    我喜欢和你听音乐，你喜欢的歌我也很喜欢</p>\n<p>​    我喜欢看你用Github插件(我没记错的话)做的个人主页，我说的崇拜是认真的</p>\n<p>​    我喜欢你在机场把背包放到我身上的右手，让我觉得很温暖</p>\n<p>​    我喜欢和你在机场的最后一个再见，我从安检的地方伸出头时你还在</p>\n<p>​    对于爱情我渴望又畏惧</p>\n<p>​    我谈过恋爱，也没谈过恋爱</p>\n<p>​    除了爸爸和哥哥，我没有真正的在肢体上亲近过任何一个男生</p>\n<p>​    对于谈恋爱，我内心是忐忑的</p>\n<p>​    我希望自己成为你的小可爱，但又对这个身份有着些许陌生</p>\n<p>​    我其实是个小孩子，在爱的包围下长大，内心渴望被保护</p>\n<p>​    我习惯了爸爸的疼爱和哥哥的关心，但是还不太习惯男朋友的存在</p>\n<p>​    我说不出肉麻的情话</p>\n<p>​    我希望你可以慢慢靠近我，我担心自己会因为害怕而逃跑</p>\n<p>​    唉，我22了，听起来很矫情哈，你就当我还是小孩子吧</p>\n<p>​    偷偷告诉你</p>\n<p>​    和你的聊天记录我看了好多遍了</p>\n<p>​    刚刚在楼下一个人鬼鬼祟祟的给你打电话，像在做啥见不得人的事</p>\n<p>​    我会尽快适应我的新身份</p>\n<p>​    虽然表达上有点障碍，但是喜欢你是认真的</p>\n<p>​    还有啊，我的毕设题目确实是我自己选的，这确实是缘分噢</p>\n<p>​    我虽然不缺爱，但是缺乏安全感</p>\n<p>​    你要好好对我啊</p>\n<p>​                                                                                                                                                                毛慧慧</p>\n<p>​                                                                                                                                                               2020.6.17</p>\n</blockquote>\n<p>今天暂时到这儿啦～</p>\n<p>6.18 小可爱 属虎 超凶的那种（感觉明明属猪嘛，最幸福的品种，毕竟有我宠O(∩_∩)O哈哈哈~）</p>\n<p>6.18 小可爱要是真秃了，我一定给小可爱买假发(要不要考虑买点枸杞呢)</p>\n<ul>\n<li><p>[x] 6.18 想看小可爱戴帽子的样子</p>\n</li>\n<li><p>[ ] 6.18 小可爱什么时候给我拍新疆的日出呀==</p>\n</li>\n</ul>\n<p>6.18 喜欢听小可爱喘的声音（有点变态的样子，但真的好听）</p>\n<p>6.18 小可爱生气了 会不想说话</p>\n<p>6.18 小可爱戏精真好玩</p>\n<p>6.18 小可爱说以后领钱了就要投喂我呢，真幸福，让我来做家庭主男吧，嘻嘻</p>\n<p>6.18 现在要好好饲养小可爱，那么可爱，多招人喜欢呀，还省钱（（））</p>\n<ul>\n<li>[ ] 给小可爱穿自己的衣服</li>\n<li>[ ] 小可爱7.21生日策划</li>\n</ul>\n<p>6.18 今天和小可爱打了好久的电话呀，特别想跟小可爱聊天</p>\n<ul>\n<li>[ ] 给小可爱准备愿望卡（一个月一张，不能过分哦，比如女装什么的）</li>\n</ul>\n<p><strong>6.18 一直喜欢慧慧小可爱</strong></p>\n<p><strong>6.18 一直喜欢慧慧小可爱</strong></p>\n<p><strong>6.18 一直喜欢慧慧小可爱</strong></p>\n<p>6.19 小可爱不怎么吃零食，怕长胖，以后我的零食给她吃两成就行了</p>\n<ul>\n<li><p>[ ] 红烧排骨</p>\n</li>\n<li><p>[ ] ‌衣服 双双的小可爱 慧慧的大保镖</p>\n</li>\n</ul>\n<p>6.19 小可爱骗我妈妈要看我，太吓人了</p>\n<p>6.19 小可爱给我看了戴帽子的样子，嘻嘻</p>\n<p>6.19 今天和小可爱打了一整天电话呢，晚上连着打了四个多小时电话 嘿嘿嘿</p>\n<ul>\n<li>[ ] 和小可爱一起拍好看的情侣照</li>\n</ul>\n<p>6.19 小可爱喜欢哭鼻子，要好好宠着呢</p>\n<p>6.19 小可爱虽然做的不对，可是小可爱不渣哦，有清清楚楚明明白白说清楚就做的很棒啦</p>\n<p>6.20 小可爱想公主抱，害羞 52公斤</p>\n<ul>\n<li>[x] 项链，对戒</li>\n</ul>\n<p>6.20 小可爱控制不了自己啦，沉迷恋爱，无心学习</p>\n<ul>\n<li>[ ] 想枕着膝枕，听小可爱唱情歌，嘿嘿嘿</li>\n</ul>\n<p>6.20 小可爱可能只适合一两面</p>\n<p>6.21 永远不凶毛慧慧小可爱</p>\n<ul>\n<li>[ ] 带小慧慧去看辛夷花（3月，4月）</li>\n<li>[ ] 带小慧慧去看电影</li>\n</ul>\n<p>6.21 忙的时候要提前告诉小可爱(๑• . •๑)</p>\n<p>6.21 看到小可爱的消息要及时回，忙不开也要说一声</p>\n<p>6.21 小可爱呀，我好喜欢你呀，特别特别喜欢你，希望余生，有你一起陪我</p>\n<p>6.21 包吃包住哦，小可爱快过来呀，超期待的</p>\n<p>6.21 小慧慧小可爱记得养我呀</p>\n<p>6.21 小可爱哭了，以后可要等我在身边再哭呀</p>\n<ul>\n<li>[ ] 见面的时候给小可爱一个大大的抱抱</li>\n</ul>\n<p>6.21 小可爱，我不会再爱上其他任何人了，以后慧慧小可爱是我的，我是慧慧小可爱的</p>\n<p>6.22 慧慧小可爱怕冷，冬天的时候要用脖子帮慧慧小可爱暖手</p>\n<p>6.22 慧慧小可爱讨厌冷暴力，不可以生气太久，要及时找慧慧小可爱说话</p>\n<p>6.22 要把慧慧小可爱的指纹录入手机</p>\n<p>6.22 手机的主题壁纸要和慧慧小可爱一样，开学见面的时候慧慧小可爱要看</p>\n<p>6.22 慧慧小可爱生气的时候要哄，要多和慧慧小可爱说话</p>\n<p>6.22 不可以说对慧慧小可爱没感觉，慧慧小可爱会伤心的</p>\n<p>6.23 起来工作啦</p>\n<p>6.23 小可爱越来越流氓了（哈哈哈）</p>\n<p>6.23 选好了戒指和项链</p>\n<p>6.24 小可爱腿腿看起来蛮修长的诶</p>\n<p>6.24 今天一天都在和小可爱打电话，真的打了一天电话哦</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090518.png\" style=\"zoom: 25%;\" /></p>\n<p>6.25 小可爱给爸爸妈妈说了我，超开心啦，还晚上偷偷表白，我都有记在心里，真好，世界上最幸福的事就是我喜欢的人刚好喜欢我，比这更幸福的是我喜欢的人比我想象的更喜欢我，爱你噢，小慧慧</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/QQ图片20200627090859.png\" style=\"zoom: 25%;\" /></p>\n<p>6.25 今天和室友聚餐了，说起来都在成都，但真的五个人在一起聚都快一年没有了，晚上小可爱吃的烤羊肉串很好吃的样子，晚上十点多了到家，小可爱还出去顶着被蚊子叮咬和我打电话，就是个小傻蛋（给小可爱亲亲安慰）</p>\n<p>6.25 超级无敌可爱温柔爱我比爱自己多的慧慧小可爱，人又乖，笑起来又好看，声音又好听，我最喜欢了，小可爱别惩罚我了（今天打牌没有及时回小可爱消息，小可爱可难过了）</p>\n<p>6.26 小可爱小小的愿望：一个甜甜的有惊喜的初吻</p>\n<p>6.26 小可爱就是个小傻蛋，做一些奇奇怪怪的梦，哼，都说梦是反的，我要一直一直和小可爱在一起。</p>\n<p>6.26 戒指到了，戴起来真好看，戴着就感觉承担起了一个家庭的责任（有点怪怪的样子，第一次戴诶）</p>\n<p>6.26 慧慧大傻蛋，说不给我写，我可伤心了，就不想理小可爱了，虽然知道是开玩笑的，可还是会伤心，就还是会难过，我也是一个小公主呢，哼。</p>\n<p>6.26 小可爱伤心难过，我也会伤心难过，小可爱哭，我就会手足无措，我不想小可爱再伤心难过了，小可爱，我想你了，想见到你，超级超级想，超级超级超级想，看见室友和他女朋友，就很羡慕，好想你也在我身边呀，你伤心难过的时候我可以抱抱你，你哭的时候我可以陪着你，可以擦去你眼角的泪水，而不是像现在这样什么也做不了，真的好想你呀。</p>\n<p>6.27 对待小可爱我是认真的，超级认真的，小可爱说缺少安全感，所以我可以让小可爱看我的ＱＱ和电脑，我说想和小可爱一直走下去也是认真的，或许会遇到比小可爱更优秀的女生，但那不是我的，我认定了小可爱，小可爱就是我的唯一，我是个很专一的人，你若不离不弃，我便陪你到老，这不是我说说而已，我就是这么固执的一个人，有了你，我就会屏蔽掉其他人，保持应有的距离，小可爱，余生很长，我们慢慢变老，和小可爱在一起我才发现，我可以这么喜欢一个人，就很幸福，头一次发现原来我也会这么粘人。</p>\n<ul>\n<li>[ ] 6.27 小可爱给我煮粥、煎饼</li>\n</ul>\n<p>6.27 小可爱大懒虫，睡了一下午</p>\n<p>6.28 喜欢一直一直和小可爱在一起，做小可爱的避湾港</p>\n<p>6.29 陪小可爱看《十日游戏》</p>\n<p>6.30 超喜欢小可爱唱歌，期待小可爱的《喜欢你》</p>\n<p>7.9 宝宝下雨时会难过，接送宝宝</p>\n<p>7.9 宝宝在身边的时候告诉答应告诉宝宝的事</p>\n<p>7.20宝宝要亲亲抱抱我（（））</p>\n<p>7.20宝宝QQ换了和我一样的字体气泡，宝宝想我亲脚脚</p>\n<p>7.20和宝宝5200分了，纪念一下</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722144821.jpg\" style=\"zoom:25%;\" /></p>\n<p>7.21小宝宝生日，没有我陪着太可怜了，臭宝宝呀，好想你哦，小宝宝真的喜欢粉色的哇，宝宝呀，以后过生在我身边过吧，我来宠着宝宝。宝宝发朋友圈了，开心</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200722142949.jpg\" style=\"zoom: 33%;\" /></p>\n<p>7.22宝宝什么时候看《死神来了》勒，宝宝要越来越忙了，难受==</p>\n<p>7.22想宝宝的第N天，宝宝喜欢看我的博客，肯定是喜欢被在乎的感觉，宝宝呀，即使没更，还是很在乎宝宝的哦</p>\n<ul>\n<li>[ ] 7.22等着验证臭宝宝会不会睁着眼睡觉</li>\n</ul>\n<p>7.22和宝宝的聊天记录好多好多哦</p>\n<p>7.24宝宝脖子疼，多休息休息啦</p>\n<p>7.24宝宝说过来了就照顾我，我可记下来了</p>\n<p>7.24宝宝给我做好吃的</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200724191906.jpg\" style=\"zoom: 33%;\" /></p>\n<p>7.25宝宝情感需求大，就像一个刚出生的小宝宝，需要细心呵护</p>\n<p>7.25宝宝说两个人走到最后靠的是“相信”、“依赖”，我觉得两个人走到最后靠的是“包容”和“沟通”，包容缺点也好，包容性格也好，包容就像粘合剂，能让两个人贴的更近，沟通是一种方法，能有效传达双方情感心意的方法，也是有效解决双方矛盾误解方法，深层次的沟通能让我们更加了解彼此，更加懂得对方。相信和依赖都是一种状态，一种情感上的交流，代表着我百分之百的心意，全身心的投入。</p>\n<p>7.25我希望宝宝有什么都能和我讲，我喜欢共享宝宝的所有情绪</p>\n<p>7.25我以后可能是个醋王，连宝宝弟弟的醋都吃（（））</p>\n<p>7.25深深演唱会还行，喜欢《随风》、《不想睡》，晚安，明天见</p>\n<p>7.27臭宝宝，问了四次爱我不，都说不爱，可难过了，昨晚上失眠了，睡眠质量很差，精神不咋好，本来是想找宝宝充电的，哼。</p>\n<p>7.27臭宝宝，认错态度良好，还很积极，我真的很开心，我挺害怕把宝宝宠坏，害怕宝宝持宠而娇，那样会非常难受的，因为我也挺喜欢宝宝宠我的，看到宝宝认错发的消息，眼泪哗哗的就掉下来了，真的好喜欢宝宝呀。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200728100236.jpg\" style=\"zoom: 25%;\" /></p>\n<p>7.27宝宝要记得回家了手写哦，要给我看（（））爱你，宝宝</p>\n<p>7.28今天去买虾回来做</p>\n<p>8.8宝宝哭的很厉害，手脚发麻，一直抽泣不能控制，对不起宝宝，让宝宝遭受了这么大痛苦，宝宝真的吓到我了。</p>\n<p>8.9小宝宝情感比我细腻很多很多，情绪积累很快，宝宝真的真的好爱我呀，我的一些举动对宝宝的影响很大，宝宝在我身上寄托了很深的情感，深到我会不经意间让宝宝伤心难过</p>\n<p>8.9宝宝，我们好相似呀，对对方需求大，生气了不想说话，不开心了想对方找自己，哄自己。</p>\n<p>8.9宝宝不开心了要记得把宝宝哄开心，不能自己也不开心了就不管宝宝，把宝宝丢下，对不起宝宝，我知道错啦，以后多主动找找宝宝，宝宝也需要多多的宠爱，爱你 ，宝宝</p>\n<p>8.9宝宝开心的样子真的很好看，宝宝开心我也开心，宝宝穿漏肩长裙真的又性感又漂亮，以后要多穿穿</p>\n<p>8.9早上神智不清晰，最容易受宝宝撩拨了，宝宝可得负责，我现在可想亲宝宝肩了，既想靠在肩上又想亲亲的咬它，我不纯洁了，宝宝（雾）</p>\n<p>8.10早起，体检，去学校，打扫寝室，羞羞</p>\n<p>8.11爱宝宝，也爱学习</p>\n"},{"title":"潜在语义分析","top":false,"cover":false,"toc":true,"mathjax":false,"reprintPolicy":"cc_by","date":"2020-07-07T07:56:33.000Z","author":null,"password":null,"img":null,"summary":null,"keywords":"潜在语义分析 LSA","_content":"\n潜在语义分析（latent semantic analysis，LSA）是一种无监督学习方法，是非概率的话题分析模型，其特点是通过矩阵分解发现文本与单词之间的基于话题的与语义关系，具体地，将文本集合表示为单词-文本矩阵，对单词-文本矩阵进行奇异值分解（singular value decomposition，SVD），从而得到话题向量空间以及文本在话题向量空间中的表示。\n\n### 单词向量空间\n\n对文本进行语义相似度计算，最简单的方法是利用向量空间模型（vector space model，VSM），也就是单词向量空间模型（word vector space model）。其前提假设是**文本中所有单词的出现情况表示了文本的语义内容**，其基本想法是，给定一份文本，用向量表示该文本的语义，向量的每一维度对应一个单词，其数值为该单词在该文本中出现的频数或权值。文本集合中每个文本都表示为一个向量，存在于一个向量空间；向量空间的度量，如內积或标准化內积表示文本之间的语义相似度。\n\n#### 数学定义\n\n给定一个含有$n$分文本的集合$D={d_1,d_2,\\dots,d_n}$，以及在所有文本中出现的$m$个单词的集合$W={w_1,w_2,\\dots,w_m}$。将单词在文本中出现的数据用一个单词-文本矩阵表示，记作$X$\n$$\nX = \\left[\\begin{matrix}\n  x_{11} & x_{12} & \\dots & x_{1n}  \\\\\n  x_{21} & x_{22} & \\dots & x_{2n}  \\\\ \n  \\vdots & \\vdots &       & \\vdots& \\\\ \n  x_{m1} & x_{m2} & … & x_{nn}\n\\end{matrix}\\right]\n$$\n这是一个$m*n$的矩阵，元素$x_{ij}$表示单词$w_i$在文本$d_j$中出现的频数或权值，权值通常由TF-IDF表示。矩阵中每一列是一个文本向量，直观上，**在两个文本中共同出现的单词越多，其语义内容越相近**，这时对应的向量同不为零的维度越多，內积越大，表示两个文本在语义上越相似\n\n\n\n","source":"_posts/潜在语义分析.md","raw":"---\ntitle: 潜在语义分析\ntop: false\ncover: false\ntoc: true\nmathjax: false\nreprintPolicy: cc_by\ndate: 2020-07-07 15:56:33\nauthor:\npassword:\nimg:\nsummary: \ncategories: 机器学习\nkeywords: 潜在语义分析 LSA\ntags:\n\t- 潜在语义分析\n\t- LSA\n---\n\n潜在语义分析（latent semantic analysis，LSA）是一种无监督学习方法，是非概率的话题分析模型，其特点是通过矩阵分解发现文本与单词之间的基于话题的与语义关系，具体地，将文本集合表示为单词-文本矩阵，对单词-文本矩阵进行奇异值分解（singular value decomposition，SVD），从而得到话题向量空间以及文本在话题向量空间中的表示。\n\n### 单词向量空间\n\n对文本进行语义相似度计算，最简单的方法是利用向量空间模型（vector space model，VSM），也就是单词向量空间模型（word vector space model）。其前提假设是**文本中所有单词的出现情况表示了文本的语义内容**，其基本想法是，给定一份文本，用向量表示该文本的语义，向量的每一维度对应一个单词，其数值为该单词在该文本中出现的频数或权值。文本集合中每个文本都表示为一个向量，存在于一个向量空间；向量空间的度量，如內积或标准化內积表示文本之间的语义相似度。\n\n#### 数学定义\n\n给定一个含有$n$分文本的集合$D={d_1,d_2,\\dots,d_n}$，以及在所有文本中出现的$m$个单词的集合$W={w_1,w_2,\\dots,w_m}$。将单词在文本中出现的数据用一个单词-文本矩阵表示，记作$X$\n$$\nX = \\left[\\begin{matrix}\n  x_{11} & x_{12} & \\dots & x_{1n}  \\\\\n  x_{21} & x_{22} & \\dots & x_{2n}  \\\\ \n  \\vdots & \\vdots &       & \\vdots& \\\\ \n  x_{m1} & x_{m2} & … & x_{nn}\n\\end{matrix}\\right]\n$$\n这是一个$m*n$的矩阵，元素$x_{ij}$表示单词$w_i$在文本$d_j$中出现的频数或权值，权值通常由TF-IDF表示。矩阵中每一列是一个文本向量，直观上，**在两个文本中共同出现的单词越多，其语义内容越相近**，这时对应的向量同不为零的维度越多，內积越大，表示两个文本在语义上越相似\n\n\n\n","slug":"潜在语义分析","published":1,"updated":"2020-08-20T08:41:44.653Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mb30016ewse3dm9cqym","content":"<p>潜在语义分析（latent semantic analysis，LSA）是一种无监督学习方法，是非概率的话题分析模型，其特点是通过矩阵分解发现文本与单词之间的基于话题的与语义关系，具体地，将文本集合表示为单词-文本矩阵，对单词-文本矩阵进行奇异值分解（singular value decomposition，SVD），从而得到话题向量空间以及文本在话题向量空间中的表示。</p>\n<h3 id=\"单词向量空间\"><a href=\"#单词向量空间\" class=\"headerlink\" title=\"单词向量空间\"></a>单词向量空间</h3><p>对文本进行语义相似度计算，最简单的方法是利用向量空间模型（vector space model，VSM），也就是单词向量空间模型（word vector space model）。其前提假设是<strong>文本中所有单词的出现情况表示了文本的语义内容</strong>，其基本想法是，给定一份文本，用向量表示该文本的语义，向量的每一维度对应一个单词，其数值为该单词在该文本中出现的频数或权值。文本集合中每个文本都表示为一个向量，存在于一个向量空间；向量空间的度量，如內积或标准化內积表示文本之间的语义相似度。</p>\n<h4 id=\"数学定义\"><a href=\"#数学定义\" class=\"headerlink\" title=\"数学定义\"></a>数学定义</h4><p>给定一个含有$n$分文本的集合$D={d_1,d_2,\\dots,d_n}$，以及在所有文本中出现的$m$个单词的集合$W={w_1,w_2,\\dots,w_m}$。将单词在文本中出现的数据用一个单词-文本矩阵表示，记作$X$</p>\n<script type=\"math/tex; mode=display\">\nX = \\left[\\begin{matrix}\n  x_{11} & x_{12} & \\dots & x_{1n}  \\\\\n  x_{21} & x_{22} & \\dots & x_{2n}  \\\\ \n  \\vdots & \\vdots &       & \\vdots& \\\\ \n  x_{m1} & x_{m2} & … & x_{nn}\n\\end{matrix}\\right]</script><p>这是一个$m<em>n$的矩阵，元素$x_{ij}$表示单词$w_i$在文本$d_j$中出现的频数或权值，权值通常由TF-IDF表示。矩阵中每一列是一个文本向量，直观上，<em>*在两个文本中共同出现的单词越多，其语义内容越相近</em></em>，这时对应的向量同不为零的维度越多，內积越大，表示两个文本在语义上越相似</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>潜在语义分析（latent semantic analysis，LSA）是一种无监督学习方法，是非概率的话题分析模型，其特点是通过矩阵分解发现文本与单词之间的基于话题的与语义关系，具体地，将文本集合表示为单词-文本矩阵，对单词-文本矩阵进行奇异值分解（singular value decomposition，SVD），从而得到话题向量空间以及文本在话题向量空间中的表示。</p>\n<h3 id=\"单词向量空间\"><a href=\"#单词向量空间\" class=\"headerlink\" title=\"单词向量空间\"></a>单词向量空间</h3><p>对文本进行语义相似度计算，最简单的方法是利用向量空间模型（vector space model，VSM），也就是单词向量空间模型（word vector space model）。其前提假设是<strong>文本中所有单词的出现情况表示了文本的语义内容</strong>，其基本想法是，给定一份文本，用向量表示该文本的语义，向量的每一维度对应一个单词，其数值为该单词在该文本中出现的频数或权值。文本集合中每个文本都表示为一个向量，存在于一个向量空间；向量空间的度量，如內积或标准化內积表示文本之间的语义相似度。</p>\n<h4 id=\"数学定义\"><a href=\"#数学定义\" class=\"headerlink\" title=\"数学定义\"></a>数学定义</h4><p>给定一个含有$n$分文本的集合$D={d_1,d_2,\\dots,d_n}$，以及在所有文本中出现的$m$个单词的集合$W={w_1,w_2,\\dots,w_m}$。将单词在文本中出现的数据用一个单词-文本矩阵表示，记作$X$</p>\n<script type=\"math/tex; mode=display\">\nX = \\left[\\begin{matrix}\n  x_{11} & x_{12} & \\dots & x_{1n}  \\\\\n  x_{21} & x_{22} & \\dots & x_{2n}  \\\\ \n  \\vdots & \\vdots &       & \\vdots& \\\\ \n  x_{m1} & x_{m2} & … & x_{nn}\n\\end{matrix}\\right]</script><p>这是一个$m<em>n$的矩阵，元素$x_{ij}$表示单词$w_i$在文本$d_j$中出现的频数或权值，权值通常由TF-IDF表示。矩阵中每一列是一个文本向量，直观上，<em>*在两个文本中共同出现的单词越多，其语义内容越相近</em></em>，这时对应的向量同不为零的维度越多，內积越大，表示两个文本在语义上越相似</p>\n"},{"title":"短文本分类","top":false,"cover":false,"toc":true,"mathjax":false,"reprintPolicy":"cc_by","date":"2020-06-18T07:59:07.000Z","author":null,"password":null,"img":null,"summary":"短文本分类综述","keywords":"短文本 分类 综述","_content":"\n### 大纲目录\n\n* 短文本分类的难点和挑战\n  * 和长文本相比，有什么不同，特殊在哪里，长文本的分类方法为什么不能适用于短文本分类\n* 短文本分类包含哪些应用领域\n  * 评论数据等的情感分析\n  * 聊天记录分析\n  * 问答系统\n  * 搜索\n  * 其他\n* 短文本分类方法划分\n  * 按照流程划分\n    * 短文本分类流程是怎样的，与长文本分类流程有什么不同\n    * 特征抽取文本表示分类\n  * 标准划分\n    * 基于机器学习\n    * 基于深度学习\n    * 融合\n  * 是否引入外部知识源\n    * 无外部知识源\n    * 有外部知识源\n* 相关数据集整理，给出评价指标\n* 各个数据集上实验结果对比分析\n* 总结，分析未来发展趋势\n\n### 引言\n\n短文本分类是自然语言处理领域的重要任务，包括情感分析、问答、对话管理等。随着Internet的大规模普及和用户数量的进一步增加,互联网上的各种短文本正在爆炸式地增长。短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.\n\n本文根据所需知识源的属性,将短文本理解模型分为3类:\n\n* 隐性(implicit)语义模型\n* 半显性(semi-explicit)语义模型\n* 显性(explicit)语义模型\n\n其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本分类.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本分类.\n\n从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度\n\n### 文本分类方法\n\n#### 隐性(implicit)语义模型\n\n隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.\n\n- LSA(latent semantic analysis)^[3]^\n\nLSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.\n\n* 超空间模拟语言模型^[4]^\n\n超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止\n词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.\n\n* 神经网络语言模型\n\n近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不\n同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).\n\n最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使\n用目标词预测窗口中的每个语境词出现的概率.\n\n总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.\n\n* 段向量\n\n段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.\n\n![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%25E5%259B%25BE%25E7%2589%258720200630104534.png)\n\n#### 半显性(semi-xpIicit)语义模型\n\n半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic\nmodels).\n\nLSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏\n参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:\n\n$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$\n\n虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).\n\n![QQ截图20200701115340](/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200701115340.png)\n\n总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.\n\n#### 显性(explicit)语义模型\n\n近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常\n是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.\n\n1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每\n个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.\n\n![QQ截图20200702142120](/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200702142120.png)\n\n2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad\"这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad\").为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而\n计算$p(concept |word ,topic)$.\n\n### 模型粒度分析\n\n本节将深入讨论第1节的短文本理解模型在文本分析粒度上的差异,并从应用层面论证不同方法的适用性.\n\n#### 文本粒度模型\n\n首先,文本粒度的模型包含LSA,LDA和PV.这些模型均尝试直接推导出短文本的向量表示作为模型的输出.在LSA中,通过构建一个词与文本的共现矩阵,每个文本可用以词为维度的向量表示.类似地,LDA试图模拟文本的生成过程.作为结果,可得到每个文本的主题分布.PV通过神经网络推测(inference)的方式获取文本向量的最优参数.上述模型所得的文本向量均可以直接用于与这些文本相关的任务,如文本分类^[17-18]^、聚类^[19]^、摘要生成^[20]^。值得注意的是,LSA同时输出词向量.因而在短文本数量不足的情况下,可以先采用基于大量完整文本的LSA获取词向量,再通过额外的合成方法获取短文本向量.对于LDA和PV而言,其模型亦可以通过额外的文本训练,然后应用于短文本.\n\n#### 2.2词粒度模型\n\n同LSA,LDA和PV相比,其他模型(LSA,NLM,ESA等)均属于词粒度的模型.这是由于这些模型的产出仅为词向量.针对短文本理解这一任务,必须使用额外的合成手段来推出短文本的表示.例如,在文献[21-25]工作中,作者均利用词向量推导出文本表示,并用于后续的文本相似度判断、文本复述、情感分析等任务.这里的一个特例为概念化模型.由于概念化可以直接基于语境推出短文本中每个词的概念,这样的输出方式已经可以满足机器短文本理解的需求.因而概念化虽属于词粒度的模型但并不需要额外的文本合成.\n\n### 2.3文本合成\n\n如何通过词向量获取任意长度的文本向量(包括短文本)是时下流行的一个研究领域.根据复杂度的不同,文本合成方法可被大致分为代数向量模型^[5,21-23,25]^、张量模型^[26-28]^和神经网络模型^[7,24,29-32]^.\n\n1)代数运算模型.最早的合成模型由Mitchell和Lapata^[21]^提出.其模型使用逐点的(point-wise)向量相加的方式从词向量推出文本向量.虽然这一基于“词袋”的方法忽略了句子中的词序(“cat eats fish”和“fish eats cat”将有相同的表示),事实表明其在很多自然语言处理任务上有着不错的效果,且其常常被用作复杂模型的基准^[23]^,类似的代数运算\n模型还有逐点的向量乘积^[5,21-22]^以及乘法与加法的结合运算^[24]^.\n\n2)张量模型.张量模型^[26-27]^为代数运算模型的延伸.其试图强调不同词性的词在语义合成中的不同角色.例如在“red car”这个词组中,形容词“red”对名词“car”起修饰作用;而在“eat apple”中,动词“eat”的角色好比作用于“apple”的函数.从这个角度而言,将不同词性的词均表示为同等维度的向量过于简化.因而,在张量模型中,不同词性的词被表示为\n不同维度的张量,整个句子的表示方式以张量乘法的形式获取.目前,张量模型的最大挑战是如何获取向量与张量的映射关系^[28]^.\n\n3)神经网络模型.时下最为流行的文本合成模型为基于神经网络的模型,如recursive neural network(RecNN)^[29-30]^,recurrent neural network(RNN)^[5]^,convolutional neural\n network(CNN)^[31-32]^等.在这些模型中,最基本的合成单元为神经网络.通常的形式为神经网络根据输入向量工**x~1~**，**x~2~**:推出其组合向量**y**。\n\n在具体的文本合成中,不同的神经网络模型的构造不同.例如,RecNN依赖于语法树开展逐层的语义合成,它无法被用于短文本.相比之下,RNN和CNN都可以快速通过词向量推导出短文本向量.\n\n\n\n### 文献引用\n\n[1] Linmei H, Yang T, Shi C, et al. Heterogeneous graph attention networks for semi-supervised short text classification[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 4823-4832.\n\n[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.\n\n[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.\n\n[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.\n\n[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.\n\n[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n\n[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.\n\n[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n\n[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.\n\n[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.\n\n[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e\n\n[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.\n\n[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.","source":"_posts/短文本分类.md","raw":"---\ntitle: 短文本分类\ntop: false\ncover: false\ntoc: true\nmathjax: false\nreprintPolicy: cc_by\ndate: 2020-06-18 15:59:07\nauthor:\npassword:\nimg:\nsummary: 短文本分类综述\ncategories: 短文本\nkeywords: 短文本 分类 综述\ntags:\n\t- 短文本\n\t- 分类\n\t- 综述\n---\n\n### 大纲目录\n\n* 短文本分类的难点和挑战\n  * 和长文本相比，有什么不同，特殊在哪里，长文本的分类方法为什么不能适用于短文本分类\n* 短文本分类包含哪些应用领域\n  * 评论数据等的情感分析\n  * 聊天记录分析\n  * 问答系统\n  * 搜索\n  * 其他\n* 短文本分类方法划分\n  * 按照流程划分\n    * 短文本分类流程是怎样的，与长文本分类流程有什么不同\n    * 特征抽取文本表示分类\n  * 标准划分\n    * 基于机器学习\n    * 基于深度学习\n    * 融合\n  * 是否引入外部知识源\n    * 无外部知识源\n    * 有外部知识源\n* 相关数据集整理，给出评价指标\n* 各个数据集上实验结果对比分析\n* 总结，分析未来发展趋势\n\n### 引言\n\n短文本分类是自然语言处理领域的重要任务，包括情感分析、问答、对话管理等。随着Internet的大规模普及和用户数量的进一步增加,互联网上的各种短文本正在爆炸式地增长。短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.\n\n本文根据所需知识源的属性,将短文本理解模型分为3类:\n\n* 隐性(implicit)语义模型\n* 半显性(semi-explicit)语义模型\n* 显性(explicit)语义模型\n\n其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本分类.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本分类.\n\n从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度\n\n### 文本分类方法\n\n#### 隐性(implicit)语义模型\n\n隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.\n\n- LSA(latent semantic analysis)^[3]^\n\nLSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.\n\n* 超空间模拟语言模型^[4]^\n\n超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止\n词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.\n\n* 神经网络语言模型\n\n近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不\n同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).\n\n最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使\n用目标词预测窗口中的每个语境词出现的概率.\n\n总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.\n\n* 段向量\n\n段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.\n\n![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%25E5%259B%25BE%25E7%2589%258720200630104534.png)\n\n#### 半显性(semi-xpIicit)语义模型\n\n半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic\nmodels).\n\nLSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏\n参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:\n\n$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$\n\n虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).\n\n![QQ截图20200701115340](/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200701115340.png)\n\n总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.\n\n#### 显性(explicit)语义模型\n\n近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常\n是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.\n\n1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每\n个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.\n\n![QQ截图20200702142120](/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200702142120.png)\n\n2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad\"这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad\").为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而\n计算$p(concept |word ,topic)$.\n\n### 模型粒度分析\n\n本节将深入讨论第1节的短文本理解模型在文本分析粒度上的差异,并从应用层面论证不同方法的适用性.\n\n#### 文本粒度模型\n\n首先,文本粒度的模型包含LSA,LDA和PV.这些模型均尝试直接推导出短文本的向量表示作为模型的输出.在LSA中,通过构建一个词与文本的共现矩阵,每个文本可用以词为维度的向量表示.类似地,LDA试图模拟文本的生成过程.作为结果,可得到每个文本的主题分布.PV通过神经网络推测(inference)的方式获取文本向量的最优参数.上述模型所得的文本向量均可以直接用于与这些文本相关的任务,如文本分类^[17-18]^、聚类^[19]^、摘要生成^[20]^。值得注意的是,LSA同时输出词向量.因而在短文本数量不足的情况下,可以先采用基于大量完整文本的LSA获取词向量,再通过额外的合成方法获取短文本向量.对于LDA和PV而言,其模型亦可以通过额外的文本训练,然后应用于短文本.\n\n#### 2.2词粒度模型\n\n同LSA,LDA和PV相比,其他模型(LSA,NLM,ESA等)均属于词粒度的模型.这是由于这些模型的产出仅为词向量.针对短文本理解这一任务,必须使用额外的合成手段来推出短文本的表示.例如,在文献[21-25]工作中,作者均利用词向量推导出文本表示,并用于后续的文本相似度判断、文本复述、情感分析等任务.这里的一个特例为概念化模型.由于概念化可以直接基于语境推出短文本中每个词的概念,这样的输出方式已经可以满足机器短文本理解的需求.因而概念化虽属于词粒度的模型但并不需要额外的文本合成.\n\n### 2.3文本合成\n\n如何通过词向量获取任意长度的文本向量(包括短文本)是时下流行的一个研究领域.根据复杂度的不同,文本合成方法可被大致分为代数向量模型^[5,21-23,25]^、张量模型^[26-28]^和神经网络模型^[7,24,29-32]^.\n\n1)代数运算模型.最早的合成模型由Mitchell和Lapata^[21]^提出.其模型使用逐点的(point-wise)向量相加的方式从词向量推出文本向量.虽然这一基于“词袋”的方法忽略了句子中的词序(“cat eats fish”和“fish eats cat”将有相同的表示),事实表明其在很多自然语言处理任务上有着不错的效果,且其常常被用作复杂模型的基准^[23]^,类似的代数运算\n模型还有逐点的向量乘积^[5,21-22]^以及乘法与加法的结合运算^[24]^.\n\n2)张量模型.张量模型^[26-27]^为代数运算模型的延伸.其试图强调不同词性的词在语义合成中的不同角色.例如在“red car”这个词组中,形容词“red”对名词“car”起修饰作用;而在“eat apple”中,动词“eat”的角色好比作用于“apple”的函数.从这个角度而言,将不同词性的词均表示为同等维度的向量过于简化.因而,在张量模型中,不同词性的词被表示为\n不同维度的张量,整个句子的表示方式以张量乘法的形式获取.目前,张量模型的最大挑战是如何获取向量与张量的映射关系^[28]^.\n\n3)神经网络模型.时下最为流行的文本合成模型为基于神经网络的模型,如recursive neural network(RecNN)^[29-30]^,recurrent neural network(RNN)^[5]^,convolutional neural\n network(CNN)^[31-32]^等.在这些模型中,最基本的合成单元为神经网络.通常的形式为神经网络根据输入向量工**x~1~**，**x~2~**:推出其组合向量**y**。\n\n在具体的文本合成中,不同的神经网络模型的构造不同.例如,RecNN依赖于语法树开展逐层的语义合成,它无法被用于短文本.相比之下,RNN和CNN都可以快速通过词向量推导出短文本向量.\n\n\n\n### 文献引用\n\n[1] Linmei H, Yang T, Shi C, et al. Heterogeneous graph attention networks for semi-supervised short text classification[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 4823-4832.\n\n[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.\n\n[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.\n\n[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.\n\n[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.\n\n[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n\n[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.\n\n[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n\n[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.\n\n[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.\n\n[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e\n\n[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.\n\n[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.","slug":"短文本分类","published":1,"updated":"2020-08-20T08:41:44.654Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mba001aewse845dajsg","content":"<h3 id=\"大纲目录\"><a href=\"#大纲目录\" class=\"headerlink\" title=\"大纲目录\"></a>大纲目录</h3><ul>\n<li>短文本分类的难点和挑战<ul>\n<li>和长文本相比，有什么不同，特殊在哪里，长文本的分类方法为什么不能适用于短文本分类</li>\n</ul>\n</li>\n<li>短文本分类包含哪些应用领域<ul>\n<li>评论数据等的情感分析</li>\n<li>聊天记录分析</li>\n<li>问答系统</li>\n<li>搜索</li>\n<li>其他</li>\n</ul>\n</li>\n<li>短文本分类方法划分<ul>\n<li>按照流程划分<ul>\n<li>短文本分类流程是怎样的，与长文本分类流程有什么不同</li>\n<li>特征抽取文本表示分类</li>\n</ul>\n</li>\n<li>标准划分<ul>\n<li>基于机器学习</li>\n<li>基于深度学习</li>\n<li>融合</li>\n</ul>\n</li>\n<li>是否引入外部知识源<ul>\n<li>无外部知识源</li>\n<li>有外部知识源</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>相关数据集整理，给出评价指标</li>\n<li>各个数据集上实验结果对比分析</li>\n<li>总结，分析未来发展趋势</li>\n</ul>\n<h3 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h3><p>短文本分类是自然语言处理领域的重要任务，包括情感分析、问答、对话管理等。随着Internet的大规模普及和用户数量的进一步增加,互联网上的各种短文本正在爆炸式地增长。短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.</p>\n<p>本文根据所需知识源的属性,将短文本理解模型分为3类:</p>\n<ul>\n<li>隐性(implicit)语义模型</li>\n<li>半显性(semi-explicit)语义模型</li>\n<li>显性(explicit)语义模型</li>\n</ul>\n<p>其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本分类.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本分类.</p>\n<p>从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度</p>\n<h3 id=\"文本分类方法\"><a href=\"#文本分类方法\" class=\"headerlink\" title=\"文本分类方法\"></a>文本分类方法</h3><h4 id=\"隐性-implicit-语义模型\"><a href=\"#隐性-implicit-语义模型\" class=\"headerlink\" title=\"隐性(implicit)语义模型\"></a>隐性(implicit)语义模型</h4><p>隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.</p>\n<ul>\n<li>LSA(latent semantic analysis)^[3]^</li>\n</ul>\n<p>LSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.</p>\n<ul>\n<li>超空间模拟语言模型^[4]^</li>\n</ul>\n<p>超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止<br>词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.</p>\n<ul>\n<li>神经网络语言模型</li>\n</ul>\n<p>近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不<br>同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).</p>\n<p>最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使<br>用目标词预测窗口中的每个语境词出现的概率.</p>\n<p>总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.</p>\n<ul>\n<li>段向量</li>\n</ul>\n<p>段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%25E5%259B%25BE%25E7%2589%258720200630104534.png\" alt=\"QQ图片20200630104534\"></p>\n<h4 id=\"半显性-semi-xpIicit-语义模型\"><a href=\"#半显性-semi-xpIicit-语义模型\" class=\"headerlink\" title=\"半显性(semi-xpIicit)语义模型\"></a>半显性(semi-xpIicit)语义模型</h4><p>半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic<br>models).</p>\n<p>LSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏<br>参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:</p>\n<p>$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$</p>\n<p>虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200701115340.png\" alt=\"QQ截图20200701115340\"></p>\n<p>总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.</p>\n<h4 id=\"显性-explicit-语义模型\"><a href=\"#显性-explicit-语义模型\" class=\"headerlink\" title=\"显性(explicit)语义模型\"></a>显性(explicit)语义模型</h4><p>近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常<br>是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.</p>\n<p>1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每<br>个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200702142120.png\" alt=\"QQ截图20200702142120\"></p>\n<p>2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad”这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad”).为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而<br>计算$p(concept |word ,topic)$.</p>\n<h3 id=\"模型粒度分析\"><a href=\"#模型粒度分析\" class=\"headerlink\" title=\"模型粒度分析\"></a>模型粒度分析</h3><p>本节将深入讨论第1节的短文本理解模型在文本分析粒度上的差异,并从应用层面论证不同方法的适用性.</p>\n<h4 id=\"文本粒度模型\"><a href=\"#文本粒度模型\" class=\"headerlink\" title=\"文本粒度模型\"></a>文本粒度模型</h4><p>首先,文本粒度的模型包含LSA,LDA和PV.这些模型均尝试直接推导出短文本的向量表示作为模型的输出.在LSA中,通过构建一个词与文本的共现矩阵,每个文本可用以词为维度的向量表示.类似地,LDA试图模拟文本的生成过程.作为结果,可得到每个文本的主题分布.PV通过神经网络推测(inference)的方式获取文本向量的最优参数.上述模型所得的文本向量均可以直接用于与这些文本相关的任务,如文本分类^[17-18]^、聚类^[19]^、摘要生成^[20]^。值得注意的是,LSA同时输出词向量.因而在短文本数量不足的情况下,可以先采用基于大量完整文本的LSA获取词向量,再通过额外的合成方法获取短文本向量.对于LDA和PV而言,其模型亦可以通过额外的文本训练,然后应用于短文本.</p>\n<h4 id=\"2-2词粒度模型\"><a href=\"#2-2词粒度模型\" class=\"headerlink\" title=\"2.2词粒度模型\"></a>2.2词粒度模型</h4><p>同LSA,LDA和PV相比,其他模型(LSA,NLM,ESA等)均属于词粒度的模型.这是由于这些模型的产出仅为词向量.针对短文本理解这一任务,必须使用额外的合成手段来推出短文本的表示.例如,在文献[21-25]工作中,作者均利用词向量推导出文本表示,并用于后续的文本相似度判断、文本复述、情感分析等任务.这里的一个特例为概念化模型.由于概念化可以直接基于语境推出短文本中每个词的概念,这样的输出方式已经可以满足机器短文本理解的需求.因而概念化虽属于词粒度的模型但并不需要额外的文本合成.</p>\n<h3 id=\"2-3文本合成\"><a href=\"#2-3文本合成\" class=\"headerlink\" title=\"2.3文本合成\"></a>2.3文本合成</h3><p>如何通过词向量获取任意长度的文本向量(包括短文本)是时下流行的一个研究领域.根据复杂度的不同,文本合成方法可被大致分为代数向量模型^[5,21-23,25]^、张量模型^[26-28]^和神经网络模型^[7,24,29-32]^.</p>\n<p>1)代数运算模型.最早的合成模型由Mitchell和Lapata^[21]^提出.其模型使用逐点的(point-wise)向量相加的方式从词向量推出文本向量.虽然这一基于“词袋”的方法忽略了句子中的词序(“cat eats fish”和“fish eats cat”将有相同的表示),事实表明其在很多自然语言处理任务上有着不错的效果,且其常常被用作复杂模型的基准^[23]^,类似的代数运算<br>模型还有逐点的向量乘积^[5,21-22]^以及乘法与加法的结合运算^[24]^.</p>\n<p>2)张量模型.张量模型^[26-27]^为代数运算模型的延伸.其试图强调不同词性的词在语义合成中的不同角色.例如在“red car”这个词组中,形容词“red”对名词“car”起修饰作用;而在“eat apple”中,动词“eat”的角色好比作用于“apple”的函数.从这个角度而言,将不同词性的词均表示为同等维度的向量过于简化.因而,在张量模型中,不同词性的词被表示为<br>不同维度的张量,整个句子的表示方式以张量乘法的形式获取.目前,张量模型的最大挑战是如何获取向量与张量的映射关系^[28]^.</p>\n<p>3)神经网络模型.时下最为流行的文本合成模型为基于神经网络的模型,如recursive neural network(RecNN)^[29-30]^,recurrent neural network(RNN)^[5]^,convolutional neural<br> network(CNN)^[31-32]^等.在这些模型中,最基本的合成单元为神经网络.通常的形式为神经网络根据输入向量工<strong>x~1~</strong>，<strong>x~2~</strong>:推出其组合向量<strong>y</strong>。</p>\n<p>在具体的文本合成中,不同的神经网络模型的构造不同.例如,RecNN依赖于语法树开展逐层的语义合成,它无法被用于短文本.相比之下,RNN和CNN都可以快速通过词向量推导出短文本向量.</p>\n<h3 id=\"文献引用\"><a href=\"#文献引用\" class=\"headerlink\" title=\"文献引用\"></a>文献引用</h3><p>[1] Linmei H, Yang T, Shi C, et al. Heterogeneous graph attention networks for semi-supervised short text classification[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 4823-4832.</p>\n<p>[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.</p>\n<p>[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.</p>\n<p>[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, &amp; computers, 1996, 28(2): 203-208.</p>\n<p>[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.</p>\n<p>[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p>\n<p>[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.</p>\n<p>[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.</p>\n<p>[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.</p>\n<p>[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.</p>\n<p>[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e</p>\n<p>[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.</p>\n<p>[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h3 id=\"大纲目录\"><a href=\"#大纲目录\" class=\"headerlink\" title=\"大纲目录\"></a>大纲目录</h3><ul>\n<li>短文本分类的难点和挑战<ul>\n<li>和长文本相比，有什么不同，特殊在哪里，长文本的分类方法为什么不能适用于短文本分类</li>\n</ul>\n</li>\n<li>短文本分类包含哪些应用领域<ul>\n<li>评论数据等的情感分析</li>\n<li>聊天记录分析</li>\n<li>问答系统</li>\n<li>搜索</li>\n<li>其他</li>\n</ul>\n</li>\n<li>短文本分类方法划分<ul>\n<li>按照流程划分<ul>\n<li>短文本分类流程是怎样的，与长文本分类流程有什么不同</li>\n<li>特征抽取文本表示分类</li>\n</ul>\n</li>\n<li>标准划分<ul>\n<li>基于机器学习</li>\n<li>基于深度学习</li>\n<li>融合</li>\n</ul>\n</li>\n<li>是否引入外部知识源<ul>\n<li>无外部知识源</li>\n<li>有外部知识源</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>相关数据集整理，给出评价指标</li>\n<li>各个数据集上实验结果对比分析</li>\n<li>总结，分析未来发展趋势</li>\n</ul>\n<h3 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h3><p>短文本分类是自然语言处理领域的重要任务，包括情感分析、问答、对话管理等。随着Internet的大规模普及和用户数量的进一步增加,互联网上的各种短文本正在爆炸式地增长。短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.</p>\n<p>本文根据所需知识源的属性,将短文本理解模型分为3类:</p>\n<ul>\n<li>隐性(implicit)语义模型</li>\n<li>半显性(semi-explicit)语义模型</li>\n<li>显性(explicit)语义模型</li>\n</ul>\n<p>其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本分类.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本分类.</p>\n<p>从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度</p>\n<h3 id=\"文本分类方法\"><a href=\"#文本分类方法\" class=\"headerlink\" title=\"文本分类方法\"></a>文本分类方法</h3><h4 id=\"隐性-implicit-语义模型\"><a href=\"#隐性-implicit-语义模型\" class=\"headerlink\" title=\"隐性(implicit)语义模型\"></a>隐性(implicit)语义模型</h4><p>隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.</p>\n<ul>\n<li>LSA(latent semantic analysis)^[3]^</li>\n</ul>\n<p>LSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.</p>\n<ul>\n<li>超空间模拟语言模型^[4]^</li>\n</ul>\n<p>超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止<br>词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.</p>\n<ul>\n<li>神经网络语言模型</li>\n</ul>\n<p>近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不<br>同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).</p>\n<p>最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使<br>用目标词预测窗口中的每个语境词出现的概率.</p>\n<p>总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.</p>\n<ul>\n<li>段向量</li>\n</ul>\n<p>段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%25E5%259B%25BE%25E7%2589%258720200630104534.png\" alt=\"QQ图片20200630104534\"></p>\n<h4 id=\"半显性-semi-xpIicit-语义模型\"><a href=\"#半显性-semi-xpIicit-语义模型\" class=\"headerlink\" title=\"半显性(semi-xpIicit)语义模型\"></a>半显性(semi-xpIicit)语义模型</h4><p>半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic<br>models).</p>\n<p>LSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏<br>参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:</p>\n<p>$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$</p>\n<p>虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200701115340.png\" alt=\"QQ截图20200701115340\"></p>\n<p>总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.</p>\n<h4 id=\"显性-explicit-语义模型\"><a href=\"#显性-explicit-语义模型\" class=\"headerlink\" title=\"显性(explicit)语义模型\"></a>显性(explicit)语义模型</h4><p>近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常<br>是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.</p>\n<p>1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每<br>个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200702142120.png\" alt=\"QQ截图20200702142120\"></p>\n<p>2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad”这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad”).为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而<br>计算$p(concept |word ,topic)$.</p>\n<h3 id=\"模型粒度分析\"><a href=\"#模型粒度分析\" class=\"headerlink\" title=\"模型粒度分析\"></a>模型粒度分析</h3><p>本节将深入讨论第1节的短文本理解模型在文本分析粒度上的差异,并从应用层面论证不同方法的适用性.</p>\n<h4 id=\"文本粒度模型\"><a href=\"#文本粒度模型\" class=\"headerlink\" title=\"文本粒度模型\"></a>文本粒度模型</h4><p>首先,文本粒度的模型包含LSA,LDA和PV.这些模型均尝试直接推导出短文本的向量表示作为模型的输出.在LSA中,通过构建一个词与文本的共现矩阵,每个文本可用以词为维度的向量表示.类似地,LDA试图模拟文本的生成过程.作为结果,可得到每个文本的主题分布.PV通过神经网络推测(inference)的方式获取文本向量的最优参数.上述模型所得的文本向量均可以直接用于与这些文本相关的任务,如文本分类^[17-18]^、聚类^[19]^、摘要生成^[20]^。值得注意的是,LSA同时输出词向量.因而在短文本数量不足的情况下,可以先采用基于大量完整文本的LSA获取词向量,再通过额外的合成方法获取短文本向量.对于LDA和PV而言,其模型亦可以通过额外的文本训练,然后应用于短文本.</p>\n<h4 id=\"2-2词粒度模型\"><a href=\"#2-2词粒度模型\" class=\"headerlink\" title=\"2.2词粒度模型\"></a>2.2词粒度模型</h4><p>同LSA,LDA和PV相比,其他模型(LSA,NLM,ESA等)均属于词粒度的模型.这是由于这些模型的产出仅为词向量.针对短文本理解这一任务,必须使用额外的合成手段来推出短文本的表示.例如,在文献[21-25]工作中,作者均利用词向量推导出文本表示,并用于后续的文本相似度判断、文本复述、情感分析等任务.这里的一个特例为概念化模型.由于概念化可以直接基于语境推出短文本中每个词的概念,这样的输出方式已经可以满足机器短文本理解的需求.因而概念化虽属于词粒度的模型但并不需要额外的文本合成.</p>\n<h3 id=\"2-3文本合成\"><a href=\"#2-3文本合成\" class=\"headerlink\" title=\"2.3文本合成\"></a>2.3文本合成</h3><p>如何通过词向量获取任意长度的文本向量(包括短文本)是时下流行的一个研究领域.根据复杂度的不同,文本合成方法可被大致分为代数向量模型^[5,21-23,25]^、张量模型^[26-28]^和神经网络模型^[7,24,29-32]^.</p>\n<p>1)代数运算模型.最早的合成模型由Mitchell和Lapata^[21]^提出.其模型使用逐点的(point-wise)向量相加的方式从词向量推出文本向量.虽然这一基于“词袋”的方法忽略了句子中的词序(“cat eats fish”和“fish eats cat”将有相同的表示),事实表明其在很多自然语言处理任务上有着不错的效果,且其常常被用作复杂模型的基准^[23]^,类似的代数运算<br>模型还有逐点的向量乘积^[5,21-22]^以及乘法与加法的结合运算^[24]^.</p>\n<p>2)张量模型.张量模型^[26-27]^为代数运算模型的延伸.其试图强调不同词性的词在语义合成中的不同角色.例如在“red car”这个词组中,形容词“red”对名词“car”起修饰作用;而在“eat apple”中,动词“eat”的角色好比作用于“apple”的函数.从这个角度而言,将不同词性的词均表示为同等维度的向量过于简化.因而,在张量模型中,不同词性的词被表示为<br>不同维度的张量,整个句子的表示方式以张量乘法的形式获取.目前,张量模型的最大挑战是如何获取向量与张量的映射关系^[28]^.</p>\n<p>3)神经网络模型.时下最为流行的文本合成模型为基于神经网络的模型,如recursive neural network(RecNN)^[29-30]^,recurrent neural network(RNN)^[5]^,convolutional neural<br> network(CNN)^[31-32]^等.在这些模型中,最基本的合成单元为神经网络.通常的形式为神经网络根据输入向量工<strong>x~1~</strong>，<strong>x~2~</strong>:推出其组合向量<strong>y</strong>。</p>\n<p>在具体的文本合成中,不同的神经网络模型的构造不同.例如,RecNN依赖于语法树开展逐层的语义合成,它无法被用于短文本.相比之下,RNN和CNN都可以快速通过词向量推导出短文本向量.</p>\n<h3 id=\"文献引用\"><a href=\"#文献引用\" class=\"headerlink\" title=\"文献引用\"></a>文献引用</h3><p>[1] Linmei H, Yang T, Shi C, et al. Heterogeneous graph attention networks for semi-supervised short text classification[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 4823-4832.</p>\n<p>[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.</p>\n<p>[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.</p>\n<p>[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, &amp; computers, 1996, 28(2): 203-208.</p>\n<p>[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.</p>\n<p>[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p>\n<p>[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.</p>\n<p>[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.</p>\n<p>[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.</p>\n<p>[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.</p>\n<p>[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e</p>\n<p>[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.</p>\n<p>[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.</p>\n"},{"_content":"美颜技术的 **普及** 对 **社会审美** **好**\n\n分析：定义社会审美为形象美和内涵美\n\n* 分析普及对社会审美的好\n  * 普及是相较于少数而言的，普及之前，社会审美无疑是以内涵美为主的（主流美），普及之后，社会审美是以形象美为主（思考为什么）（以形象美为主对社会审美就是坏事吗？不是的，我们的形象美是建立在内涵美之上的，现有内涵美，再有的形象美，不是说普及之后，大家都不要内涵了，而是大家在内涵美有一定修养之后开始提升形象美）（**感觉可以深入考虑**）\n  * 普及本身在一定程度上就是一个褒义词，它能让以前只有少数人能拥有的东西让大家也有机会拥有，在社会审美这点上，能让大众接触到各种各样的美（考虑多元化，旁征博引）\n\n* 分析普及对社会审美的坏\n  * 坏处无非是反方认为美颜技术的普及误导了大众的审美，使得大众对美有错误的认知，只注意形象美而忽略内涵美（确实，我们可以抓住第一点反击，反方可能会利用小朋友不学习去当主播啥的来反击，我们可以甩锅，毕竟只是个例，君不见现如今社会依然好好的）\n* 其他\n  * 反方可能会抓出一堆不好的例子来反驳，考虑怎么反击","source":"_posts/美颜技术的普及对社会审美是好事.md","raw":"美颜技术的 **普及** 对 **社会审美** **好**\n\n分析：定义社会审美为形象美和内涵美\n\n* 分析普及对社会审美的好\n  * 普及是相较于少数而言的，普及之前，社会审美无疑是以内涵美为主的（主流美），普及之后，社会审美是以形象美为主（思考为什么）（以形象美为主对社会审美就是坏事吗？不是的，我们的形象美是建立在内涵美之上的，现有内涵美，再有的形象美，不是说普及之后，大家都不要内涵了，而是大家在内涵美有一定修养之后开始提升形象美）（**感觉可以深入考虑**）\n  * 普及本身在一定程度上就是一个褒义词，它能让以前只有少数人能拥有的东西让大家也有机会拥有，在社会审美这点上，能让大众接触到各种各样的美（考虑多元化，旁征博引）\n\n* 分析普及对社会审美的坏\n  * 坏处无非是反方认为美颜技术的普及误导了大众的审美，使得大众对美有错误的认知，只注意形象美而忽略内涵美（确实，我们可以抓住第一点反击，反方可能会利用小朋友不学习去当主播啥的来反击，我们可以甩锅，毕竟只是个例，君不见现如今社会依然好好的）\n* 其他\n  * 反方可能会抓出一堆不好的例子来反驳，考虑怎么反击","slug":"美颜技术的普及对社会审美是好事","published":1,"date":"2020-08-20T08:41:44.655Z","updated":"2020-08-20T08:41:44.656Z","title":"美颜技术的普及对社会审美是好事","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mbe001cewse24m2bw87","content":"<p>美颜技术的 <strong>普及</strong> 对 <strong>社会审美</strong> <strong>好</strong></p>\n<p>分析：定义社会审美为形象美和内涵美</p>\n<ul>\n<li><p>分析普及对社会审美的好</p>\n<ul>\n<li>普及是相较于少数而言的，普及之前，社会审美无疑是以内涵美为主的（主流美），普及之后，社会审美是以形象美为主（思考为什么）（以形象美为主对社会审美就是坏事吗？不是的，我们的形象美是建立在内涵美之上的，现有内涵美，再有的形象美，不是说普及之后，大家都不要内涵了，而是大家在内涵美有一定修养之后开始提升形象美）（<strong>感觉可以深入考虑</strong>）</li>\n<li>普及本身在一定程度上就是一个褒义词，它能让以前只有少数人能拥有的东西让大家也有机会拥有，在社会审美这点上，能让大众接触到各种各样的美（考虑多元化，旁征博引）</li>\n</ul>\n</li>\n<li><p>分析普及对社会审美的坏</p>\n<ul>\n<li>坏处无非是反方认为美颜技术的普及误导了大众的审美，使得大众对美有错误的认知，只注意形象美而忽略内涵美（确实，我们可以抓住第一点反击，反方可能会利用小朋友不学习去当主播啥的来反击，我们可以甩锅，毕竟只是个例，君不见现如今社会依然好好的）</li>\n</ul>\n</li>\n<li>其他<ul>\n<li>反方可能会抓出一堆不好的例子来反驳，考虑怎么反击</li>\n</ul>\n</li>\n</ul>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<p>美颜技术的 <strong>普及</strong> 对 <strong>社会审美</strong> <strong>好</strong></p>\n<p>分析：定义社会审美为形象美和内涵美</p>\n<ul>\n<li><p>分析普及对社会审美的好</p>\n<ul>\n<li>普及是相较于少数而言的，普及之前，社会审美无疑是以内涵美为主的（主流美），普及之后，社会审美是以形象美为主（思考为什么）（以形象美为主对社会审美就是坏事吗？不是的，我们的形象美是建立在内涵美之上的，现有内涵美，再有的形象美，不是说普及之后，大家都不要内涵了，而是大家在内涵美有一定修养之后开始提升形象美）（<strong>感觉可以深入考虑</strong>）</li>\n<li>普及本身在一定程度上就是一个褒义词，它能让以前只有少数人能拥有的东西让大家也有机会拥有，在社会审美这点上，能让大众接触到各种各样的美（考虑多元化，旁征博引）</li>\n</ul>\n</li>\n<li><p>分析普及对社会审美的坏</p>\n<ul>\n<li>坏处无非是反方认为美颜技术的普及误导了大众的审美，使得大众对美有错误的认知，只注意形象美而忽略内涵美（确实，我们可以抓住第一点反击，反方可能会利用小朋友不学习去当主播啥的来反击，我们可以甩锅，毕竟只是个例，君不见现如今社会依然好好的）</li>\n</ul>\n</li>\n<li>其他<ul>\n<li>反方可能会抓出一堆不好的例子来反驳，考虑怎么反击</li>\n</ul>\n</li>\n</ul>\n"},{"ssstitle":"短文本分类架构梳理","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-06-06T08:55:38.000Z","password":null,"summary":null,"img":null,"keywords":"短文本 分类 NLP","_content":"\n### 什么是短文本\n\n短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常词汇个数少且描述信息弱，具有稀疏性和歧义且通常标签数据较少。\n\n### 传统短文本分类算法\n\n基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模\n\n传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响\n\n### 基于深度学习的短文本分类算法\n\n短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息\n\n与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。\n\n一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。\n\n文本分类模型如下：\n\n​\t1） FastText\n\nFastText是Facebook开源的词向量与文本分类工具，模型简单，训练速度快。FastText 的原理是将短文本中的所有词向量进行平均，然后直接接softmax层，同时加入一些n-gram 特征的 trick 来捕获局部序列信息。相对于其它文本分类模型，如SVM，Logistic Regression和Neural Network等模型，FastText在保持分类效果的同时，大大缩短了训练时间，同时支持多语言表达，但其模型是基于词袋针对英文的文本分类方法，组成英文句子的单词是有间隔的，而应用于中文文本，需分词去标点转化为模型需要的数据格式。\n\n​\t2）TextCNN\n\nTextCNN相比于FastText，利用CNN (Convolutional Neural Network）来提取句子中类似 n-gram 的关键信息，且结构简单，效果好。\n\n​\t3）TextRNN\n\n尽管TextCNN能够在很多任务里面能有不错的表现，但CNN最大的问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 \"n-gram\" 信息。\n\n4）TextRNN + Attention\n\nCNN和RNN用在文本分类任务中尽管效果显著，但都有一个缺点，直观性和可解释性差。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够直观的给出每个词对结果的贡献，是Seq2Seq模型的标配。实际上文本分类从某种意义上也、可以理解为一种特殊的Seq2Seq，所以可以考虑将Attention机制引入。\n\nAttention的核心点是在翻译每个目标词（或预测商品标题文本所属类别）所用的上下文是不同的，这样更合理。加入Attention之后能够直观的解释各个句子和词对分类类别的重要性。\n\n5）TextRCNN（TextRNN + CNN）\n\n用前向和后向RNN得到每个词的前向和后向上下文的表示，这样词的表示就变成词向量和前向后向上下文向量concat起来的形式，最后连接TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1。\n\n\n\n### 短文本分类应用场景\n\n短文本分类算法广泛应用于各个行业领域,如新闻分类、人机写作判断、垃圾邮件识别、用户情感分类、文案智能生成、商品智能推荐等。\n\n场景一：商品智能推荐，根据用户购买的商品名称作为预测样本进行文本分类，得到用户交易类别，结合其他数据构建用户画像，针对不同特征的用户画像预测用户下一步的购买行为，智能推荐商品及服务。\n\n场景二：文案智能生成，基于优质文案作为训练集，得到文本分类模型，当用户输入关键词时，智能推荐适配文案。\n\n场景三：给新闻自动分类或打标签，多个标签。\n\n场景四：判断文章是人写还是机器写的。\n\n场景五：判断影评中的情感是正向、负向、中立，相类似应用场景很\n\n\n\n\n\n\n\n### 文献引用\n\n[1] Linmei, H., Yang, T., Shi, C., Ji, H., & Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* (pp. 4823-4832).\n\n[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.\n\n[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.\n\n[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.\n\n[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.\n\n[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n\n[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.\n\n[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n\n\n\n\n\n## 论文阅读\n\n### 题目：Short Text Classification: A Survey\n\n分类方式：\n\n* short text classification using sematic analysis（语义分析）\n* semi-supervised short text classification（半监督）\n* ensemble short text classification（集成学习）\n* real-time classification.\n\n### I. INTRODUCTION\n\n随着电子商务和在线通信的爆炸式增长，在许多应用领域中都可以使用短文本，例如即时消息，在线聊天日志，公告板系统标题，Web日志评论，Internet新闻评论，SMS，Twitter等。因此，成功在许多Web和IR应用程序中处理它们变得越来越重要。 但是，对这些类型的文本和Web数据进行分类是一个新的挑战。\n\n与普通文档不同，这些文本和Web段通常更嘈杂，主题更少，并且更短，也就是说，它们由十几个单词到几个句子组成[1]。 由于篇幅短，它们无法提供足够的单词共现或共享语境，无法实现良好的相似度[40]。 因此，依赖于词频，足够的词共现或共享上下文来测量文档相似度的常规机器学习方法通常由于数据稀疏而无法达到期望的准确性。\n\n出现了针对短文本的新分类方法，例如语义分析，半监督短文本分类，集成短文本分类、实时分类。然而，与许多有关文本分类的评论和调查相比，只有很少的调查来讨论有关短文本分类的最新研究。 本文分析了与短文本分类相关的挑战，并系统地总结了使用分析方法对短文本分类进行分类的现有方法。\n\n在分析了短文本的特点和难点之后，我们在第二节中指出了短文本分类的过程。 第三节介绍了基于语义分析的短文本分类。 我们在第四节中描述了一些关于半监督短文本分类的算法。 第五节和第六节分别介绍了用于对短文本进行分类和在线短文本分类的集成模型。 我们在第七节中分析了相关的评估措施。 在第八节中，我们总结了对短文本进行分类的方法。\n\n### II. BACKGROUNDS\n\n#### A. Feature of Short Text\n\n通常，短文本的特征如下[1] [2]：\n\n稀疏性：简短的文本仅包含数个到十几个具有某些功能的单词，它不能提供足够的单词共现或共享上下文，无法提供良好的相似性度量。 很难提取其有效的语言功能。\n\n即时性：短文本会立即发送并实时接收。 另外，数量非常大。\n\n非标准性：简短的文字说明简洁明了，有许多拼写错误，非标准术语和噪音。\n\n噪音和分布不平衡：应用程序背景（例如网络安全性）需要处理大量的短文本数据。 但是，我们可能只关注大规模数据中的一小部分（检测对象）。 因此，有用的实例是有限的，并且短文本的分布是不平衡的。\n\n大型数据和标记瓶颈：手动标记所有大型实例很困难。 标签数量有限的实例只能提供有限的信息。 因此，如何充分利用这些标记的实例和其他未标记的实例已经成为短文本分类的关键问题。\n\n大多数传统方法（例如SVM，BAYES和KNN）都是基于术语频率的相似性，而忽略了短文本的特征。 这些传统方法可能无法处理短文本分类。 如果标记的信息不足，它们中的大多数（例如BAYES）可能无法获得高精度。 另外，一些基于向量空间模型（SVM）的分类方法应该使用语义信息来提高分类器的性能。\n\n#### B. Short Text Classification\n\n在过去的十年中，已经深入研究了对文本和Web文档进行分类的学习。 许多学习方法，例如k个最近邻（k-NN），朴素贝叶斯（Naive Bayes），最大熵和支持向量机（SVM），已应用于具有不同基准集合的许多分类问题，并取得了令人满意的结果。 但是，由于短文字的特点和难度，传统的分类方法不适合短文本分类。 因此，如何合理地表示和选择特征项，有效地减小空间尺寸和噪声，提高分类精度成为短文本分类的问题。\n\n#### III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS\n\n目前，减少特征空间维数的解决方案主要基于语义特征和语义分析。 这是因为文本分类的处理通常是在向量空间模型（VSM）中进行的，其基本假设是单词之间的关系是独立的，而忽略了文本之间的相关性。但是，短文本的语义表达能力较弱，需要这种相关性。 传统分类不能区分自然语言的歧义词和同义词，所有这些在短文本中都很丰富。 因此，传统的分类方法通常无法达到预期的短文本准确性。\n\n语义分析更加关注概念，内部结构的语义层次以及文本的相关性，从而获得了更具表现力和客观性的逻辑结构。 在现有研究中，基于潜在语义分析的分类占有重要地位。 使用统计方法，潜在语义分析可提取潜在的语义结构，消除同义词影响，并减少特征维数和噪声。因此，提出了许多基于语义分析的算法来处理短文本分类（更多详细信息见表I）[3] [5-11] [44]。  Zelikovitz [3]将其应用于短文本分类。  Pu强等[5]将LSA和独立成分分析（ICA）[8] [42]结合在一起。  Xuan-Hieu Phan等[7]建立了大规模的短文本分类框架。 该框架主要基于最近成功的潜在主题分析模型（例如pLSA和LDA）和强大的机器学习方法（例如最大熵和SVM）。王炳坤等[9]提出了一种新的方法，通过基于潜在狄利克雷分配（LDA）和信息增益（IG）模型构建强大的词库（SFT）来解决该问题。 当使用句法或语义信息时，提出了语言独立语义（LIS）内核[10]以增强语言依赖性。 它能够有效地计算短文本文档之间的相似度，而无需使用语法标记和词汇数据库。 陈梦根等[11]提出了一种以多种粒度提取主题的方法，该方法可以更精确地建模短文本。 转换式LSA [3] [4]是基于LSA的短文本分类的另一个示例。 转导利用测试示例来选择学习者的假设。 通过合并测试示例来重新创建空间确实会根据测试示例选择一种表示形式。 训练/测试集的维数减少使得较小的空间可以更准确地反映将应用于分类的测试集。 通过将测试示例包含到原始矩阵中，LSA可以计算单词与单词的熵权以及测试集中单词的示例和共现。\n\n#### IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION\n\n半监督学习是指使用标记和未标记的数据进行训练。 它对比了监督学习（所有数据都标记）或无监督学习（所有数据都未标记）。用于学习问题的标记数据的获取通常需要熟练的人工代理手动对训练示例进行分类。 因此，与标记过程相关的成本可能使完全标记的训练集变得不可行，而未标记数据的获取相对便宜。 在这种情况下，半监督学习可能具有很大的实用价值[27]。\n\n大多数半监督的短文本分类都是灵活的半监督学习。 它可以利用未标记的数据来改进分类器。 但是，与传统的半监督学习算法不同，通用数据和训练/测试数据不需要具有相同的格式。 此外，一旦进行了估计，只要主题模型是一致的，就可以将其应用于多个分类问题。\n\n背景知识可能会提供一个文本语料库，其中包含有关单词重要性和单词联合概率的信息[29] [6]。 我们可以结合使用背景和培训示例来标记新示例。\n\nV. ENSEMBLE SHORT TEXT CLASSIFICATION\n\nVI. REAL-TIME CLASSIFICATION OF LARGE SCALE SHORT TEXT\n\n即时性是短文本的另一个功能，这意味着短文本会立即发送并实时接收，通常数量非常大。 因此，如何立即对大规模的短文本数据进行分类也成为一个重要的问题。 目前，与几种经典分类算法相比，经常选择贝叶斯算法作为在线分类器。 朴素贝叶斯算法通过计算文本属于每个类别的概率来判断类别，这是一种简单，准确且广泛使用的算法[39]。\n\n### 题目：短文本理解研究\n\n与长文本不同,短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.\n\n许多相关工作^[1-3]^证明,自动化的短文本分类需要依赖额外的知识.这些知识可以帮助机器充分挖掘短文本中词与词之间的联系,如语义相关性.例如,在英文查询“premiere Lincoln”中,“premiere”是一个重要的信息,表明“Lincoln”在这里指的是“电影”;同样,在“watch harry potter”中,正因为“watch”的出现,“harry potter”的含义可被鉴定为“电影”或“DVD”,而不是“书籍”.但是,这些关于词汇的知识(例如“watch”的对象通常是“电影”)并没有在短文本中明确表示出来,因而需要通过额外的知识源获取.\n\n根据所需知识源的属性,将短文本理解模型分为3类:\n\n* 隐性(implicit)语义模型\n* 半显性(semi-explicit)语义模型\n* 显性(explicit)语义模型\n\n其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本理解.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本理解.\n\n从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度\n\n#### 隐性(implicit)语义模型\n\n隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.\n\n- LSA(latent semantic analysis)^[3]^\n\nLSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.\n\n* 超空间模拟语言模型^[4]^\n\n超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止\n词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.\n\n* 神经网络语言模型\n\n近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不\n同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).\n\n最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使\n用目标词预测窗口中的每个语境词出现的概率.\n\n总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.\n\n* 段向量\n\n段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.\n\n![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png)\n\n#### 半显性(semi-xpIicit)语义模型\n\n半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic\nmodels).\n\nLSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏\n参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:\n\n$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$\n\n虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的\t主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).\n\n![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png)\n\n总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.\n\n#### 显性(explicit)语义模型\n\n近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常\n是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.\n\n1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每\n个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.\n\n2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad\"这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad\").为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而\n计算$p(concept |word ,topic)$.\n\n### 文献引用\n\n[1] Linmei, H., Yang, T., Shi, C., Ji, H., & Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* (pp. 4823-4832).\n\n[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.\n\n[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.\n\n[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.\n\n[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.\n\n[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n\n[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.\n\n[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n\n[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.\n\n[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e\n\n[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.\n\n[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.\n\n[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.\n\n### 《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》\n\n#### 基于机器学习\n\n* 支持向量机(Support Vector Machine, SVM)\n* 朴素贝叶斯分类法\n*  K-最近邻法\n*  决策树法(Decision Tree, DT)\n*  中心向量法 \n\n#### 基于深度学习\n\n* CNN+语义约束（Short text classification improved by learning multi-granularity topics-AAAI2011）\n\n* CNN+语义聚类（Semantic clustering and convolutional neural network for short text categorization-ACL2015）\n* CNN+RNN+顺序短文本分类（Sequential short-text classification with recurrent and convolutional neural networks-arxiv2016）\n* DBN\n\n\n\n### 标准划分\n\n#### 传统短文本分类算法\n\n基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模\n\n传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响\n\n伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典方法，整个文本分类问题就拆分成了特征工程和分类器两部分。\n\n* 特征工程\n\n特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据-->信息”的过程，决定了结果的上限，而分类器是“信息-->知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。\n\n文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统文本分类任务大部分工作也在此处。文本特征工程分为文本预处理、特征提取、文本表示三个部分，目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。\n\n（1）文本预处理\n\n文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。\n\n具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的冠词助词代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。\n\n（2）文本表示和特征提取\n\n文本表示：\n\n文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。\n\n特征提取：\n\n向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。\n\n特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。\n\n信息增益特征提取方法：该方法基于信息熵的原理，通过计算各个特征在不同类别样本中的分布情况，通过统计比较发现具有较高类别区分度的特征。信息增益能较好的描述不同特征对不同类别的区别能力，信息增益值越高则区别能力越强，在特征选择时保留较大IG 值，在分类时就更容易得到正确的分类结果，而且能降低特征的维度。\n\n卡方检验特征选择方法：该方法主要基于相关性检验原理，通过计算某属性与某一类的相关性，来衡量特征候选项项的重要性，该方法考虑到了反相关的属性。在这里反相关的特征候选项同样存在有用信息，反相关的信息量对分类和聚类算法都有积极作用。\n\n特征权重主要是经典的TF-IDF及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。\n\n（3）特征扩展\n\n由于短文本的特征少，固需要对文本向量进行特征扩展。当前对于短文本信息分类的研究主要聚焦于短文特征的扩展问题之上。特征扩展方法有基于WordNet、Wik ipedia或其他知识库等的特征扩展方法，然而这些方法来说一些的不足之处，对于短文本数据，尤其是即时数据来说，这些方法并不十分合适，因为这些知识库依靠的更多是人工参与编辑，信息相对滞后，扩展特征不够全面，缺乏全局性与一致性。另外一个更重要的原因是网络信息检索本身就会出现噪声问题。这些都会将错误因素累积，所以必须对检索结果进一步筛选。\n\n为了解决这些问题，一些研究人员通过重新计算短文本与检索结果的相似度，验证并实现了多种方法，包括语义分析等。短文本计算相似度时有一定得困难，短文本包含的特征太少，若直接使用相同特征作为度量标准的话，很多短文本之间的相似度为零。为了解决这个问题，需要采用一种适合短文本相似度计算的方法，常见的相似度计算方法有：1．基于本体词典的规则化方法；2．基于知识库的规则化方法；3．基于搜索引擎的方法；4．基于统计的方法；5．句子层面语义相似度计算方法。\n\n（4）基于语义的文本表示\n\n传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础。\n\n* 分类器\n\n分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等。\n\n### 深度学习文本分类方法\n\n传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。\n\n短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息\n\n与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。\n\n一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。","source":"_posts/短文本分类架构梳理.md","raw":"---\nssstitle: 短文本分类架构梳理\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-06-06 16:55:38\npassword:\nsummary:\ntags:\n\t- 短文本\n\t- 分类\n\t- NLP\ncategories: 短文本理解\nimg:\nkeywords: 短文本 分类 NLP\n---\n\n### 什么是短文本\n\n短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常词汇个数少且描述信息弱，具有稀疏性和歧义且通常标签数据较少。\n\n### 传统短文本分类算法\n\n基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模\n\n传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响\n\n### 基于深度学习的短文本分类算法\n\n短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息\n\n与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。\n\n一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。\n\n文本分类模型如下：\n\n​\t1） FastText\n\nFastText是Facebook开源的词向量与文本分类工具，模型简单，训练速度快。FastText 的原理是将短文本中的所有词向量进行平均，然后直接接softmax层，同时加入一些n-gram 特征的 trick 来捕获局部序列信息。相对于其它文本分类模型，如SVM，Logistic Regression和Neural Network等模型，FastText在保持分类效果的同时，大大缩短了训练时间，同时支持多语言表达，但其模型是基于词袋针对英文的文本分类方法，组成英文句子的单词是有间隔的，而应用于中文文本，需分词去标点转化为模型需要的数据格式。\n\n​\t2）TextCNN\n\nTextCNN相比于FastText，利用CNN (Convolutional Neural Network）来提取句子中类似 n-gram 的关键信息，且结构简单，效果好。\n\n​\t3）TextRNN\n\n尽管TextCNN能够在很多任务里面能有不错的表现，但CNN最大的问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 \"n-gram\" 信息。\n\n4）TextRNN + Attention\n\nCNN和RNN用在文本分类任务中尽管效果显著，但都有一个缺点，直观性和可解释性差。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够直观的给出每个词对结果的贡献，是Seq2Seq模型的标配。实际上文本分类从某种意义上也、可以理解为一种特殊的Seq2Seq，所以可以考虑将Attention机制引入。\n\nAttention的核心点是在翻译每个目标词（或预测商品标题文本所属类别）所用的上下文是不同的，这样更合理。加入Attention之后能够直观的解释各个句子和词对分类类别的重要性。\n\n5）TextRCNN（TextRNN + CNN）\n\n用前向和后向RNN得到每个词的前向和后向上下文的表示，这样词的表示就变成词向量和前向后向上下文向量concat起来的形式，最后连接TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1。\n\n\n\n### 短文本分类应用场景\n\n短文本分类算法广泛应用于各个行业领域,如新闻分类、人机写作判断、垃圾邮件识别、用户情感分类、文案智能生成、商品智能推荐等。\n\n场景一：商品智能推荐，根据用户购买的商品名称作为预测样本进行文本分类，得到用户交易类别，结合其他数据构建用户画像，针对不同特征的用户画像预测用户下一步的购买行为，智能推荐商品及服务。\n\n场景二：文案智能生成，基于优质文案作为训练集，得到文本分类模型，当用户输入关键词时，智能推荐适配文案。\n\n场景三：给新闻自动分类或打标签，多个标签。\n\n场景四：判断文章是人写还是机器写的。\n\n场景五：判断影评中的情感是正向、负向、中立，相类似应用场景很\n\n\n\n\n\n\n\n### 文献引用\n\n[1] Linmei, H., Yang, T., Shi, C., Ji, H., & Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* (pp. 4823-4832).\n\n[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.\n\n[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.\n\n[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.\n\n[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.\n\n[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n\n[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.\n\n[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n\n\n\n\n\n## 论文阅读\n\n### 题目：Short Text Classification: A Survey\n\n分类方式：\n\n* short text classification using sematic analysis（语义分析）\n* semi-supervised short text classification（半监督）\n* ensemble short text classification（集成学习）\n* real-time classification.\n\n### I. INTRODUCTION\n\n随着电子商务和在线通信的爆炸式增长，在许多应用领域中都可以使用短文本，例如即时消息，在线聊天日志，公告板系统标题，Web日志评论，Internet新闻评论，SMS，Twitter等。因此，成功在许多Web和IR应用程序中处理它们变得越来越重要。 但是，对这些类型的文本和Web数据进行分类是一个新的挑战。\n\n与普通文档不同，这些文本和Web段通常更嘈杂，主题更少，并且更短，也就是说，它们由十几个单词到几个句子组成[1]。 由于篇幅短，它们无法提供足够的单词共现或共享语境，无法实现良好的相似度[40]。 因此，依赖于词频，足够的词共现或共享上下文来测量文档相似度的常规机器学习方法通常由于数据稀疏而无法达到期望的准确性。\n\n出现了针对短文本的新分类方法，例如语义分析，半监督短文本分类，集成短文本分类、实时分类。然而，与许多有关文本分类的评论和调查相比，只有很少的调查来讨论有关短文本分类的最新研究。 本文分析了与短文本分类相关的挑战，并系统地总结了使用分析方法对短文本分类进行分类的现有方法。\n\n在分析了短文本的特点和难点之后，我们在第二节中指出了短文本分类的过程。 第三节介绍了基于语义分析的短文本分类。 我们在第四节中描述了一些关于半监督短文本分类的算法。 第五节和第六节分别介绍了用于对短文本进行分类和在线短文本分类的集成模型。 我们在第七节中分析了相关的评估措施。 在第八节中，我们总结了对短文本进行分类的方法。\n\n### II. BACKGROUNDS\n\n#### A. Feature of Short Text\n\n通常，短文本的特征如下[1] [2]：\n\n稀疏性：简短的文本仅包含数个到十几个具有某些功能的单词，它不能提供足够的单词共现或共享上下文，无法提供良好的相似性度量。 很难提取其有效的语言功能。\n\n即时性：短文本会立即发送并实时接收。 另外，数量非常大。\n\n非标准性：简短的文字说明简洁明了，有许多拼写错误，非标准术语和噪音。\n\n噪音和分布不平衡：应用程序背景（例如网络安全性）需要处理大量的短文本数据。 但是，我们可能只关注大规模数据中的一小部分（检测对象）。 因此，有用的实例是有限的，并且短文本的分布是不平衡的。\n\n大型数据和标记瓶颈：手动标记所有大型实例很困难。 标签数量有限的实例只能提供有限的信息。 因此，如何充分利用这些标记的实例和其他未标记的实例已经成为短文本分类的关键问题。\n\n大多数传统方法（例如SVM，BAYES和KNN）都是基于术语频率的相似性，而忽略了短文本的特征。 这些传统方法可能无法处理短文本分类。 如果标记的信息不足，它们中的大多数（例如BAYES）可能无法获得高精度。 另外，一些基于向量空间模型（SVM）的分类方法应该使用语义信息来提高分类器的性能。\n\n#### B. Short Text Classification\n\n在过去的十年中，已经深入研究了对文本和Web文档进行分类的学习。 许多学习方法，例如k个最近邻（k-NN），朴素贝叶斯（Naive Bayes），最大熵和支持向量机（SVM），已应用于具有不同基准集合的许多分类问题，并取得了令人满意的结果。 但是，由于短文字的特点和难度，传统的分类方法不适合短文本分类。 因此，如何合理地表示和选择特征项，有效地减小空间尺寸和噪声，提高分类精度成为短文本分类的问题。\n\n#### III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS\n\n目前，减少特征空间维数的解决方案主要基于语义特征和语义分析。 这是因为文本分类的处理通常是在向量空间模型（VSM）中进行的，其基本假设是单词之间的关系是独立的，而忽略了文本之间的相关性。但是，短文本的语义表达能力较弱，需要这种相关性。 传统分类不能区分自然语言的歧义词和同义词，所有这些在短文本中都很丰富。 因此，传统的分类方法通常无法达到预期的短文本准确性。\n\n语义分析更加关注概念，内部结构的语义层次以及文本的相关性，从而获得了更具表现力和客观性的逻辑结构。 在现有研究中，基于潜在语义分析的分类占有重要地位。 使用统计方法，潜在语义分析可提取潜在的语义结构，消除同义词影响，并减少特征维数和噪声。因此，提出了许多基于语义分析的算法来处理短文本分类（更多详细信息见表I）[3] [5-11] [44]。  Zelikovitz [3]将其应用于短文本分类。  Pu强等[5]将LSA和独立成分分析（ICA）[8] [42]结合在一起。  Xuan-Hieu Phan等[7]建立了大规模的短文本分类框架。 该框架主要基于最近成功的潜在主题分析模型（例如pLSA和LDA）和强大的机器学习方法（例如最大熵和SVM）。王炳坤等[9]提出了一种新的方法，通过基于潜在狄利克雷分配（LDA）和信息增益（IG）模型构建强大的词库（SFT）来解决该问题。 当使用句法或语义信息时，提出了语言独立语义（LIS）内核[10]以增强语言依赖性。 它能够有效地计算短文本文档之间的相似度，而无需使用语法标记和词汇数据库。 陈梦根等[11]提出了一种以多种粒度提取主题的方法，该方法可以更精确地建模短文本。 转换式LSA [3] [4]是基于LSA的短文本分类的另一个示例。 转导利用测试示例来选择学习者的假设。 通过合并测试示例来重新创建空间确实会根据测试示例选择一种表示形式。 训练/测试集的维数减少使得较小的空间可以更准确地反映将应用于分类的测试集。 通过将测试示例包含到原始矩阵中，LSA可以计算单词与单词的熵权以及测试集中单词的示例和共现。\n\n#### IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION\n\n半监督学习是指使用标记和未标记的数据进行训练。 它对比了监督学习（所有数据都标记）或无监督学习（所有数据都未标记）。用于学习问题的标记数据的获取通常需要熟练的人工代理手动对训练示例进行分类。 因此，与标记过程相关的成本可能使完全标记的训练集变得不可行，而未标记数据的获取相对便宜。 在这种情况下，半监督学习可能具有很大的实用价值[27]。\n\n大多数半监督的短文本分类都是灵活的半监督学习。 它可以利用未标记的数据来改进分类器。 但是，与传统的半监督学习算法不同，通用数据和训练/测试数据不需要具有相同的格式。 此外，一旦进行了估计，只要主题模型是一致的，就可以将其应用于多个分类问题。\n\n背景知识可能会提供一个文本语料库，其中包含有关单词重要性和单词联合概率的信息[29] [6]。 我们可以结合使用背景和培训示例来标记新示例。\n\nV. ENSEMBLE SHORT TEXT CLASSIFICATION\n\nVI. REAL-TIME CLASSIFICATION OF LARGE SCALE SHORT TEXT\n\n即时性是短文本的另一个功能，这意味着短文本会立即发送并实时接收，通常数量非常大。 因此，如何立即对大规模的短文本数据进行分类也成为一个重要的问题。 目前，与几种经典分类算法相比，经常选择贝叶斯算法作为在线分类器。 朴素贝叶斯算法通过计算文本属于每个类别的概率来判断类别，这是一种简单，准确且广泛使用的算法[39]。\n\n### 题目：短文本理解研究\n\n与长文本不同,短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.\n\n许多相关工作^[1-3]^证明,自动化的短文本分类需要依赖额外的知识.这些知识可以帮助机器充分挖掘短文本中词与词之间的联系,如语义相关性.例如,在英文查询“premiere Lincoln”中,“premiere”是一个重要的信息,表明“Lincoln”在这里指的是“电影”;同样,在“watch harry potter”中,正因为“watch”的出现,“harry potter”的含义可被鉴定为“电影”或“DVD”,而不是“书籍”.但是,这些关于词汇的知识(例如“watch”的对象通常是“电影”)并没有在短文本中明确表示出来,因而需要通过额外的知识源获取.\n\n根据所需知识源的属性,将短文本理解模型分为3类:\n\n* 隐性(implicit)语义模型\n* 半显性(semi-explicit)语义模型\n* 显性(explicit)语义模型\n\n其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本理解.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本理解.\n\n从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度\n\n#### 隐性(implicit)语义模型\n\n隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.\n\n- LSA(latent semantic analysis)^[3]^\n\nLSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.\n\n* 超空间模拟语言模型^[4]^\n\n超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止\n词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.\n\n* 神经网络语言模型\n\n近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不\n同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).\n\n最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使\n用目标词预测窗口中的每个语境词出现的概率.\n\n总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.\n\n* 段向量\n\n段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.\n\n![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png)\n\n#### 半显性(semi-xpIicit)语义模型\n\n半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic\nmodels).\n\nLSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏\n参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:\n\n$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$\n\n虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的\t主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).\n\n![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png)\n\n总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.\n\n#### 显性(explicit)语义模型\n\n近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常\n是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.\n\n1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每\n个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.\n\n2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad\"这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad\").为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而\n计算$p(concept |word ,topic)$.\n\n### 文献引用\n\n[1] Linmei, H., Yang, T., Shi, C., Ji, H., & Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* (pp. 4823-4832).\n\n[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.\n\n[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.\n\n[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.\n\n[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.\n\n[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n\n[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.\n\n[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.\n\n[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.\n\n[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e\n\n[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.\n\n[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.\n\n[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.\n\n### 《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》\n\n#### 基于机器学习\n\n* 支持向量机(Support Vector Machine, SVM)\n* 朴素贝叶斯分类法\n*  K-最近邻法\n*  决策树法(Decision Tree, DT)\n*  中心向量法 \n\n#### 基于深度学习\n\n* CNN+语义约束（Short text classification improved by learning multi-granularity topics-AAAI2011）\n\n* CNN+语义聚类（Semantic clustering and convolutional neural network for short text categorization-ACL2015）\n* CNN+RNN+顺序短文本分类（Sequential short-text classification with recurrent and convolutional neural networks-arxiv2016）\n* DBN\n\n\n\n### 标准划分\n\n#### 传统短文本分类算法\n\n基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模\n\n传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响\n\n伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典方法，整个文本分类问题就拆分成了特征工程和分类器两部分。\n\n* 特征工程\n\n特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据-->信息”的过程，决定了结果的上限，而分类器是“信息-->知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。\n\n文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统文本分类任务大部分工作也在此处。文本特征工程分为文本预处理、特征提取、文本表示三个部分，目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。\n\n（1）文本预处理\n\n文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。\n\n具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的冠词助词代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。\n\n（2）文本表示和特征提取\n\n文本表示：\n\n文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。\n\n特征提取：\n\n向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。\n\n特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。\n\n信息增益特征提取方法：该方法基于信息熵的原理，通过计算各个特征在不同类别样本中的分布情况，通过统计比较发现具有较高类别区分度的特征。信息增益能较好的描述不同特征对不同类别的区别能力，信息增益值越高则区别能力越强，在特征选择时保留较大IG 值，在分类时就更容易得到正确的分类结果，而且能降低特征的维度。\n\n卡方检验特征选择方法：该方法主要基于相关性检验原理，通过计算某属性与某一类的相关性，来衡量特征候选项项的重要性，该方法考虑到了反相关的属性。在这里反相关的特征候选项同样存在有用信息，反相关的信息量对分类和聚类算法都有积极作用。\n\n特征权重主要是经典的TF-IDF及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。\n\n（3）特征扩展\n\n由于短文本的特征少，固需要对文本向量进行特征扩展。当前对于短文本信息分类的研究主要聚焦于短文特征的扩展问题之上。特征扩展方法有基于WordNet、Wik ipedia或其他知识库等的特征扩展方法，然而这些方法来说一些的不足之处，对于短文本数据，尤其是即时数据来说，这些方法并不十分合适，因为这些知识库依靠的更多是人工参与编辑，信息相对滞后，扩展特征不够全面，缺乏全局性与一致性。另外一个更重要的原因是网络信息检索本身就会出现噪声问题。这些都会将错误因素累积，所以必须对检索结果进一步筛选。\n\n为了解决这些问题，一些研究人员通过重新计算短文本与检索结果的相似度，验证并实现了多种方法，包括语义分析等。短文本计算相似度时有一定得困难，短文本包含的特征太少，若直接使用相同特征作为度量标准的话，很多短文本之间的相似度为零。为了解决这个问题，需要采用一种适合短文本相似度计算的方法，常见的相似度计算方法有：1．基于本体词典的规则化方法；2．基于知识库的规则化方法；3．基于搜索引擎的方法；4．基于统计的方法；5．句子层面语义相似度计算方法。\n\n（4）基于语义的文本表示\n\n传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础。\n\n* 分类器\n\n分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等。\n\n### 深度学习文本分类方法\n\n传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。\n\n短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息\n\n与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。\n\n一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。","slug":"短文本分类架构梳理","published":1,"updated":"2020-08-20T08:41:44.655Z","title":"短文本分类架构梳理","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mbj001hewse85r9ho8r","content":"<h3 id=\"什么是短文本\"><a href=\"#什么是短文本\" class=\"headerlink\" title=\"什么是短文本\"></a>什么是短文本</h3><p>短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常词汇个数少且描述信息弱，具有稀疏性和歧义且通常标签数据较少。</p>\n<h3 id=\"传统短文本分类算法\"><a href=\"#传统短文本分类算法\" class=\"headerlink\" title=\"传统短文本分类算法\"></a>传统短文本分类算法</h3><p>基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模</p>\n<p>传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响</p>\n<h3 id=\"基于深度学习的短文本分类算法\"><a href=\"#基于深度学习的短文本分类算法\" class=\"headerlink\" title=\"基于深度学习的短文本分类算法\"></a>基于深度学习的短文本分类算法</h3><p>短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息</p>\n<p>与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。</p>\n<p>一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。</p>\n<p>文本分类模型如下：</p>\n<p>​    1） FastText</p>\n<p>FastText是Facebook开源的词向量与文本分类工具，模型简单，训练速度快。FastText 的原理是将短文本中的所有词向量进行平均，然后直接接softmax层，同时加入一些n-gram 特征的 trick 来捕获局部序列信息。相对于其它文本分类模型，如SVM，Logistic Regression和Neural Network等模型，FastText在保持分类效果的同时，大大缩短了训练时间，同时支持多语言表达，但其模型是基于词袋针对英文的文本分类方法，组成英文句子的单词是有间隔的，而应用于中文文本，需分词去标点转化为模型需要的数据格式。</p>\n<p>​    2）TextCNN</p>\n<p>TextCNN相比于FastText，利用CNN (Convolutional Neural Network）来提取句子中类似 n-gram 的关键信息，且结构简单，效果好。</p>\n<p>​    3）TextRNN</p>\n<p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN最大的问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p>\n<p>4）TextRNN + Attention</p>\n<p>CNN和RNN用在文本分类任务中尽管效果显著，但都有一个缺点，直观性和可解释性差。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够直观的给出每个词对结果的贡献，是Seq2Seq模型的标配。实际上文本分类从某种意义上也、可以理解为一种特殊的Seq2Seq，所以可以考虑将Attention机制引入。</p>\n<p>Attention的核心点是在翻译每个目标词（或预测商品标题文本所属类别）所用的上下文是不同的，这样更合理。加入Attention之后能够直观的解释各个句子和词对分类类别的重要性。</p>\n<p>5）TextRCNN（TextRNN + CNN）</p>\n<p>用前向和后向RNN得到每个词的前向和后向上下文的表示，这样词的表示就变成词向量和前向后向上下文向量concat起来的形式，最后连接TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1。</p>\n<h3 id=\"短文本分类应用场景\"><a href=\"#短文本分类应用场景\" class=\"headerlink\" title=\"短文本分类应用场景\"></a>短文本分类应用场景</h3><p>短文本分类算法广泛应用于各个行业领域,如新闻分类、人机写作判断、垃圾邮件识别、用户情感分类、文案智能生成、商品智能推荐等。</p>\n<p>场景一：商品智能推荐，根据用户购买的商品名称作为预测样本进行文本分类，得到用户交易类别，结合其他数据构建用户画像，针对不同特征的用户画像预测用户下一步的购买行为，智能推荐商品及服务。</p>\n<p>场景二：文案智能生成，基于优质文案作为训练集，得到文本分类模型，当用户输入关键词时，智能推荐适配文案。</p>\n<p>场景三：给新闻自动分类或打标签，多个标签。</p>\n<p>场景四：判断文章是人写还是机器写的。</p>\n<p>场景五：判断影评中的情感是正向、负向、中立，相类似应用场景很</p>\n<h3 id=\"文献引用\"><a href=\"#文献引用\" class=\"headerlink\" title=\"文献引用\"></a>文献引用</h3><p>[1] Linmei, H., Yang, T., Shi, C., Ji, H., &amp; Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> (pp. 4823-4832).</p>\n<p>[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.</p>\n<p>[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.</p>\n<p>[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, &amp; computers, 1996, 28(2): 203-208.</p>\n<p>[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.</p>\n<p>[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p>\n<p>[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.</p>\n<p>[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.</p>\n<h2 id=\"论文阅读\"><a href=\"#论文阅读\" class=\"headerlink\" title=\"论文阅读\"></a>论文阅读</h2><h3 id=\"题目：Short-Text-Classification-A-Survey\"><a href=\"#题目：Short-Text-Classification-A-Survey\" class=\"headerlink\" title=\"题目：Short Text Classification: A Survey\"></a>题目：Short Text Classification: A Survey</h3><p>分类方式：</p>\n<ul>\n<li>short text classification using sematic analysis（语义分析）</li>\n<li>semi-supervised short text classification（半监督）</li>\n<li>ensemble short text classification（集成学习）</li>\n<li>real-time classification.</li>\n</ul>\n<h3 id=\"I-INTRODUCTION\"><a href=\"#I-INTRODUCTION\" class=\"headerlink\" title=\"I. INTRODUCTION\"></a>I. INTRODUCTION</h3><p>随着电子商务和在线通信的爆炸式增长，在许多应用领域中都可以使用短文本，例如即时消息，在线聊天日志，公告板系统标题，Web日志评论，Internet新闻评论，SMS，Twitter等。因此，成功在许多Web和IR应用程序中处理它们变得越来越重要。 但是，对这些类型的文本和Web数据进行分类是一个新的挑战。</p>\n<p>与普通文档不同，这些文本和Web段通常更嘈杂，主题更少，并且更短，也就是说，它们由十几个单词到几个句子组成[1]。 由于篇幅短，它们无法提供足够的单词共现或共享语境，无法实现良好的相似度[40]。 因此，依赖于词频，足够的词共现或共享上下文来测量文档相似度的常规机器学习方法通常由于数据稀疏而无法达到期望的准确性。</p>\n<p>出现了针对短文本的新分类方法，例如语义分析，半监督短文本分类，集成短文本分类、实时分类。然而，与许多有关文本分类的评论和调查相比，只有很少的调查来讨论有关短文本分类的最新研究。 本文分析了与短文本分类相关的挑战，并系统地总结了使用分析方法对短文本分类进行分类的现有方法。</p>\n<p>在分析了短文本的特点和难点之后，我们在第二节中指出了短文本分类的过程。 第三节介绍了基于语义分析的短文本分类。 我们在第四节中描述了一些关于半监督短文本分类的算法。 第五节和第六节分别介绍了用于对短文本进行分类和在线短文本分类的集成模型。 我们在第七节中分析了相关的评估措施。 在第八节中，我们总结了对短文本进行分类的方法。</p>\n<h3 id=\"II-BACKGROUNDS\"><a href=\"#II-BACKGROUNDS\" class=\"headerlink\" title=\"II. BACKGROUNDS\"></a>II. BACKGROUNDS</h3><h4 id=\"A-Feature-of-Short-Text\"><a href=\"#A-Feature-of-Short-Text\" class=\"headerlink\" title=\"A. Feature of Short Text\"></a>A. Feature of Short Text</h4><p>通常，短文本的特征如下[1] [2]：</p>\n<p>稀疏性：简短的文本仅包含数个到十几个具有某些功能的单词，它不能提供足够的单词共现或共享上下文，无法提供良好的相似性度量。 很难提取其有效的语言功能。</p>\n<p>即时性：短文本会立即发送并实时接收。 另外，数量非常大。</p>\n<p>非标准性：简短的文字说明简洁明了，有许多拼写错误，非标准术语和噪音。</p>\n<p>噪音和分布不平衡：应用程序背景（例如网络安全性）需要处理大量的短文本数据。 但是，我们可能只关注大规模数据中的一小部分（检测对象）。 因此，有用的实例是有限的，并且短文本的分布是不平衡的。</p>\n<p>大型数据和标记瓶颈：手动标记所有大型实例很困难。 标签数量有限的实例只能提供有限的信息。 因此，如何充分利用这些标记的实例和其他未标记的实例已经成为短文本分类的关键问题。</p>\n<p>大多数传统方法（例如SVM，BAYES和KNN）都是基于术语频率的相似性，而忽略了短文本的特征。 这些传统方法可能无法处理短文本分类。 如果标记的信息不足，它们中的大多数（例如BAYES）可能无法获得高精度。 另外，一些基于向量空间模型（SVM）的分类方法应该使用语义信息来提高分类器的性能。</p>\n<h4 id=\"B-Short-Text-Classification\"><a href=\"#B-Short-Text-Classification\" class=\"headerlink\" title=\"B. Short Text Classification\"></a>B. Short Text Classification</h4><p>在过去的十年中，已经深入研究了对文本和Web文档进行分类的学习。 许多学习方法，例如k个最近邻（k-NN），朴素贝叶斯（Naive Bayes），最大熵和支持向量机（SVM），已应用于具有不同基准集合的许多分类问题，并取得了令人满意的结果。 但是，由于短文字的特点和难度，传统的分类方法不适合短文本分类。 因此，如何合理地表示和选择特征项，有效地减小空间尺寸和噪声，提高分类精度成为短文本分类的问题。</p>\n<h4 id=\"III-SHORT-TEXT-CLASSIFICATION-BASED-ON-SEMANTIC-ANALYSIS\"><a href=\"#III-SHORT-TEXT-CLASSIFICATION-BASED-ON-SEMANTIC-ANALYSIS\" class=\"headerlink\" title=\"III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS\"></a>III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS</h4><p>目前，减少特征空间维数的解决方案主要基于语义特征和语义分析。 这是因为文本分类的处理通常是在向量空间模型（VSM）中进行的，其基本假设是单词之间的关系是独立的，而忽略了文本之间的相关性。但是，短文本的语义表达能力较弱，需要这种相关性。 传统分类不能区分自然语言的歧义词和同义词，所有这些在短文本中都很丰富。 因此，传统的分类方法通常无法达到预期的短文本准确性。</p>\n<p>语义分析更加关注概念，内部结构的语义层次以及文本的相关性，从而获得了更具表现力和客观性的逻辑结构。 在现有研究中，基于潜在语义分析的分类占有重要地位。 使用统计方法，潜在语义分析可提取潜在的语义结构，消除同义词影响，并减少特征维数和噪声。因此，提出了许多基于语义分析的算法来处理短文本分类（更多详细信息见表I）[3] [5-11] [44]。  Zelikovitz [3]将其应用于短文本分类。  Pu强等[5]将LSA和独立成分分析（ICA）[8] [42]结合在一起。  Xuan-Hieu Phan等[7]建立了大规模的短文本分类框架。 该框架主要基于最近成功的潜在主题分析模型（例如pLSA和LDA）和强大的机器学习方法（例如最大熵和SVM）。王炳坤等[9]提出了一种新的方法，通过基于潜在狄利克雷分配（LDA）和信息增益（IG）模型构建强大的词库（SFT）来解决该问题。 当使用句法或语义信息时，提出了语言独立语义（LIS）内核[10]以增强语言依赖性。 它能够有效地计算短文本文档之间的相似度，而无需使用语法标记和词汇数据库。 陈梦根等[11]提出了一种以多种粒度提取主题的方法，该方法可以更精确地建模短文本。 转换式LSA [3] [4]是基于LSA的短文本分类的另一个示例。 转导利用测试示例来选择学习者的假设。 通过合并测试示例来重新创建空间确实会根据测试示例选择一种表示形式。 训练/测试集的维数减少使得较小的空间可以更准确地反映将应用于分类的测试集。 通过将测试示例包含到原始矩阵中，LSA可以计算单词与单词的熵权以及测试集中单词的示例和共现。</p>\n<h4 id=\"IV-SEMI-SUPERVISED-SHORT-TEXT-CLASSIFICATION\"><a href=\"#IV-SEMI-SUPERVISED-SHORT-TEXT-CLASSIFICATION\" class=\"headerlink\" title=\"IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION\"></a>IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION</h4><p>半监督学习是指使用标记和未标记的数据进行训练。 它对比了监督学习（所有数据都标记）或无监督学习（所有数据都未标记）。用于学习问题的标记数据的获取通常需要熟练的人工代理手动对训练示例进行分类。 因此，与标记过程相关的成本可能使完全标记的训练集变得不可行，而未标记数据的获取相对便宜。 在这种情况下，半监督学习可能具有很大的实用价值[27]。</p>\n<p>大多数半监督的短文本分类都是灵活的半监督学习。 它可以利用未标记的数据来改进分类器。 但是，与传统的半监督学习算法不同，通用数据和训练/测试数据不需要具有相同的格式。 此外，一旦进行了估计，只要主题模型是一致的，就可以将其应用于多个分类问题。</p>\n<p>背景知识可能会提供一个文本语料库，其中包含有关单词重要性和单词联合概率的信息[29] [6]。 我们可以结合使用背景和培训示例来标记新示例。</p>\n<p>V. ENSEMBLE SHORT TEXT CLASSIFICATION</p>\n<p>VI. REAL-TIME CLASSIFICATION OF LARGE SCALE SHORT TEXT</p>\n<p>即时性是短文本的另一个功能，这意味着短文本会立即发送并实时接收，通常数量非常大。 因此，如何立即对大规模的短文本数据进行分类也成为一个重要的问题。 目前，与几种经典分类算法相比，经常选择贝叶斯算法作为在线分类器。 朴素贝叶斯算法通过计算文本属于每个类别的概率来判断类别，这是一种简单，准确且广泛使用的算法[39]。</p>\n<h3 id=\"题目：短文本理解研究\"><a href=\"#题目：短文本理解研究\" class=\"headerlink\" title=\"题目：短文本理解研究\"></a>题目：短文本理解研究</h3><p>与长文本不同,短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.</p>\n<p>许多相关工作^[1-3]^证明,自动化的短文本分类需要依赖额外的知识.这些知识可以帮助机器充分挖掘短文本中词与词之间的联系,如语义相关性.例如,在英文查询“premiere Lincoln”中,“premiere”是一个重要的信息,表明“Lincoln”在这里指的是“电影”;同样,在“watch harry potter”中,正因为“watch”的出现,“harry potter”的含义可被鉴定为“电影”或“DVD”,而不是“书籍”.但是,这些关于词汇的知识(例如“watch”的对象通常是“电影”)并没有在短文本中明确表示出来,因而需要通过额外的知识源获取.</p>\n<p>根据所需知识源的属性,将短文本理解模型分为3类:</p>\n<ul>\n<li>隐性(implicit)语义模型</li>\n<li>半显性(semi-explicit)语义模型</li>\n<li>显性(explicit)语义模型</li>\n</ul>\n<p>其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本理解.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本理解.</p>\n<p>从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度</p>\n<h4 id=\"隐性-implicit-语义模型\"><a href=\"#隐性-implicit-语义模型\" class=\"headerlink\" title=\"隐性(implicit)语义模型\"></a>隐性(implicit)语义模型</h4><p>隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.</p>\n<ul>\n<li>LSA(latent semantic analysis)^[3]^</li>\n</ul>\n<p>LSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.</p>\n<ul>\n<li>超空间模拟语言模型^[4]^</li>\n</ul>\n<p>超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止<br>词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.</p>\n<ul>\n<li>神经网络语言模型</li>\n</ul>\n<p>近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不<br>同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).</p>\n<p>最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使<br>用目标词预测窗口中的每个语境词出现的概率.</p>\n<p>总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.</p>\n<ul>\n<li>段向量</li>\n</ul>\n<p>段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png\" alt=\"QQ图片20200630104534\"></p>\n<h4 id=\"半显性-semi-xpIicit-语义模型\"><a href=\"#半显性-semi-xpIicit-语义模型\" class=\"headerlink\" title=\"半显性(semi-xpIicit)语义模型\"></a>半显性(semi-xpIicit)语义模型</h4><p>半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic<br>models).</p>\n<p>LSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏<br>参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:</p>\n<p>$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$</p>\n<p>虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的    主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png\" alt=\"QQ图片20200630104534\"></p>\n<p>总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.</p>\n<h4 id=\"显性-explicit-语义模型\"><a href=\"#显性-explicit-语义模型\" class=\"headerlink\" title=\"显性(explicit)语义模型\"></a>显性(explicit)语义模型</h4><p>近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常<br>是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.</p>\n<p>1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每<br>个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.</p>\n<p>2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad”这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad”).为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而<br>计算$p(concept |word ,topic)$.</p>\n<h3 id=\"文献引用-1\"><a href=\"#文献引用-1\" class=\"headerlink\" title=\"文献引用\"></a>文献引用</h3><p>[1] Linmei, H., Yang, T., Shi, C., Ji, H., &amp; Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> (pp. 4823-4832).</p>\n<p>[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.</p>\n<p>[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.</p>\n<p>[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, &amp; computers, 1996, 28(2): 203-208.</p>\n<p>[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.</p>\n<p>[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p>\n<p>[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.</p>\n<p>[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.</p>\n<p>[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.</p>\n<p>[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e</p>\n<p>[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.</p>\n<p>[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.</p>\n<p>[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.</p>\n<h3 id=\"《基于深度学习的短文本分类研究综述-刘琴-袁家政-翁长虹》\"><a href=\"#《基于深度学习的短文本分类研究综述-刘琴-袁家政-翁长虹》\" class=\"headerlink\" title=\"《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》\"></a>《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》</h3><h4 id=\"基于机器学习\"><a href=\"#基于机器学习\" class=\"headerlink\" title=\"基于机器学习\"></a>基于机器学习</h4><ul>\n<li>支持向量机(Support Vector Machine, SVM)</li>\n<li>朴素贝叶斯分类法</li>\n<li>K-最近邻法</li>\n<li>决策树法(Decision Tree, DT)</li>\n<li>中心向量法 </li>\n</ul>\n<h4 id=\"基于深度学习\"><a href=\"#基于深度学习\" class=\"headerlink\" title=\"基于深度学习\"></a>基于深度学习</h4><ul>\n<li><p>CNN+语义约束（Short text classification improved by learning multi-granularity topics-AAAI2011）</p>\n</li>\n<li><p>CNN+语义聚类（Semantic clustering and convolutional neural network for short text categorization-ACL2015）</p>\n</li>\n<li>CNN+RNN+顺序短文本分类（Sequential short-text classification with recurrent and convolutional neural networks-arxiv2016）</li>\n<li>DBN</li>\n</ul>\n<h3 id=\"标准划分\"><a href=\"#标准划分\" class=\"headerlink\" title=\"标准划分\"></a>标准划分</h3><h4 id=\"传统短文本分类算法-1\"><a href=\"#传统短文本分类算法-1\" class=\"headerlink\" title=\"传统短文本分类算法\"></a>传统短文本分类算法</h4><p>基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模</p>\n<p>传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响</p>\n<p>伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典方法，整个文本分类问题就拆分成了特征工程和分类器两部分。</p>\n<ul>\n<li>特征工程</li>\n</ul>\n<p>特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据—&gt;信息”的过程，决定了结果的上限，而分类器是“信息—&gt;知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。</p>\n<p>文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统文本分类任务大部分工作也在此处。文本特征工程分为文本预处理、特征提取、文本表示三个部分，目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。</p>\n<p>（1）文本预处理</p>\n<p>文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。</p>\n<p>具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的冠词助词代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。</p>\n<p>（2）文本表示和特征提取</p>\n<p>文本表示：</p>\n<p>文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。</p>\n<p>特征提取：</p>\n<p>向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。</p>\n<p>特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。</p>\n<p>信息增益特征提取方法：该方法基于信息熵的原理，通过计算各个特征在不同类别样本中的分布情况，通过统计比较发现具有较高类别区分度的特征。信息增益能较好的描述不同特征对不同类别的区别能力，信息增益值越高则区别能力越强，在特征选择时保留较大IG 值，在分类时就更容易得到正确的分类结果，而且能降低特征的维度。</p>\n<p>卡方检验特征选择方法：该方法主要基于相关性检验原理，通过计算某属性与某一类的相关性，来衡量特征候选项项的重要性，该方法考虑到了反相关的属性。在这里反相关的特征候选项同样存在有用信息，反相关的信息量对分类和聚类算法都有积极作用。</p>\n<p>特征权重主要是经典的TF-IDF及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。</p>\n<p>（3）特征扩展</p>\n<p>由于短文本的特征少，固需要对文本向量进行特征扩展。当前对于短文本信息分类的研究主要聚焦于短文特征的扩展问题之上。特征扩展方法有基于WordNet、Wik ipedia或其他知识库等的特征扩展方法，然而这些方法来说一些的不足之处，对于短文本数据，尤其是即时数据来说，这些方法并不十分合适，因为这些知识库依靠的更多是人工参与编辑，信息相对滞后，扩展特征不够全面，缺乏全局性与一致性。另外一个更重要的原因是网络信息检索本身就会出现噪声问题。这些都会将错误因素累积，所以必须对检索结果进一步筛选。</p>\n<p>为了解决这些问题，一些研究人员通过重新计算短文本与检索结果的相似度，验证并实现了多种方法，包括语义分析等。短文本计算相似度时有一定得困难，短文本包含的特征太少，若直接使用相同特征作为度量标准的话，很多短文本之间的相似度为零。为了解决这个问题，需要采用一种适合短文本相似度计算的方法，常见的相似度计算方法有：1．基于本体词典的规则化方法；2．基于知识库的规则化方法；3．基于搜索引擎的方法；4．基于统计的方法；5．句子层面语义相似度计算方法。</p>\n<p>（4）基于语义的文本表示</p>\n<p>传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础。</p>\n<ul>\n<li>分类器</li>\n</ul>\n<p>分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等。</p>\n<h3 id=\"深度学习文本分类方法\"><a href=\"#深度学习文本分类方法\" class=\"headerlink\" title=\"深度学习文本分类方法\"></a>深度学习文本分类方法</h3><p>传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。</p>\n<p>短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息</p>\n<p>与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。</p>\n<p>一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":"<h3 id=\"什么是短文本\"><a href=\"#什么是短文本\" class=\"headerlink\" title=\"什么是短文本\"></a>什么是短文本</h3><p>短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常词汇个数少且描述信息弱，具有稀疏性和歧义且通常标签数据较少。</p>\n<h3 id=\"传统短文本分类算法\"><a href=\"#传统短文本分类算法\" class=\"headerlink\" title=\"传统短文本分类算法\"></a>传统短文本分类算法</h3><p>基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模</p>\n<p>传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响</p>\n<h3 id=\"基于深度学习的短文本分类算法\"><a href=\"#基于深度学习的短文本分类算法\" class=\"headerlink\" title=\"基于深度学习的短文本分类算法\"></a>基于深度学习的短文本分类算法</h3><p>短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息</p>\n<p>与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。</p>\n<p>一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。</p>\n<p>文本分类模型如下：</p>\n<p>​    1） FastText</p>\n<p>FastText是Facebook开源的词向量与文本分类工具，模型简单，训练速度快。FastText 的原理是将短文本中的所有词向量进行平均，然后直接接softmax层，同时加入一些n-gram 特征的 trick 来捕获局部序列信息。相对于其它文本分类模型，如SVM，Logistic Regression和Neural Network等模型，FastText在保持分类效果的同时，大大缩短了训练时间，同时支持多语言表达，但其模型是基于词袋针对英文的文本分类方法，组成英文句子的单词是有间隔的，而应用于中文文本，需分词去标点转化为模型需要的数据格式。</p>\n<p>​    2）TextCNN</p>\n<p>TextCNN相比于FastText，利用CNN (Convolutional Neural Network）来提取句子中类似 n-gram 的关键信息，且结构简单，效果好。</p>\n<p>​    3）TextRNN</p>\n<p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN最大的问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p>\n<p>4）TextRNN + Attention</p>\n<p>CNN和RNN用在文本分类任务中尽管效果显著，但都有一个缺点，直观性和可解释性差。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够直观的给出每个词对结果的贡献，是Seq2Seq模型的标配。实际上文本分类从某种意义上也、可以理解为一种特殊的Seq2Seq，所以可以考虑将Attention机制引入。</p>\n<p>Attention的核心点是在翻译每个目标词（或预测商品标题文本所属类别）所用的上下文是不同的，这样更合理。加入Attention之后能够直观的解释各个句子和词对分类类别的重要性。</p>\n<p>5）TextRCNN（TextRNN + CNN）</p>\n<p>用前向和后向RNN得到每个词的前向和后向上下文的表示，这样词的表示就变成词向量和前向后向上下文向量concat起来的形式，最后连接TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1。</p>\n<h3 id=\"短文本分类应用场景\"><a href=\"#短文本分类应用场景\" class=\"headerlink\" title=\"短文本分类应用场景\"></a>短文本分类应用场景</h3><p>短文本分类算法广泛应用于各个行业领域,如新闻分类、人机写作判断、垃圾邮件识别、用户情感分类、文案智能生成、商品智能推荐等。</p>\n<p>场景一：商品智能推荐，根据用户购买的商品名称作为预测样本进行文本分类，得到用户交易类别，结合其他数据构建用户画像，针对不同特征的用户画像预测用户下一步的购买行为，智能推荐商品及服务。</p>\n<p>场景二：文案智能生成，基于优质文案作为训练集，得到文本分类模型，当用户输入关键词时，智能推荐适配文案。</p>\n<p>场景三：给新闻自动分类或打标签，多个标签。</p>\n<p>场景四：判断文章是人写还是机器写的。</p>\n<p>场景五：判断影评中的情感是正向、负向、中立，相类似应用场景很</p>\n<h3 id=\"文献引用\"><a href=\"#文献引用\" class=\"headerlink\" title=\"文献引用\"></a>文献引用</h3><p>[1] Linmei, H., Yang, T., Shi, C., Ji, H., &amp; Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> (pp. 4823-4832).</p>\n<p>[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.</p>\n<p>[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.</p>\n<p>[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, &amp; computers, 1996, 28(2): 203-208.</p>\n<p>[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.</p>\n<p>[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p>\n<p>[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.</p>\n<p>[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.</p>\n<h2 id=\"论文阅读\"><a href=\"#论文阅读\" class=\"headerlink\" title=\"论文阅读\"></a>论文阅读</h2><h3 id=\"题目：Short-Text-Classification-A-Survey\"><a href=\"#题目：Short-Text-Classification-A-Survey\" class=\"headerlink\" title=\"题目：Short Text Classification: A Survey\"></a>题目：Short Text Classification: A Survey</h3><p>分类方式：</p>\n<ul>\n<li>short text classification using sematic analysis（语义分析）</li>\n<li>semi-supervised short text classification（半监督）</li>\n<li>ensemble short text classification（集成学习）</li>\n<li>real-time classification.</li>\n</ul>\n<h3 id=\"I-INTRODUCTION\"><a href=\"#I-INTRODUCTION\" class=\"headerlink\" title=\"I. INTRODUCTION\"></a>I. INTRODUCTION</h3><p>随着电子商务和在线通信的爆炸式增长，在许多应用领域中都可以使用短文本，例如即时消息，在线聊天日志，公告板系统标题，Web日志评论，Internet新闻评论，SMS，Twitter等。因此，成功在许多Web和IR应用程序中处理它们变得越来越重要。 但是，对这些类型的文本和Web数据进行分类是一个新的挑战。</p>\n<p>与普通文档不同，这些文本和Web段通常更嘈杂，主题更少，并且更短，也就是说，它们由十几个单词到几个句子组成[1]。 由于篇幅短，它们无法提供足够的单词共现或共享语境，无法实现良好的相似度[40]。 因此，依赖于词频，足够的词共现或共享上下文来测量文档相似度的常规机器学习方法通常由于数据稀疏而无法达到期望的准确性。</p>\n<p>出现了针对短文本的新分类方法，例如语义分析，半监督短文本分类，集成短文本分类、实时分类。然而，与许多有关文本分类的评论和调查相比，只有很少的调查来讨论有关短文本分类的最新研究。 本文分析了与短文本分类相关的挑战，并系统地总结了使用分析方法对短文本分类进行分类的现有方法。</p>\n<p>在分析了短文本的特点和难点之后，我们在第二节中指出了短文本分类的过程。 第三节介绍了基于语义分析的短文本分类。 我们在第四节中描述了一些关于半监督短文本分类的算法。 第五节和第六节分别介绍了用于对短文本进行分类和在线短文本分类的集成模型。 我们在第七节中分析了相关的评估措施。 在第八节中，我们总结了对短文本进行分类的方法。</p>\n<h3 id=\"II-BACKGROUNDS\"><a href=\"#II-BACKGROUNDS\" class=\"headerlink\" title=\"II. BACKGROUNDS\"></a>II. BACKGROUNDS</h3><h4 id=\"A-Feature-of-Short-Text\"><a href=\"#A-Feature-of-Short-Text\" class=\"headerlink\" title=\"A. Feature of Short Text\"></a>A. Feature of Short Text</h4><p>通常，短文本的特征如下[1] [2]：</p>\n<p>稀疏性：简短的文本仅包含数个到十几个具有某些功能的单词，它不能提供足够的单词共现或共享上下文，无法提供良好的相似性度量。 很难提取其有效的语言功能。</p>\n<p>即时性：短文本会立即发送并实时接收。 另外，数量非常大。</p>\n<p>非标准性：简短的文字说明简洁明了，有许多拼写错误，非标准术语和噪音。</p>\n<p>噪音和分布不平衡：应用程序背景（例如网络安全性）需要处理大量的短文本数据。 但是，我们可能只关注大规模数据中的一小部分（检测对象）。 因此，有用的实例是有限的，并且短文本的分布是不平衡的。</p>\n<p>大型数据和标记瓶颈：手动标记所有大型实例很困难。 标签数量有限的实例只能提供有限的信息。 因此，如何充分利用这些标记的实例和其他未标记的实例已经成为短文本分类的关键问题。</p>\n<p>大多数传统方法（例如SVM，BAYES和KNN）都是基于术语频率的相似性，而忽略了短文本的特征。 这些传统方法可能无法处理短文本分类。 如果标记的信息不足，它们中的大多数（例如BAYES）可能无法获得高精度。 另外，一些基于向量空间模型（SVM）的分类方法应该使用语义信息来提高分类器的性能。</p>\n<h4 id=\"B-Short-Text-Classification\"><a href=\"#B-Short-Text-Classification\" class=\"headerlink\" title=\"B. Short Text Classification\"></a>B. Short Text Classification</h4><p>在过去的十年中，已经深入研究了对文本和Web文档进行分类的学习。 许多学习方法，例如k个最近邻（k-NN），朴素贝叶斯（Naive Bayes），最大熵和支持向量机（SVM），已应用于具有不同基准集合的许多分类问题，并取得了令人满意的结果。 但是，由于短文字的特点和难度，传统的分类方法不适合短文本分类。 因此，如何合理地表示和选择特征项，有效地减小空间尺寸和噪声，提高分类精度成为短文本分类的问题。</p>\n<h4 id=\"III-SHORT-TEXT-CLASSIFICATION-BASED-ON-SEMANTIC-ANALYSIS\"><a href=\"#III-SHORT-TEXT-CLASSIFICATION-BASED-ON-SEMANTIC-ANALYSIS\" class=\"headerlink\" title=\"III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS\"></a>III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS</h4><p>目前，减少特征空间维数的解决方案主要基于语义特征和语义分析。 这是因为文本分类的处理通常是在向量空间模型（VSM）中进行的，其基本假设是单词之间的关系是独立的，而忽略了文本之间的相关性。但是，短文本的语义表达能力较弱，需要这种相关性。 传统分类不能区分自然语言的歧义词和同义词，所有这些在短文本中都很丰富。 因此，传统的分类方法通常无法达到预期的短文本准确性。</p>\n<p>语义分析更加关注概念，内部结构的语义层次以及文本的相关性，从而获得了更具表现力和客观性的逻辑结构。 在现有研究中，基于潜在语义分析的分类占有重要地位。 使用统计方法，潜在语义分析可提取潜在的语义结构，消除同义词影响，并减少特征维数和噪声。因此，提出了许多基于语义分析的算法来处理短文本分类（更多详细信息见表I）[3] [5-11] [44]。  Zelikovitz [3]将其应用于短文本分类。  Pu强等[5]将LSA和独立成分分析（ICA）[8] [42]结合在一起。  Xuan-Hieu Phan等[7]建立了大规模的短文本分类框架。 该框架主要基于最近成功的潜在主题分析模型（例如pLSA和LDA）和强大的机器学习方法（例如最大熵和SVM）。王炳坤等[9]提出了一种新的方法，通过基于潜在狄利克雷分配（LDA）和信息增益（IG）模型构建强大的词库（SFT）来解决该问题。 当使用句法或语义信息时，提出了语言独立语义（LIS）内核[10]以增强语言依赖性。 它能够有效地计算短文本文档之间的相似度，而无需使用语法标记和词汇数据库。 陈梦根等[11]提出了一种以多种粒度提取主题的方法，该方法可以更精确地建模短文本。 转换式LSA [3] [4]是基于LSA的短文本分类的另一个示例。 转导利用测试示例来选择学习者的假设。 通过合并测试示例来重新创建空间确实会根据测试示例选择一种表示形式。 训练/测试集的维数减少使得较小的空间可以更准确地反映将应用于分类的测试集。 通过将测试示例包含到原始矩阵中，LSA可以计算单词与单词的熵权以及测试集中单词的示例和共现。</p>\n<h4 id=\"IV-SEMI-SUPERVISED-SHORT-TEXT-CLASSIFICATION\"><a href=\"#IV-SEMI-SUPERVISED-SHORT-TEXT-CLASSIFICATION\" class=\"headerlink\" title=\"IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION\"></a>IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION</h4><p>半监督学习是指使用标记和未标记的数据进行训练。 它对比了监督学习（所有数据都标记）或无监督学习（所有数据都未标记）。用于学习问题的标记数据的获取通常需要熟练的人工代理手动对训练示例进行分类。 因此，与标记过程相关的成本可能使完全标记的训练集变得不可行，而未标记数据的获取相对便宜。 在这种情况下，半监督学习可能具有很大的实用价值[27]。</p>\n<p>大多数半监督的短文本分类都是灵活的半监督学习。 它可以利用未标记的数据来改进分类器。 但是，与传统的半监督学习算法不同，通用数据和训练/测试数据不需要具有相同的格式。 此外，一旦进行了估计，只要主题模型是一致的，就可以将其应用于多个分类问题。</p>\n<p>背景知识可能会提供一个文本语料库，其中包含有关单词重要性和单词联合概率的信息[29] [6]。 我们可以结合使用背景和培训示例来标记新示例。</p>\n<p>V. ENSEMBLE SHORT TEXT CLASSIFICATION</p>\n<p>VI. REAL-TIME CLASSIFICATION OF LARGE SCALE SHORT TEXT</p>\n<p>即时性是短文本的另一个功能，这意味着短文本会立即发送并实时接收，通常数量非常大。 因此，如何立即对大规模的短文本数据进行分类也成为一个重要的问题。 目前，与几种经典分类算法相比，经常选择贝叶斯算法作为在线分类器。 朴素贝叶斯算法通过计算文本属于每个类别的概率来判断类别，这是一种简单，准确且广泛使用的算法[39]。</p>\n<h3 id=\"题目：短文本理解研究\"><a href=\"#题目：短文本理解研究\" class=\"headerlink\" title=\"题目：短文本理解研究\"></a>题目：短文本理解研究</h3><p>与长文本不同,短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.</p>\n<p>许多相关工作^[1-3]^证明,自动化的短文本分类需要依赖额外的知识.这些知识可以帮助机器充分挖掘短文本中词与词之间的联系,如语义相关性.例如,在英文查询“premiere Lincoln”中,“premiere”是一个重要的信息,表明“Lincoln”在这里指的是“电影”;同样,在“watch harry potter”中,正因为“watch”的出现,“harry potter”的含义可被鉴定为“电影”或“DVD”,而不是“书籍”.但是,这些关于词汇的知识(例如“watch”的对象通常是“电影”)并没有在短文本中明确表示出来,因而需要通过额外的知识源获取.</p>\n<p>根据所需知识源的属性,将短文本理解模型分为3类:</p>\n<ul>\n<li>隐性(implicit)语义模型</li>\n<li>半显性(semi-explicit)语义模型</li>\n<li>显性(explicit)语义模型</li>\n</ul>\n<p>其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本理解.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本理解.</p>\n<p>从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度</p>\n<h4 id=\"隐性-implicit-语义模型\"><a href=\"#隐性-implicit-语义模型\" class=\"headerlink\" title=\"隐性(implicit)语义模型\"></a>隐性(implicit)语义模型</h4><p>隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.</p>\n<ul>\n<li>LSA(latent semantic analysis)^[3]^</li>\n</ul>\n<p>LSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.</p>\n<ul>\n<li>超空间模拟语言模型^[4]^</li>\n</ul>\n<p>超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止<br>词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.</p>\n<ul>\n<li>神经网络语言模型</li>\n</ul>\n<p>近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不<br>同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).</p>\n<p>最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使<br>用目标词预测窗口中的每个语境词出现的概率.</p>\n<p>总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.</p>\n<ul>\n<li>段向量</li>\n</ul>\n<p>段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png\" alt=\"QQ图片20200630104534\"></p>\n<h4 id=\"半显性-semi-xpIicit-语义模型\"><a href=\"#半显性-semi-xpIicit-语义模型\" class=\"headerlink\" title=\"半显性(semi-xpIicit)语义模型\"></a>半显性(semi-xpIicit)语义模型</h4><p>半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic<br>models).</p>\n<p>LSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏<br>参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:</p>\n<p>$p(d, w)=p(d) \\sum_{x \\in Z} p(w \\mid z) p(z \\mid d)$</p>\n<p>虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的    主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).</p>\n<p><img src=\"/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png\" alt=\"QQ图片20200630104534\"></p>\n<p>总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.</p>\n<h4 id=\"显性-explicit-语义模型\"><a href=\"#显性-explicit-语义模型\" class=\"headerlink\" title=\"显性(explicit)语义模型\"></a>显性(explicit)语义模型</h4><p>近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常<br>是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.</p>\n<p>1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每<br>个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.</p>\n<p>2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad”这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad”).为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而<br>计算$p(concept |word ,topic)$.</p>\n<h3 id=\"文献引用-1\"><a href=\"#文献引用-1\" class=\"headerlink\" title=\"文献引用\"></a>文献引用</h3><p>[1] Linmei, H., Yang, T., Shi, C., Ji, H., &amp; Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> (pp. 4823-4832).</p>\n<p>[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.</p>\n<p>[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.</p>\n<p>[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, &amp; computers, 1996, 28(2): 203-208.</p>\n<p>[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.</p>\n<p>[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p>\n<p>[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.</p>\n<p>[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.</p>\n<p>[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.</p>\n<p>[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e</p>\n<p>[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.</p>\n<p>[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.</p>\n<p>[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.</p>\n<h3 id=\"《基于深度学习的短文本分类研究综述-刘琴-袁家政-翁长虹》\"><a href=\"#《基于深度学习的短文本分类研究综述-刘琴-袁家政-翁长虹》\" class=\"headerlink\" title=\"《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》\"></a>《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》</h3><h4 id=\"基于机器学习\"><a href=\"#基于机器学习\" class=\"headerlink\" title=\"基于机器学习\"></a>基于机器学习</h4><ul>\n<li>支持向量机(Support Vector Machine, SVM)</li>\n<li>朴素贝叶斯分类法</li>\n<li>K-最近邻法</li>\n<li>决策树法(Decision Tree, DT)</li>\n<li>中心向量法 </li>\n</ul>\n<h4 id=\"基于深度学习\"><a href=\"#基于深度学习\" class=\"headerlink\" title=\"基于深度学习\"></a>基于深度学习</h4><ul>\n<li><p>CNN+语义约束（Short text classification improved by learning multi-granularity topics-AAAI2011）</p>\n</li>\n<li><p>CNN+语义聚类（Semantic clustering and convolutional neural network for short text categorization-ACL2015）</p>\n</li>\n<li>CNN+RNN+顺序短文本分类（Sequential short-text classification with recurrent and convolutional neural networks-arxiv2016）</li>\n<li>DBN</li>\n</ul>\n<h3 id=\"标准划分\"><a href=\"#标准划分\" class=\"headerlink\" title=\"标准划分\"></a>标准划分</h3><h4 id=\"传统短文本分类算法-1\"><a href=\"#传统短文本分类算法-1\" class=\"headerlink\" title=\"传统短文本分类算法\"></a>传统短文本分类算法</h4><p>基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模</p>\n<p>传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响</p>\n<p>伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典方法，整个文本分类问题就拆分成了特征工程和分类器两部分。</p>\n<ul>\n<li>特征工程</li>\n</ul>\n<p>特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据—&gt;信息”的过程，决定了结果的上限，而分类器是“信息—&gt;知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。</p>\n<p>文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统文本分类任务大部分工作也在此处。文本特征工程分为文本预处理、特征提取、文本表示三个部分，目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。</p>\n<p>（1）文本预处理</p>\n<p>文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。</p>\n<p>具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的冠词助词代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。</p>\n<p>（2）文本表示和特征提取</p>\n<p>文本表示：</p>\n<p>文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。</p>\n<p>特征提取：</p>\n<p>向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。</p>\n<p>特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。</p>\n<p>信息增益特征提取方法：该方法基于信息熵的原理，通过计算各个特征在不同类别样本中的分布情况，通过统计比较发现具有较高类别区分度的特征。信息增益能较好的描述不同特征对不同类别的区别能力，信息增益值越高则区别能力越强，在特征选择时保留较大IG 值，在分类时就更容易得到正确的分类结果，而且能降低特征的维度。</p>\n<p>卡方检验特征选择方法：该方法主要基于相关性检验原理，通过计算某属性与某一类的相关性，来衡量特征候选项项的重要性，该方法考虑到了反相关的属性。在这里反相关的特征候选项同样存在有用信息，反相关的信息量对分类和聚类算法都有积极作用。</p>\n<p>特征权重主要是经典的TF-IDF及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。</p>\n<p>（3）特征扩展</p>\n<p>由于短文本的特征少，固需要对文本向量进行特征扩展。当前对于短文本信息分类的研究主要聚焦于短文特征的扩展问题之上。特征扩展方法有基于WordNet、Wik ipedia或其他知识库等的特征扩展方法，然而这些方法来说一些的不足之处，对于短文本数据，尤其是即时数据来说，这些方法并不十分合适，因为这些知识库依靠的更多是人工参与编辑，信息相对滞后，扩展特征不够全面，缺乏全局性与一致性。另外一个更重要的原因是网络信息检索本身就会出现噪声问题。这些都会将错误因素累积，所以必须对检索结果进一步筛选。</p>\n<p>为了解决这些问题，一些研究人员通过重新计算短文本与检索结果的相似度，验证并实现了多种方法，包括语义分析等。短文本计算相似度时有一定得困难，短文本包含的特征太少，若直接使用相同特征作为度量标准的话，很多短文本之间的相似度为零。为了解决这个问题，需要采用一种适合短文本相似度计算的方法，常见的相似度计算方法有：1．基于本体词典的规则化方法；2．基于知识库的规则化方法；3．基于搜索引擎的方法；4．基于统计的方法；5．句子层面语义相似度计算方法。</p>\n<p>（4）基于语义的文本表示</p>\n<p>传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础。</p>\n<ul>\n<li>分类器</li>\n</ul>\n<p>分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等。</p>\n<h3 id=\"深度学习文本分类方法\"><a href=\"#深度学习文本分类方法\" class=\"headerlink\" title=\"深度学习文本分类方法\"></a>深度学习文本分类方法</h3><p>传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。</p>\n<p>短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息</p>\n<p>与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。</p>\n<p>一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。</p>\n"},{"title":"胶囊网络","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2020-05-01T08:32:06.000Z","password":null,"img":null,"summary":null,"keywords":"胶囊网络","_content":"\n","source":"_posts/胶囊网络.md","raw":"---\ntitle: 胶囊网络\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2020-05-01 16:32:06\npassword:\nimg:\nsummary:\ncategories: 深度学习\nkeywords: 胶囊网络\ntags:\n\t- 胶囊网络\n\t- Capsule Networks\n---\n\n","slug":"胶囊网络","published":1,"updated":"2020-08-20T08:41:44.656Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2l0mbn001jewse3z3u59uv","content":"<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{"musics":[{"name":"夜曲","artist":"周杰伦","url":"/medias/music/yequ.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"一路向北","artist":"周杰伦","url":"/medias/music/yiluxiangbei.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"来自天堂的魔鬼","artist":"邓紫棋","url":"/medias/music/tiantangdemogui.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"倒数","artist":"邓紫棋","url":"/medias/music/daoshu.mp3","cover":"/medias/music/avatars/daoshu.jpg"}],"friends":[{"name":"AntNLP","url":"https://antnlp.org","title":"访问主页","introduction":"华东师范大学自然语言处理实验室欢迎您的加入！","avatar":"/medias/avatars/antnlp.ico"}]}},"excerpt":"","more":""}],"PostAsset":[],"PostCategory":[{"post_id":"cke2l0m8j0000ewsea6el4vln","category_id":"cke2l0m930004ewse0yzb4qvf","_id":"cke2l0mac000iewsea0ai28os"},{"post_id":"cke2l0m9u000aewse7l1a1qls","category_id":"cke2l0m930004ewse0yzb4qvf","_id":"cke2l0maf000lewseb4wafn9f"},{"post_id":"cke2l0m8y0002ewse815g51qf","category_id":"cke2l0m930004ewse0yzb4qvf","_id":"cke2l0mah000oewse1n7w7xpj"},{"post_id":"cke2l0ma8000gewseb968a02x","category_id":"cke2l0m930004ewse0yzb4qvf","_id":"cke2l0man000tewse8lreg31x"},{"post_id":"cke2l0m990006ewse38npgmlw","category_id":"cke2l0mab000hewse8ggsgbx6","_id":"cke2l0map000vewse8yemccti"},{"post_id":"cke2l0m9o0008ewseg0f501qt","category_id":"cke2l0mai000qewse4i0n6rex","_id":"cke2l0maw0010ewse56u61v0g"},{"post_id":"cke2l0ma5000eewse9uz66zua","category_id":"cke2l0map000wewseg16n6iw3","_id":"cke2l0mb30015ewse9w9kd8tz"},{"post_id":"cke2l0mae000kewsebf9v0vqr","category_id":"cke2l0maw0011ewseeuabdidi","_id":"cke2l0mbg001dewsed2ch2g3w"},{"post_id":"cke2l0mag000newsegiov5d5k","category_id":"cke2l0mb80017ewse9ljse8h1","_id":"cke2l0mbp001kewseh0ox3bat"},{"post_id":"cke2l0mal000sewse9td0ekj0","category_id":"cke2l0mbh001eewse0h5xen10","_id":"cke2l0mbr001newseg78n5m69"},{"post_id":"cke2l0man000uewse1x9fftgd","category_id":"cke2l0mbp001lewsebov892xq","_id":"cke2l0mbt001qewse3fqx0nkj"},{"post_id":"cke2l0mar000yewse6tcgauhj","category_id":"cke2l0mbr001oewse0h6egk25","_id":"cke2l0mbw001vewsedi639q3n"},{"post_id":"cke2l0mau000zewsecikha55f","category_id":"cke2l0mbt001rewsecxnsfwcn","_id":"cke2l0mc00020ewsea7zs181f"},{"post_id":"cke2l0max0012ewse39pub8qw","category_id":"cke2l0mbw001wewse8w73atm4","_id":"cke2l0mc10024ewse7mwo4tyl"},{"post_id":"cke2l0mb10014ewsegmtfaa0r","category_id":"cke2l0mc00022ewsebe85g7oj","_id":"cke2l0mc20027ewse92kw4ctl"},{"post_id":"cke2l0mb30016ewse3dm9cqym","category_id":"cke2l0mc20026ewse0s8u663j","_id":"cke2l0mc5002bewse6hddd7ns"},{"post_id":"cke2l0mba001aewse845dajsg","category_id":"cke2l0mc30029ewsefi7ad7ug","_id":"cke2l0mc6002gewsee5996vzv"},{"post_id":"cke2l0mbj001hewse85r9ho8r","category_id":"cke2l0mc5002cewsefdv7f9et","_id":"cke2l0mc8002lewse0vuqf1kc"},{"post_id":"cke2l0mbn001jewse3z3u59uv","category_id":"cke2l0mc6002hewse7ggsf34c","_id":"cke2l0mca002pewse8p1cbkgo"}],"PostTag":[{"post_id":"cke2l0m8j0000ewsea6el4vln","tag_id":"cke2l0m970005ewse8ll93va1","_id":"cke2l0maf000mewsehtwb6ult"},{"post_id":"cke2l0m8j0000ewsea6el4vln","tag_id":"cke2l0m9y000cewsefzb624ul","_id":"cke2l0mah000pewse1a7jhwnv"},{"post_id":"cke2l0m8y0002ewse815g51qf","tag_id":"cke2l0mad000jewse62cv46rx","_id":"cke2l0mba0019ewse0sae5dhc"},{"post_id":"cke2l0m8y0002ewse815g51qf","tag_id":"cke2l0mak000rewsea91fb4ey","_id":"cke2l0mbd001bewse5z7o1fjo"},{"post_id":"cke2l0m8y0002ewse815g51qf","tag_id":"cke2l0maq000xewse1jta4ect","_id":"cke2l0mbj001gewsedcpw3gp5"},{"post_id":"cke2l0m8y0002ewse815g51qf","tag_id":"cke2l0m9y000cewsefzb624ul","_id":"cke2l0mbm001iewsed6g45eav"},{"post_id":"cke2l0m990006ewse38npgmlw","tag_id":"cke2l0mb90018ewse4daqf7i9","_id":"cke2l0mbv001tewsedhsu1fwe"},{"post_id":"cke2l0m990006ewse38npgmlw","tag_id":"cke2l0mbi001fewse4r7c07qc","_id":"cke2l0mbv001uewse8fa5dqd9"},{"post_id":"cke2l0m990006ewse38npgmlw","tag_id":"cke2l0mbq001mewse6njw9ite","_id":"cke2l0mby001yewse408f487y"},{"post_id":"cke2l0m990006ewse38npgmlw","tag_id":"cke2l0mbs001pewse6qjgebpg","_id":"cke2l0mbz001zewse8vgpfnrh"},{"post_id":"cke2l0m9o0008ewseg0f501qt","tag_id":"cke2l0mbu001sewseawvtatco","_id":"cke2l0mc10023ewseh4aw21t4"},{"post_id":"cke2l0m9u000aewse7l1a1qls","tag_id":"cke2l0maq000xewse1jta4ect","_id":"cke2l0mc5002dewse3h5watmh"},{"post_id":"cke2l0m9u000aewse7l1a1qls","tag_id":"cke2l0mc00021ewse6o80enlb","_id":"cke2l0mc5002eewse8safa1s8"},{"post_id":"cke2l0m9u000aewse7l1a1qls","tag_id":"cke2l0mc20025ewse3z4lbm88","_id":"cke2l0mc7002iewse0t3y6ior"},{"post_id":"cke2l0m9u000aewse7l1a1qls","tag_id":"cke2l0m9y000cewsefzb624ul","_id":"cke2l0mc7002jewse0tf7byto"},{"post_id":"cke2l0ma5000eewse9uz66zua","tag_id":"cke2l0mc4002aewse7f8j893e","_id":"cke2l0mc9002mewsehh6n2h4a"},{"post_id":"cke2l0ma5000eewse9uz66zua","tag_id":"cke2l0mc6002fewse0uh7aoq4","_id":"cke2l0mc9002newse4z8x6jxs"},{"post_id":"cke2l0ma8000gewseb968a02x","tag_id":"cke2l0maq000xewse1jta4ect","_id":"cke2l0mcb002rewse472je47w"},{"post_id":"cke2l0ma8000gewseb968a02x","tag_id":"cke2l0mc9002oewse2m47dgl2","_id":"cke2l0mcb002sewseeboqb45p"},{"post_id":"cke2l0mae000kewsebf9v0vqr","tag_id":"cke2l0mca002qewse24ks7kvh","_id":"cke2l0mce002wewsec4s5aphb"},{"post_id":"cke2l0mae000kewsebf9v0vqr","tag_id":"cke2l0mcb002tewse3b7bhm5x","_id":"cke2l0mce002xewsegdydbg7q"},{"post_id":"cke2l0mae000kewsebf9v0vqr","tag_id":"cke2l0mcc002uewsehic4euff","_id":"cke2l0mcg002zewsebnuj1fgf"},{"post_id":"cke2l0mag000newsegiov5d5k","tag_id":"cke2l0mcd002vewse5q7k2vwy","_id":"cke2l0mcg0030ewse5xdvhmo9"},{"post_id":"cke2l0mal000sewse9td0ekj0","tag_id":"cke2l0mbq001mewse6njw9ite","_id":"cke2l0mck0034ewse7h2ecz3l"},{"post_id":"cke2l0mal000sewse9td0ekj0","tag_id":"cke2l0mcg0031ewsec76x107r","_id":"cke2l0mck0035ewse04bsg8zx"},{"post_id":"cke2l0mal000sewse9td0ekj0","tag_id":"cke2l0mci0032ewse2tik5bjc","_id":"cke2l0mcl0037ewsed2vxciup"},{"post_id":"cke2l0man000uewse1x9fftgd","tag_id":"cke2l0mcj0033ewse84cdcvcm","_id":"cke2l0mcl0038ewse62m9fyet"},{"post_id":"cke2l0mar000yewse6tcgauhj","tag_id":"cke2l0mck0036ewsee1h23yty","_id":"cke2l0mcn003aewse10t34lrr"},{"post_id":"cke2l0mau000zewsecikha55f","tag_id":"cke2l0mcm0039ewse6aq4fuzb","_id":"cke2l0mcp003dewse7xwn3c9c"},{"post_id":"cke2l0mau000zewsecikha55f","tag_id":"cke2l0mcn003bewsehdqi2b4h","_id":"cke2l0mcp003eewseg0w01b3j"},{"post_id":"cke2l0max0012ewse39pub8qw","tag_id":"cke2l0mco003cewse2nerbvle","_id":"cke2l0mcr003hewse2us9aaie"},{"post_id":"cke2l0max0012ewse39pub8qw","tag_id":"cke2l0mcq003fewseb5z10xqj","_id":"cke2l0mcs003iewse14j198xg"},{"post_id":"cke2l0mb10014ewsegmtfaa0r","tag_id":"cke2l0mcr003gewse0ne1f6zf","_id":"cke2l0mcs003kewse9e2v7s02"},{"post_id":"cke2l0mb30016ewse3dm9cqym","tag_id":"cke2l0mcs003jewsecdow3cf8","_id":"cke2l0mcv003newseer9l43dq"},{"post_id":"cke2l0mb30016ewse3dm9cqym","tag_id":"cke2l0mct003lewsefr0h6mqh","_id":"cke2l0mcv003oewse6c3y8yer"},{"post_id":"cke2l0mba001aewse845dajsg","tag_id":"cke2l0mcu003mewse3kkq5m35","_id":"cke2l0mcy003sewse1xam7s54"},{"post_id":"cke2l0mba001aewse845dajsg","tag_id":"cke2l0mcv003pewse1ibj1ejr","_id":"cke2l0mcy003tewse3s8l6npi"},{"post_id":"cke2l0mba001aewse845dajsg","tag_id":"cke2l0mcg0031ewsec76x107r","_id":"cke2l0mcz003vewsec1b6dtjl"},{"post_id":"cke2l0mbj001hewse85r9ho8r","tag_id":"cke2l0mcu003mewse3kkq5m35","_id":"cke2l0md3003yewsefj2nc070"},{"post_id":"cke2l0mbj001hewse85r9ho8r","tag_id":"cke2l0mcv003pewse1ibj1ejr","_id":"cke2l0md3003zewsegjn6beu9"},{"post_id":"cke2l0mbj001hewse85r9ho8r","tag_id":"cke2l0mcz003wewseekhyfpki","_id":"cke2l0md40041ewse3bcn7o78"},{"post_id":"cke2l0mbn001jewse3z3u59uv","tag_id":"cke2l0md1003xewse6nnq36wd","_id":"cke2l0md50042ewse7eok3dcb"},{"post_id":"cke2l0mbn001jewse3z3u59uv","tag_id":"cke2l0md30040ewseek3agn7z","_id":"cke2l0md60043ewse2z833hqr"}],"Tag":[{"name":"hbsse","_id":"cke2l0m970005ewse8ll93va1"},{"name":"大数据","_id":"cke2l0m9y000cewsefzb624ul"},{"name":"高可用","_id":"cke2l0mad000jewse62cv46rx"},{"name":"HA","_id":"cke2l0mak000rewsea91fb4ey"},{"name":"hadoop","_id":"cke2l0maq000xewse1jta4ect"},{"name":"RNN","_id":"cke2l0mb90018ewse4daqf7i9"},{"name":"架构梳理","_id":"cke2l0mbi001fewse4r7c07qc"},{"name":"文本分类","_id":"cke2l0mbq001mewse6njw9ite"},{"name":"深度学习","_id":"cke2l0mbs001pewse6qjgebpg"},{"name":"todo list","_id":"cke2l0mbu001sewseawvtatco"},{"name":"zookeeper","_id":"cke2l0mc00021ewse6o80enlb"},{"name":"hbase","_id":"cke2l0mc20025ewse3z4lbm88"},{"name":"mathjax渲染","_id":"cke2l0mc4002aewse7f8j893e"},{"name":"hexo","_id":"cke2l0mc6002fewse0uh7aoq4"},{"name":"集群","_id":"cke2l0mc9002oewse2m47dgl2"},{"name":"IP","_id":"cke2l0mca002qewse24ks7kvh"},{"name":"静态IP","_id":"cke2l0mcb002tewse3b7bhm5x"},{"name":"Ubuntu","_id":"cke2l0mcc002uewsehic4euff"},{"name":"求甚解","_id":"cke2l0mcd002vewse5q7k2vwy"},{"name":"综述","_id":"cke2l0mcg0031ewsec76x107r"},{"name":"文献翻译","_id":"cke2l0mci0032ewse2tik5bjc"},{"name":"可持续性发展","_id":"cke2l0mcj0033ewse84cdcvcm"},{"name":"影评","_id":"cke2l0mck0036ewsee1h23yty"},{"name":"异构图","_id":"cke2l0mcm0039ewse6aq4fuzb"},{"name":"图网络","_id":"cke2l0mcn003bewsehdqi2b4h"},{"name":"课程","_id":"cke2l0mco003cewse2nerbvle"},{"name":"价值网与云服务平台技术","_id":"cke2l0mcq003fewseb5z10xqj"},{"name":"小可爱","_id":"cke2l0mcr003gewse0ne1f6zf"},{"name":"潜在语义分析","_id":"cke2l0mcs003jewsecdow3cf8"},{"name":"LSA","_id":"cke2l0mct003lewsefr0h6mqh"},{"name":"短文本","_id":"cke2l0mcu003mewse3kkq5m35"},{"name":"分类","_id":"cke2l0mcv003pewse1ibj1ejr"},{"name":"NLP","_id":"cke2l0mcz003wewseekhyfpki"},{"name":"胶囊网络","_id":"cke2l0md1003xewse6nnq36wd"},{"name":"Capsule Networks","_id":"cke2l0md30040ewseek3agn7z"}]}}