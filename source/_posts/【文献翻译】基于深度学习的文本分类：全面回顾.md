---
title: 【文献翻译】基于深度学习的文本分类：全面回顾
top: false
cover: false
toc: true
mathjax: false
date: 2020-04-27 20:42:51
password:
summary:
tags:
categories:
img:
keywords:
---

### 摘要

基于深度学习的模型已经在各种文本分类任务中超过了经典的基于机器学的方法，例如情感分析，新闻分类，问答以及自然语言处理。在这次工作中，我们对近些年开发的150多种基于深度学习的文本分类模型进行了详尽的回顾，讨论了他们的技术贡献，相似点以及优点。我们还对广泛应用于文本分类的40多个流行数据集进行了总结。最后，我们对不同的深度学习模型在**流行基准**上的性能进行了定量分析。

### Introduction

文本分类是自然语言处理中的一个经典问题，旨在为文本单元（例如句子，**询问**，段落和文档）分配标签。文本分类有十分广泛的应用，例如问答，垃圾邮件检测，情感分析，新闻分类，用户意图识别，内容审核等等。文本数据可以来自不同的数据源，例如网页数据，邮件，聊天，社交媒体，机票，保险理赔，用户评论，客户服务中的问题和解答等等。文本中含有极其丰富的信息，但由于它的非结构化特征，想要从中提取信息便极具挑战和耗时。

文本分类可以通过人工标注和自动标注两种方式进行，随着工业应用中文本数据规模不断增大，自动文本分类变得越来越重要。自动文本分类的方法可以被分为3类：

* 基于规则的方法
* 基于机器学习的方法（数据驱动）
* 混合方法

基于规则的方法使用一组预定义的规则将文本分为不同的类别。例如，所有包含“足球”，”篮球“或者”棒球“的文档都被标记为”运动“标签。

### 2 用于文本分类的深度学习模型

在这个部分，我们回顾了针对各种文本分类问题提出的150多种深度学习框架。为了更易于遵循，我们根据模型的主要架构贡献将其分为以下类别：

* 基于前馈网络的模型，该模型将文本视为一堆单词（a bag of words）（第2.1节）
* 基于RNN的模型，该模型将文本视为单词序列，旨在捕获单词相关性和文本结构（第2.2节）
* 基于CNN的模型，经过训练可识别文本中的模式（例如关键短语）以进行分类（第2.3节）
* 胶囊网络(Capsule networks)解决了CNN的池化操作所带来的信息丢失问题，最近已应用于文本分类（第2.4节）
* 注意机制(Attention mechanism)可有效识别文本中的相关单词，并已成为开发深度学习模型的有用工具（第2.5节）
* 记忆增强网络(Memory-augmented networks)，将神经网络与外部记忆形式结合在一起，模型可以从中读取和写入（第2.6节）
* Transformers，允许比RNN更多的并行化，因此可以使用GPU集群有效地（预）训练非常大的语言模型（第2.7节）
* 图神经网络(Graph neural networks)，旨在捕获自然语言的内部图结构，例如句法和语义解析树（第2.8节）
* 孪生神经网络(Siamese Neural Networks)，用于文本匹配，文本匹配是文本分类的一种特殊情况（第2.9节）
* 混合模型(Hybrid models)，结合注意力，RNN，CNN等模型来捕获句子和文档的局部和全局特征（第2.10节）
* 最后，在2.11节中，我们回顾了有监督学习之外的建模技术，包括使用自动编码器(Autoencoder)和对抗训练(Adversarial training)的无监督学习(Unsupervised learning)，以及强化学习(Reinforcement learning)

### 2.1 前馈神经网络

前馈网络是用于文本表示的最简单的深度学习模型之一。但是，它们已经在许多文本分类基准上达到了很高的准确性。 这些模型将文本视为一堆单词。对于每个单词，他们使用诸如word2vec [8]或Glove [9]之类的嵌入模型学习向量表示，将嵌入向量的和或平均值作为文本的表示，将其通过一个或多个前馈层，称为多层感知器（MLP），然后使用诸如逻辑回归、朴素贝叶斯或SVM等分类器对最终层的表示进行分类[10]。一个例子如深度平均网络（Deep Average Network，DAN）[10]，其体系结构如图1所示。尽管简单，但DAN却胜过了其他更复杂的模型，这些模型旨在显式地学习文本的组成。例如，DAN在语法差异较大的数据集上的表现优于语法模型。Joulin等[11]提出了一种简单而有效的文本分类器，称为fastText。 像DAN一样，fastText将文本视为一堆单词，与DAN不同的是，fastText使用n-gram作为附加特征来捕获局部单词顺序信息。 事实证明，这在实践中非常有效，同时可达到与显式使用单词顺序的方法[12]相当的结果。

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200428171503.png)

Le和Mikolov [13]提出了doc2vec，它使用一种无监督算法来学习可变长度文本（例如句子，段落和文档）的定长特征表示。如图2所示，doc2vec的体系结构类似于连续词袋（CBOW）模型的体系结构[8，14]。唯一的区别是附加的段落标记通过矩阵D映射到段落向量。在doc2vec中，此向量与三个单词的上下文的连接或平均值用于预测第四个单词。 段落向量表示当前上下文中丢失的信息，可以用作该段落的主题记忆。经过训练后，段落向量将用作段落的特征并送入分类器进行预测。  Doc2vec在发布时，在一些文本分类和情感分析任务上获得了最优结果。

### 2.2 基于RNN的模型

基于RNN的模型将文本视为一系列单词，旨在捕获单词依赖性和文本结构以进行文本分类。但是，普通的RNN(vanilla RNN)模型不能很好地工作，并且通常表现不如前馈神经网络。 在RNN的许多变体中，长短期记忆网络（LSTM）是最受欢迎的结构，旨在更好地捕获长期依赖关系。LSTM通过引入存储单元以记住任意时间间隔的值以及三个门（输入门，输出门，遗忘门）来调节信息的流动，从而解决了普通RNN遇到的梯度消失或爆炸问题。已经有工作通过捕获更丰富的信息（例如自然语言的树结构，文本中的大跨度单词关系，文档主题等）来改进用于文本分类的RNN和LSTM模型。

Tai等[15]已经开发了Tree-LSTM模型，将LSTM推广到树结构网络类型，以学习丰富的语义表示。作者认为，针对自然语言处理任务，Tree-LSTM比链结构LSTM更好，因为自然语言具有句法属性，可以自然地将单词和短语组合在一起。他们在两个任务上验证了Tree-LSTM的有效性：情感分类和预测两个句子的语义相关性。这些模型的架构如图3所示。 [16]通过使用存储单元在递归过程中存储多个子单元或多个后代单元的历史将chain-structured LSTM展到树状结构。他们认为，新模型提供了一种原则上的方法，可以考虑在层次结构（例如语言或图像解析结构）上进行长距离交互。

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200430103448.png)

为了对机器学习的大跨度单词关系进行建模，Cheng等人[17]用一个存储网络代替单个存储单元来增强LSTM体系结构。这可以在神经注意力复发期间启用自适应内存使用，从而提供一种弱化标记之间关系的方法。 该模型在语言建模，情感分析和NLI上取得了可喜的结果。

多时标LSTM（MT-LSTM）神经网络[18]被设计为通过捕获具有不同时标的有价值的信息来对长文本（例如句子和文档）建模。MT-LSTM将标准LSTM模型的隐藏状态分为几组。 每个组在不同的时间段被激活和更新。 因此，MT-LSTM可以对很长的文档进行建模。MT-LSTM在文本分类方面优于包括基于LSTM和RNN的模型在内的基准。

RNN擅长捕获单词序列的局部结构，但是面对远距离依赖关系会有点力不从心。相反，潜在主题模型（latent topic models）能够捕获文档的全局语义结构，但不考虑单词顺序。Bieng等 [19]提出了TopicRNN模型，以整合RNN和潜在主题模型的优点。 它使用RNN捕获局部（语法）依赖性，并使用潜在主题捕获全局（语义）依赖性。 TopicRNN在情感分析方面优于RNN基线。

还有其他有趣的基于RNN的模型。 刘等[20]使用多任务学习来训练RNN，以利用来自多个相关任务的标记训练数据。Johnson和Rie [21]探索了使用LSTM的文本区域嵌入方法。周等 [22]集成了双向LSTM（Bi-LSTM）模型和二维最大池来捕获文本特征。Wang等[23]提出了在“matching-aggregation”框架下的双边多视角匹配模型。  Wan等[24]使用双向LSMT模型生成的多个位置句子表示来探索语义匹配。

### 2.3 基于CNN的模型

训练RNN识别跨时间的模式，而CNN学会识别跨空间的模式[25]。在需要理解远程语义的POS标签或QA等NLP任务中，RNN效果很好，而在检测局部和位置不变模式很重要的情况下，CNN效果很好。这些模式可能是表达特定情绪（例如“我喜欢”）或主题（例如“濒危物种”）的关键短语。 因此，CNN已成为最受欢迎的文本分类模型体系结构之一。

Kalchbrenner等人提出了最早的基于CNN的文本分类模型之一[26]。 该模型使用动态k-max池，称为动态CNN（DCNN）。如图4所示，DCNN的第一层使用对句子中每个单词的嵌入来构造句子矩阵。 然后使用将宽卷积层与动态k-max池给定的动态池层交替的卷积体系结构来生成句子的特征映射，该特征映射能够显式捕获单词和短语的短时和长时关系。可以根据句子大小和卷积层次结构的级别来动态选择池化参数k。

后来，Kim [27]提出了一种比DCNN简单得多的基于CNN的模型，用于文本分类。 如图5所示，Kim的模型仅在从无监督神经语言模型（即word2vec）获得的单词向量上使用一层卷积。Kim还比较了四种学习单词嵌入的方法：

* CNN-rand，其中所有单词嵌入都在训练过程中被随机初始化，然后进行修改
* CNN-static，在模型训练期间使用预训练的word2vec嵌入并保持固定
* CNN-non-static，其中word2vec嵌入在针对每个任务的训练过程中进行了微调
* CNN-multi-channel，其中使用了两组词嵌入向量集，都使用word2vec进行了初始化，其中一个在模型训练期间进行了更新，而另一个则在固定的情况下进行了更新。

这些基于CNN的模型将改进情感分析和问题分类的SOTA。

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164349.png)

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164600.png)

[26，27]已经做出了一些努力来改进基于CNN模型的体系结构。刘等[28]提出了一种新的基于CNN的模型，该模型对TextCNN [27]的体系结构进行了两次修改。首先，采用动态最大池化方案来从文档的不同区域捕获更多细粒度的特征。其次，在池化层和输出层之间插入一个隐藏的瓶颈层（bottleneck layer）学习紧凑的文档表示形式，以减小模型大小并提高模型性能。在[29，30]中，作者没有使用预先训练的低维词向量作为CNN的输入，而是直接将CNN应用于高维文本数据，以学习小文本区域的嵌入进行分类。

字符级的CNN也已经被用于文本分类[31，32]。Zhang等人提出了最早的此类模型之一[31]。如图6所示，该模型以固定大小的字符作为输入，将其编码为一个one-hot向量，然后将它们通过一个深CNN模型，该模型由具有池化操作的六个卷积层和三个全连接层组成。Prusa等[33]提出了一种使用CNN编码文本的方法，该方法大大减少了学习字符级文本表示所需的内存消耗和训练时间。此方法可根据字母大小很好地缩放，从而可以保留原始文本中的更多信息以增强分类性能

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502164852.png)

有研究调查词嵌入和CNN架构对模型性能的影响。受到VGG [34]和ResNets [35]的启发，Conneau等人 [36]提出了一种非常深的CNN（VDCNN）模型用于文本处理。它直接在字符级别上操作，并且仅使用小的卷积和池化操作。 研究表明，VDCNN的性能随着深度的增加而增加。杜克等[37]修改了VDCNN的结构，以适应移动平台的限制并保持性能。他们能够将模型大小压缩10倍至20倍，而精度损失在0.4％至1.3％之间。Le等[38]表明，当文本输入表示为字符序列时，深层模型确实优于浅层模型。但是，一个简单的浅层和广域网络在词输入方面胜过DenseNet [39]等深层模型。郭等[40]研究了词嵌入的影响，并提出通过多通道CNN模型使用加权词嵌入。张等[41]研究了不同词嵌入方法和池化机制的影响，发现使用非静态word2vec和GloVe优于one-hot向量，并且最大池化始终优于其他池化方法。

还有其他有趣的基于CNN的模型。Mou等[42]提出了一种基于树的CNN来捕获句子级语义。庞等[43]将文本匹配转换为图像识别任务，并使用多层CNN识别显著n-gram模式。Wang等[44]提出了一种基于CNN的模型，该模型结合了短文本的显式和隐式表示形式进行分类。 将CNN应用于生物医学文本分类的兴趣也越来越高[45-48]。

### 2.4 胶囊网络

CNN通过使用连续的卷积和池化层对图像或文本进行分类。 尽管池化操作可识别显著特征并降低卷积操作的计算复杂性，但它们会丢失有关空间关系的信息，并可能根据其方向或比例对实体进行错误分类

为了解决池化带来的问题，Geoffrey Hinton提出了一种新方法，称为胶囊网络（CapsNets）[49，50]。一个胶囊是一组神经元，其活动向量代表特定类型的实体（例如对象或对象部分）的不同属性。向量的长度代表实体存在的概率，向量的方向代表实体的属性。与CNN的最大池化（选择一些信息并丢弃其余信息）不同，胶囊使用网络中直到最后一层的所有可用信息，将底层的每个胶囊“路由”到上层最匹配的父胶囊。可以使用不同的算法来实现路由，例如协议动态路由[50]或EM算法[51]。

近来，胶囊网络已经被应用于文本分类，其中胶囊适于将句子或文档表示为向量。  [52–54]提出了一种基于CapsNets变体的文本分类模型。该模型由四层组成：（1）n-gram卷积层，（2）胶囊层，（3）卷积胶囊层，以及（4）完全连接的胶囊层。 作者尝试了三种策略来稳定动态路由过程，以减轻包含背景信息（例如停用词或与任何文档类别无关的词）的噪声包的干扰。 他们还探索了两种胶囊架构，如图7所示。分别为Capsule-A和Capsule-B。Capsule-A与[50]中的CapsNet类似。  Capsule-B使用三个并行网络，并在n-gram卷积层中使用具有不同窗口大小的过滤器，以学习更全面的文本表示形式。  CapsNet-B在实验中表现更好。

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165324.png)

Kim等人提出的基于CapsNet的模型[55]使用类似的架构。 该模型包括（1）一个输入层，该输入层将文档作为单词嵌入的序列；（2）卷积层，生成特征图并使用门控线性单元保留空间信息；（3）卷积胶囊层，通过聚合卷积层检测到的局部特征形成整体特征；（4）文本胶囊层以预测类标签。作者观察到，相比于图像，对象可以更自由地组合在文本中。 例如，即使某些句子的顺序改变了，文档的语义也可以保持不变，这与人脸上的眼睛和鼻子的位置不同。 因此，他们使用静态路由方案，该方案始终优于动态路由[50]进行文本分类。Aly等[56]提议使用CapsNets进行分层多标签分类（HMC），认为CapsNet编码子代关系的能力使其比传统方法更好地解决了HMC任务，在HMC任务中，文档被分配了一个或多个分类标签，这些标签被组织在一个层次结构。 他们模型的架构类似于[52，53，55]中的架构。任等人 [57]提出了CapsNets的另一种变体，它使用了胶囊之间的成分编码机制和基于k-means聚类的新路由算法。 首先，使用codebooks中的所有 codeword vectors 形成单词嵌入。 然后，通过k均值路由将下层胶囊捕获的特征汇总到高层胶囊中。

### 2.5 基于注意力机制模型

在开发用于NLP的深度学习模型时，注意力已成为越来越流行的概念和有用的工具[58，59]。 简而言之，语言模型中的注意力可被解释为重要权重的向量。 为了预测句子中的单词，我们使用注意力向量来估计它与其他单词的相关性或“与之相关”的程度，然后将注意力向量加权的值之和作为目标的近似值

本节回顾了一些最突出的注意力模型，这些模型在发布时就在文本分类任务上取得了SOTA。

杨等[60]提出了一种用于文本分类的分层注意力网络。 该模型具有两个鲜明的特征：（1）反映文档的层次结构的层次结构，（2）在单词和句子级别上应用的两个级别的注意力机制，使它能够在构建文档表示形式时以不同的方式参加重要或不重要的内容。 在六个文本分类任务上，该模型大大优于以前的方法。周等[61]将分层注意力模型扩展到跨语言情感分类。 在每种语言中，都使用LSTM网络对文档进行建模。 然后，通过使用分层注意机制实现分类，其中句子级别的注意模型了解文档的哪些句子对于确定总体情绪更重要。 而词级注意力模型则学习每个句子中哪些词具有决定性。

沉等[62]提出了一种定向自我注意网络，用于无RNN / CNN语言理解，其中来自输入序列的元素之间的注意力是定向的和多维的。 轻量级神经网络仅基于所提出的注意力而无需任何RNN / CNN结构即可用于学习句子嵌入。刘等[63]提出了一个具有inner-attention的LSTM模型,用来做NLI任务。 该模型使用两阶段过程对句子进行编码。 首先，在词级Bi-LSTM上使用平均池化以生成第一阶段句子表示。 其次，采用注意力机制来代替同一句子的平均池，以获得更好的表示。 句子的第一阶段表示法用于出现在句子中的单词。

注意模型也广泛应用于成对排序（pair-wise ranking）或匹配任务。  Santos等[64]提出了一种双向注意机制，称为Attentive Pooling（AP），用于成对排名。  AP可以使池化层知道当前的输入对（例如，问题-答案对），以使两个输入项的信息可以直接影响彼此表示的计算。 除了学习输入对的表示之外，AP联合学习该对投影段的相似性度量，然后为每个输入导出相应的注意力向量以指导池化。  AP是独立于底层表示学习的通用框架，并且可以应用于CNN和RNN，如图8（a）所示。Wang等[65]将文本分类视为标签-单词匹配问题：每个标签与单词向量一起嵌入相同的空间。作者介绍了一种注意力框架，该框架通过余弦相似性来度量文本序列和标签之间嵌入的兼容性，如图8（b）所示。

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200502165837.png)

Kim等[66]提出了一种使用密集连接的循环共同注意力网络的语义句子匹配方法。 类似于DenseNet [39]，该模型的每一层都使用所有先前的递归层的注意特征以及隐藏特征的级联信息。它可以保留从最底层单词嵌入层到最上层循环层的原始和共同注意特征信息。Yin等[67]提出了另一种基于注意力的CNN模型，用于句子对匹配。 他们提出了三种将句子之间的相互影响整合到CNN中的注意力方案，以便每个句子的表示都考虑到其成对的句子。 这些相互依赖的句子对表示形式比孤立的句子表示形式更为强大，这在包括答案选择，复述识别和文本蕴涵在内的多个分类任务中得到了验证。Tan等[68]在匹配聚合框架下采用了多种注意函数来匹配句子对。 杨等。  [69]介绍了一种基于注意力的神经匹配模型，用于对简短答案文本进行排名。 他们采用价值共享加权方案代替位置共享加权方案来组合不同的匹配信号，并使用问题关注网络将问题术语重要性学习纳入其中。 该模型在TREC QA数据集上取得了可喜的结果。

还有其他有趣的注意力模型。  Lin等[70]使用自注意力来提取可解释的句子嵌入。  Wang等[71]提出了一种具有多尺度特征关注度的紧密连接的CNN，以产生可变的n-gram特征。  Yamada和Shindo [72] 使用neural attentive bag-of-entities 模型（使用知识库中的实体）进行文本分类。  Parikh等。  [73]使用注意力将问题分解为可以单独解决的子问题。  Chen等[74]探索了通用的池化方法来增强句子嵌入，并提出了一个基于向量的多头注意力模型。  Liu和Lane [75]提出了一种基于注意力的RNN模型，用于联合意图检测和空缺填充。

### 2.6 记忆增强网络

注意力模型在编码过程中存储的隐藏向量可以看作是模型内部记忆，而记忆增强网络则将神经网络与外部记忆结合在一起，模型可以对其进行读写。

Munkhdalai和Yu[76]提出了一种记忆增强的神经网络，称为神经语义编码器（NSE），用于文本分类和QA。  NSE配备了一个可变大小的编码记忆，该编码记忆会随着时间的推移而发展，并通过读取，编写和写入操作保持对输入序列的理解，如图9所示。

![](https://cdn.jsdelivr.net/gh/zhishuangR/myImg@master/md/20200503113637.png)

韦斯顿等[77]设计了一个用于综合QA任务的记忆网络，在该网络中，向模型提供了一系列语句（记忆记录），作为问题的支持事实。 该模型会根据问题和先前检索到的记忆来一次从记忆中检索一个条目。Sukhbaatar等[78]扩展了这项工作，并提出了端到端的记忆网络，在记忆网络中以柔和的方式利用注意力机制检索记忆条目，从而实现了端到端的训练。 他们表明，通过多次回合（跳数），该模型能够检索并推理几个支持事实，以回答特定问题。

Kumar等[79]提出了一种动态记忆方法（DMN），它处理输入序列和问题，形成情节记忆，并产生相关的答案。 问题触发迭代注意进程，该进程允许模型将注意条件设置为输入和先前迭代的结果。 然后，在分层递归序列模型中对这些结果进行推理以生成答案。 对DMN进行端到端训练，并获得有关QA和POS标记的最新结果。 熊等[80]提出了DMN的详细分析，并改进了其就医和输入模块。