---
title: 短文本分类
top: false
cover: false
toc: true
mathjax: false
reprintPolicy: cc_by
date: 2020-06-18 15:59:07
author:
password:
img:
summary: 短文本分类综述
categories: 短文本
keywords: 短文本 分类 综述
tags:
	- 短文本
	- 分类
	- 综述
---

### 大纲目录

* 短文本分类的难点和挑战
  * 和长文本相比，有什么不同，特殊在哪里，长文本的分类方法为什么不能适用于短文本分类
* 短文本分类包含哪些应用领域
  * 评论数据等的情感分析
  * 聊天记录分析
  * 问答系统
  * 搜索
  * 其他
* 短文本分类方法划分
  * 按照流程划分
    * 短文本分类流程是怎样的，与长文本分类流程有什么不同
    * 特征抽取文本表示分类
  * 标准划分
    * 基于机器学习
    * 基于深度学习
    * 融合
  * 是否引入外部知识源
    * 无外部知识源
    * 有外部知识源
* 相关数据集整理，给出评价指标
* 各个数据集上实验结果对比分析
* 总结，分析未来发展趋势

### 引言

短文本分类是自然语言处理领域的重要任务，包括情感分析、问答、对话管理等。随着Internet的大规模普及和用户数量的进一步增加,互联网上的各种短文本正在爆炸式地增长。短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.

本文根据所需知识源的属性,将短文本理解模型分为3类:

* 隐性(implicit)语义模型
* 半显性(semi-explicit)语义模型
* 显性(explicit)语义模型

其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本分类.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本分类.

从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度

### 文本分类方法

#### 隐性(implicit)语义模型

隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.

- LSA(latent semantic analysis)^[3]^

LSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.

* 超空间模拟语言模型^[4]^

超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止
词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.

* 神经网络语言模型

近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不
同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).

最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使
用目标词预测窗口中的每个语境词出现的概率.

总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.

* 段向量

段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.

![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%25E5%259B%25BE%25E7%2589%258720200630104534.png)

#### 半显性(semi-xpIicit)语义模型

半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic
models).

LSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏
参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:

$p(d, w)=p(d) \sum_{x \in Z} p(w \mid z) p(z \mid d)$

虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).

![QQ截图20200701115340](/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200701115340.png)

总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.

#### 显性(explicit)语义模型

近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常
是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.

1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每
个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.

![QQ截图20200702142120](/home/zhishuang/Pictures/QQ%E6%88%AA%E5%9B%BE20200702142120.png)

2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad"这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad").为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而
计算$p(concept |word ,topic)$.

### 模型粒度分析

本节将深入讨论第1节的短文本理解模型在文本分析粒度上的差异,并从应用层面论证不同方法的适用性.

#### 文本粒度模型

首先,文本粒度的模型包含LSA,LDA和PV.这些模型均尝试直接推导出短文本的向量表示作为模型的输出.在LSA中,通过构建一个词与文本的共现矩阵,每个文本可用以词为维度的向量表示.类似地,LDA试图模拟文本的生成过程.作为结果,可得到每个文本的主题分布.PV通过神经网络推测(inference)的方式获取文本向量的最优参数.上述模型所得的文本向量均可以直接用于与这些文本相关的任务,如文本分类^[17-18]^、聚类^[19]^、摘要生成^[20]^。值得注意的是,LSA同时输出词向量.因而在短文本数量不足的情况下,可以先采用基于大量完整文本的LSA获取词向量,再通过额外的合成方法获取短文本向量.对于LDA和PV而言,其模型亦可以通过额外的文本训练,然后应用于短文本.

#### 2.2词粒度模型

同LSA,LDA和PV相比,其他模型(LSA,NLM,ESA等)均属于词粒度的模型.这是由于这些模型的产出仅为词向量.针对短文本理解这一任务,必须使用额外的合成手段来推出短文本的表示.例如,在文献[21-25]工作中,作者均利用词向量推导出文本表示,并用于后续的文本相似度判断、文本复述、情感分析等任务.这里的一个特例为概念化模型.由于概念化可以直接基于语境推出短文本中每个词的概念,这样的输出方式已经可以满足机器短文本理解的需求.因而概念化虽属于词粒度的模型但并不需要额外的文本合成.

### 2.3文本合成

如何通过词向量获取任意长度的文本向量(包括短文本)是时下流行的一个研究领域.根据复杂度的不同,文本合成方法可被大致分为代数向量模型^[5,21-23,25]^、张量模型^[26-28]^和神经网络模型^[7,24,29-32]^.

1)代数运算模型.最早的合成模型由Mitchell和Lapata^[21]^提出.其模型使用逐点的(point-wise)向量相加的方式从词向量推出文本向量.虽然这一基于“词袋”的方法忽略了句子中的词序(“cat eats fish”和“fish eats cat”将有相同的表示),事实表明其在很多自然语言处理任务上有着不错的效果,且其常常被用作复杂模型的基准^[23]^,类似的代数运算
模型还有逐点的向量乘积^[5,21-22]^以及乘法与加法的结合运算^[24]^.

2)张量模型.张量模型^[26-27]^为代数运算模型的延伸.其试图强调不同词性的词在语义合成中的不同角色.例如在“red car”这个词组中,形容词“red”对名词“car”起修饰作用;而在“eat apple”中,动词“eat”的角色好比作用于“apple”的函数.从这个角度而言,将不同词性的词均表示为同等维度的向量过于简化.因而,在张量模型中,不同词性的词被表示为
不同维度的张量,整个句子的表示方式以张量乘法的形式获取.目前,张量模型的最大挑战是如何获取向量与张量的映射关系^[28]^.

3)神经网络模型.时下最为流行的文本合成模型为基于神经网络的模型,如recursive neural network(RecNN)^[29-30]^,recurrent neural network(RNN)^[5]^,convolutional neural
 network(CNN)^[31-32]^等.在这些模型中,最基本的合成单元为神经网络.通常的形式为神经网络根据输入向量工**x~1~**，**x~2~**:推出其组合向量**y**。

在具体的文本合成中,不同的神经网络模型的构造不同.例如,RecNN依赖于语法树开展逐层的语义合成,它无法被用于短文本.相比之下,RNN和CNN都可以快速通过词向量推导出短文本向量.



### 文献引用

[1] Linmei H, Yang T, Shi C, et al. Heterogeneous graph attention networks for semi-supervised short text classification[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 4823-4832.

[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.

[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.

[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.

[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.

[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.

[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.

[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.

[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.

[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.

[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e

[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.

[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.