---
ssstitle: 短文本分类架构梳理
top: false
cover: false
toc: true
mathjax: false
date: 2020-06-06 16:55:38
password:
summary:
tags:
	- 短文本
	- 分类
	- NLP
categories: 短文本理解
img:
keywords: 短文本 分类 NLP
---

### 什么是短文本

短文本是相对于长文本而言的，通常指书评、影评、聊天记录、产品介绍等短文本，这些文本中包含了大量有价值的隐含信息，但现有的研究大多集中在长文本上，且长文本的分类算法无法直接应用于短文本分类^[1]^,这是因为短文本通常词汇个数少且描述信息弱，具有稀疏性和歧义且通常标签数据较少。

### 传统短文本分类算法

基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模

传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响

### 基于深度学习的短文本分类算法

短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息

与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。

一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。

文本分类模型如下：

​	1） FastText

FastText是Facebook开源的词向量与文本分类工具，模型简单，训练速度快。FastText 的原理是将短文本中的所有词向量进行平均，然后直接接softmax层，同时加入一些n-gram 特征的 trick 来捕获局部序列信息。相对于其它文本分类模型，如SVM，Logistic Regression和Neural Network等模型，FastText在保持分类效果的同时，大大缩短了训练时间，同时支持多语言表达，但其模型是基于词袋针对英文的文本分类方法，组成英文句子的单词是有间隔的，而应用于中文文本，需分词去标点转化为模型需要的数据格式。

​	2）TextCNN

TextCNN相比于FastText，利用CNN (Convolutional Neural Network）来提取句子中类似 n-gram 的关键信息，且结构简单，效果好。

​	3）TextRNN

尽管TextCNN能够在很多任务里面能有不错的表现，但CNN最大的问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 "n-gram" 信息。

4）TextRNN + Attention

CNN和RNN用在文本分类任务中尽管效果显著，但都有一个缺点，直观性和可解释性差。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够直观的给出每个词对结果的贡献，是Seq2Seq模型的标配。实际上文本分类从某种意义上也、可以理解为一种特殊的Seq2Seq，所以可以考虑将Attention机制引入。

Attention的核心点是在翻译每个目标词（或预测商品标题文本所属类别）所用的上下文是不同的，这样更合理。加入Attention之后能够直观的解释各个句子和词对分类类别的重要性。

5）TextRCNN（TextRNN + CNN）

用前向和后向RNN得到每个词的前向和后向上下文的表示，这样词的表示就变成词向量和前向后向上下文向量concat起来的形式，最后连接TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1。



### 短文本分类应用场景

短文本分类算法广泛应用于各个行业领域,如新闻分类、人机写作判断、垃圾邮件识别、用户情感分类、文案智能生成、商品智能推荐等。

场景一：商品智能推荐，根据用户购买的商品名称作为预测样本进行文本分类，得到用户交易类别，结合其他数据构建用户画像，针对不同特征的用户画像预测用户下一步的购买行为，智能推荐商品及服务。

场景二：文案智能生成，基于优质文案作为训练集，得到文本分类模型，当用户输入关键词时，智能推荐适配文案。

场景三：给新闻自动分类或打标签，多个标签。

场景四：判断文章是人写还是机器写的。

场景五：判断影评中的情感是正向、负向、中立，相类似应用场景很







### 文献引用

[1] Linmei, H., Yang, T., Shi, C., Ji, H., & Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* (pp. 4823-4832).

[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.

[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.

[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.

[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.

[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.

[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.

[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.





## 论文阅读

### 题目：Short Text Classification: A Survey

分类方式：

* short text classification using sematic analysis（语义分析）
* semi-supervised short text classification（半监督）
* ensemble short text classification（集成学习）
* real-time classification.

### I. INTRODUCTION

随着电子商务和在线通信的爆炸式增长，在许多应用领域中都可以使用短文本，例如即时消息，在线聊天日志，公告板系统标题，Web日志评论，Internet新闻评论，SMS，Twitter等。因此，成功在许多Web和IR应用程序中处理它们变得越来越重要。 但是，对这些类型的文本和Web数据进行分类是一个新的挑战。

与普通文档不同，这些文本和Web段通常更嘈杂，主题更少，并且更短，也就是说，它们由十几个单词到几个句子组成[1]。 由于篇幅短，它们无法提供足够的单词共现或共享语境，无法实现良好的相似度[40]。 因此，依赖于词频，足够的词共现或共享上下文来测量文档相似度的常规机器学习方法通常由于数据稀疏而无法达到期望的准确性。

出现了针对短文本的新分类方法，例如语义分析，半监督短文本分类，集成短文本分类、实时分类。然而，与许多有关文本分类的评论和调查相比，只有很少的调查来讨论有关短文本分类的最新研究。 本文分析了与短文本分类相关的挑战，并系统地总结了使用分析方法对短文本分类进行分类的现有方法。

在分析了短文本的特点和难点之后，我们在第二节中指出了短文本分类的过程。 第三节介绍了基于语义分析的短文本分类。 我们在第四节中描述了一些关于半监督短文本分类的算法。 第五节和第六节分别介绍了用于对短文本进行分类和在线短文本分类的集成模型。 我们在第七节中分析了相关的评估措施。 在第八节中，我们总结了对短文本进行分类的方法。

### II. BACKGROUNDS

#### A. Feature of Short Text

通常，短文本的特征如下[1] [2]：

稀疏性：简短的文本仅包含数个到十几个具有某些功能的单词，它不能提供足够的单词共现或共享上下文，无法提供良好的相似性度量。 很难提取其有效的语言功能。

即时性：短文本会立即发送并实时接收。 另外，数量非常大。

非标准性：简短的文字说明简洁明了，有许多拼写错误，非标准术语和噪音。

噪音和分布不平衡：应用程序背景（例如网络安全性）需要处理大量的短文本数据。 但是，我们可能只关注大规模数据中的一小部分（检测对象）。 因此，有用的实例是有限的，并且短文本的分布是不平衡的。

大型数据和标记瓶颈：手动标记所有大型实例很困难。 标签数量有限的实例只能提供有限的信息。 因此，如何充分利用这些标记的实例和其他未标记的实例已经成为短文本分类的关键问题。

大多数传统方法（例如SVM，BAYES和KNN）都是基于术语频率的相似性，而忽略了短文本的特征。 这些传统方法可能无法处理短文本分类。 如果标记的信息不足，它们中的大多数（例如BAYES）可能无法获得高精度。 另外，一些基于向量空间模型（SVM）的分类方法应该使用语义信息来提高分类器的性能。

#### B. Short Text Classification

在过去的十年中，已经深入研究了对文本和Web文档进行分类的学习。 许多学习方法，例如k个最近邻（k-NN），朴素贝叶斯（Naive Bayes），最大熵和支持向量机（SVM），已应用于具有不同基准集合的许多分类问题，并取得了令人满意的结果。 但是，由于短文字的特点和难度，传统的分类方法不适合短文本分类。 因此，如何合理地表示和选择特征项，有效地减小空间尺寸和噪声，提高分类精度成为短文本分类的问题。

#### III. SHORT TEXT CLASSIFICATION BASED ON SEMANTIC ANALYSIS

目前，减少特征空间维数的解决方案主要基于语义特征和语义分析。 这是因为文本分类的处理通常是在向量空间模型（VSM）中进行的，其基本假设是单词之间的关系是独立的，而忽略了文本之间的相关性。但是，短文本的语义表达能力较弱，需要这种相关性。 传统分类不能区分自然语言的歧义词和同义词，所有这些在短文本中都很丰富。 因此，传统的分类方法通常无法达到预期的短文本准确性。

语义分析更加关注概念，内部结构的语义层次以及文本的相关性，从而获得了更具表现力和客观性的逻辑结构。 在现有研究中，基于潜在语义分析的分类占有重要地位。 使用统计方法，潜在语义分析可提取潜在的语义结构，消除同义词影响，并减少特征维数和噪声。因此，提出了许多基于语义分析的算法来处理短文本分类（更多详细信息见表I）[3] [5-11] [44]。  Zelikovitz [3]将其应用于短文本分类。  Pu强等[5]将LSA和独立成分分析（ICA）[8] [42]结合在一起。  Xuan-Hieu Phan等[7]建立了大规模的短文本分类框架。 该框架主要基于最近成功的潜在主题分析模型（例如pLSA和LDA）和强大的机器学习方法（例如最大熵和SVM）。王炳坤等[9]提出了一种新的方法，通过基于潜在狄利克雷分配（LDA）和信息增益（IG）模型构建强大的词库（SFT）来解决该问题。 当使用句法或语义信息时，提出了语言独立语义（LIS）内核[10]以增强语言依赖性。 它能够有效地计算短文本文档之间的相似度，而无需使用语法标记和词汇数据库。 陈梦根等[11]提出了一种以多种粒度提取主题的方法，该方法可以更精确地建模短文本。 转换式LSA [3] [4]是基于LSA的短文本分类的另一个示例。 转导利用测试示例来选择学习者的假设。 通过合并测试示例来重新创建空间确实会根据测试示例选择一种表示形式。 训练/测试集的维数减少使得较小的空间可以更准确地反映将应用于分类的测试集。 通过将测试示例包含到原始矩阵中，LSA可以计算单词与单词的熵权以及测试集中单词的示例和共现。

#### IV. SEMI-SUPERVISED SHORT TEXT CLASSIFICATION

半监督学习是指使用标记和未标记的数据进行训练。 它对比了监督学习（所有数据都标记）或无监督学习（所有数据都未标记）。用于学习问题的标记数据的获取通常需要熟练的人工代理手动对训练示例进行分类。 因此，与标记过程相关的成本可能使完全标记的训练集变得不可行，而未标记数据的获取相对便宜。 在这种情况下，半监督学习可能具有很大的实用价值[27]。

大多数半监督的短文本分类都是灵活的半监督学习。 它可以利用未标记的数据来改进分类器。 但是，与传统的半监督学习算法不同，通用数据和训练/测试数据不需要具有相同的格式。 此外，一旦进行了估计，只要主题模型是一致的，就可以将其应用于多个分类问题。

背景知识可能会提供一个文本语料库，其中包含有关单词重要性和单词联合概率的信息[29] [6]。 我们可以结合使用背景和培训示例来标记新示例。

V. ENSEMBLE SHORT TEXT CLASSIFICATION

VI. REAL-TIME CLASSIFICATION OF LARGE SCALE SHORT TEXT

即时性是短文本的另一个功能，这意味着短文本会立即发送并实时接收，通常数量非常大。 因此，如何立即对大规模的短文本数据进行分类也成为一个重要的问题。 目前，与几种经典分类算法相比，经常选择贝叶斯算法作为在线分类器。 朴素贝叶斯算法通过计算文本属于每个类别的概率来判断类别，这是一种简单，准确且广泛使用的算法[39]。

### 题目：短文本理解研究

与长文本不同,短文本通常不遵循语法规则,并且长度短、没有足够的信息量来进行统计推断,机器很难在有限的语境中进行准确的推断.此外,由于短文本常常不遵循语法,自然语言处理技术(如词性标注和句法解析等)难以直接应用于短文本分析^[2]^.

许多相关工作^[1-3]^证明,自动化的短文本分类需要依赖额外的知识.这些知识可以帮助机器充分挖掘短文本中词与词之间的联系,如语义相关性.例如,在英文查询“premiere Lincoln”中,“premiere”是一个重要的信息,表明“Lincoln”在这里指的是“电影”;同样,在“watch harry potter”中,正因为“watch”的出现,“harry potter”的含义可被鉴定为“电影”或“DVD”,而不是“书籍”.但是,这些关于词汇的知识(例如“watch”的对象通常是“电影”)并没有在短文本中明确表示出来,因而需要通过额外的知识源获取.

根据所需知识源的属性,将短文本理解模型分为3类:

* 隐性(implicit)语义模型
* 半显性(semi-explicit)语义模型
* 显性(explicit)语义模型

其中,隐形和半显性模型试图从大量文本数据中挖掘出词与词之间的联系,从而应用于短文本理解.相比之下,显性模型使用人工构建的大规模知识库和词典辅助短文本理解.

从另一个角度而言,短文本理解模型在文本分析上的粒度也有差异.部分方法直接模拟短文本的表示方式,因此本文将其归为“文本”粒度.其余大多方法则以词为基础.这些方法首先推出每个词的表示,然后使用额外的合成方式推出短文本的表示.本文将这些方法归为“词”粒度

#### 隐性(implicit)语义模型

隐性语义模型产生的短文本通常表示为映射在一个语义空间上的隐性向量.这个向量的每个维度所代表的含义人们无法解释,只能用于机器计算.以下将介绍4种代表性的隐性语义模型.

- LSA(latent semantic analysis)^[3]^

LSA旨在用统计方法分析大量文本,从而推出词与文本的含义表示.其思想核心是在相同语境下出现的词具有较高的语义相关性.具体而言,LSA构建一个庞大的词与文本的共现矩阵.对于每个词向量,它的每个维度都代表一个文本;对于每个文本向量,其每个维度代表一个词.通常,矩阵每项的输入是经过平滑或转化的共现次数.常用的转化方法为TF-IDF.最终,LSA通过奇异值分解(SVD)的方法将原始矩阵降维.在短文本的情境下,LSA有2种使用方式:首先,在语料足够多的离线任务上，LSA可以直接构建一个词与短文本的共现矩阵,从而推出每个短文本的表示;其次,在训练数据较小的情境下,或针对线上任务(针对测试数据),可以事先通过标准的LSA方法得到每个词向量,然后使用额外的语义合成方式获取短文本向量.

* 超空间模拟语言模型^[4]^

超空间模拟语言模型(hyperspace analogue to language model, HAL)与LSA的主要区别在于前者是更加纯粹的“词模型”.HAL旨在构建一个词与词的共现矩阵.对于每个词向量,它的每个维度代表一个“语境词”. 模型统计目标词汇与语境词汇的共现次数,并经过相应的平滑或转换(如TF-IDF, pointwise mutual information等)得到矩阵中每个输入的值.通常,语境词的选取有较大的灵活性.例如,语境词可被选为整个词汇,或者除停止
词外的高频词[5].类比LSA,在HAL中可以根据原始向量的维度和任务要求选择是否对原始向量进行降维.由于HAL的产出仅仅为词向量,在短文本理解这一任务中需采用额外的合成方式(如向量相加)来推出短文本向量.

* 神经网络语言模型

近年来，随着神经网络和特征学习的发展,传统的HAL逐渐被神经网络语言模型(neural language model, NLM)^[6-8]^取代.与HAL通过明确共现统计构建词向量的思想不
同,NLM旨在将词向量当成待学习的模型参数,并通过神经网络在大规模非结构化文本中训练来更新这些参数以得到最优的词语义编码(常被称作word embedding).

最早的概率性NLM由Bengio等人提出^[6]^,其模型使用前向神经网络(feedforward neural network)根据语境预测下一个词出现的概率.通过对训练文本中每个词的极大似然估计,模型参数(包括词向量和神经网络参数)可使用误差反向传播算法(BP)进行更新.此模型的一个缺点在于仅仅使用了有限的语境.后来,Mikolov等人^[7]^提出使用递归神经网络(recurrent neural network) 来代替前向神经网络，从而模拟较长的语境.此外,原始NLM的计算复杂度很高,这主要是由于网络中大量参数和非线性转换所致.针对这一问题,Mikolov等人^[8]^提出2种简化(去掉神经网络权重和非线性转换)的NLM,即continuous bag of words(CBOW)和Skip-gram.前者通过窗口语境预测目标词出现的概率,而后者使
用目标词预测窗口中的每个语境词出现的概率.

总而言之,NLM同HAL相似，所得到的词向量并不能直接用于短文本理解,而需要额外的合成模型依据词向量得到短文本向量.

* 段向量

段向量(paragraph vector, PV)^[10]^是另一种基于神经网络的隐性短文本理解模型. PV可被视作文献^[8]^中CBOW和Skip-gram的延伸，可直接应用于短文本向量的学习.PV的核心思想是将短文本向量当作“语境”,用于辅助推理(例如根据当前词预测语境词).在极大似然的估计过程中，文本向量亦被作为模型参数更新. PV的产出是词向量和文本向量.对于(线上任务中的)测试短文本，PV需要使用额外的推理获取其向量.图3比较了CBOW、Skip-gram和2种PV的异同.

![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png)

#### 半显性(semi-xpIicit)语义模型

半显性语义模型产生的短文本表示方法,也是一种映射在语义空间里的向量.与隐性语义模型不同的是,半显性语义模型的向量的每一个维度是一个“主题(topic)”.这个主题通常是一组词的聚类.人们可以通过这个主题猜测这个维度所代表的含义;但是这个维度的语义仍然不是明确的、可解释的.半显性语义模型的代表性工作是主题模型(topic
models).

LSA尝试通过线性代数(奇异值分解)的处理方式发现文本中的隐藏语义结构,从而得到词和文本的特征表示;而主题模型则尝试从概率生成模型(generative model)的角度分析文本语义结构,模拟“主题”这一隐藏参数,从而解释词与文本的共现关系.最早的主题模型PLSA( probabilistic LSA)为LSA的延伸，由Hofmann提出^[11]^PLSA假设文本具有主题分布,而文本中的词从主题对应的词分布中抽取.以d表示文本,w表示词,z表示主题(隐藏
参数),文本和词的联系概率$p(d,w)$的生成过程可被表示为:

$p(d, w)=p(d) \sum_{x \in Z} p(w \mid z) p(z \mid d)$

虽然PLSA可以模拟每个文本的主题分布,然而其没有假设主题的先验分布(每个训练文本的主题分布相对独立),它的参数随训练文本的个数呈线性增长，且无法应用于测试文本.一个更加完善的	主题模型为LDA ( latent dirichlet allocation)^[12]^. LDA从贝叶斯的角度为2个多项式分布添加了狄利克雷先验分布,从而解决了PLSA 中存在的问题.在LDA中,每个文本的主题分布为多项式分布$Mult (θ)$,其中$θ$从狄利克雷先验$Dir(α)$抽取.同理,对于主题的词分布$Mult(φ)$,其参数$φ$从狄利克雷先验$Dir(β)$获取.图4对比了PLSA和LDA的盘子表示法(plate notation).

![QQ图片20200630104534](/home/zhishuang/Pictures/QQ%E5%9B%BE%E7%89%8720200630104534.png)

总之,通过采用主题模型对短文本进行训练,最终可以获取每个短文本的主题分布，以作为其表示方式.这种表示方法将短文本转为了机器可以用于计算的向量.

#### 显性(explicit)语义模型

近年来，随着大规模知识库系统的出现(如Wikipedia ,Freebase,Probase等),越来越多的研究关注于如何将短文本转化成人和机器都可以理解的表示方法.这类模型称之为显性语义模型.与前2类模型相比,显性语义模型最大的特点就是它所产生的短文本向量表示不仅是可用于机器计算的,也是人类可以理解的,每一个维度都有明确的含义,通常
是一个明确的“概念(concept)”.这意味着机器将短文本转为显性语义向量后,人们很容易就可以判断这个向量的质量,发现其中的问题，从而方便进一步的模型调整与优化.

1)显性语义分析模型.在基于隐性语义的模型中,向量的每个维度并没有明确的含义标注.与之相对的是显性语义模型,向量空间的构建由知识库辅助完成.显性语义分析模型(explicit semantic analysis,ESA)^[13]^同LSA的构建思路一致, 旨在构建一个庞大的词与文本的共现矩阵.在这个矩阵中，每个输入为词与文本的TF-IDF.然而,在ESA中词向量的每
个维度代表一个明确的知识库文本,例如Wikipedia文章(或标题).此外,原始的ESA模型没有对共现矩阵进行降维处理，因而产生的词向量具有较高维度.在短文本理解这一任务中,需使用额外的语义合成方法推导出短文本向量.图5比较了LSA,HAL,ESA和LDA在本质上的区别与联系.

2)概念化,另一类基于显性语义的短文本理解方法为概念(conceptualization).概念化旨在借助知识库推出短文本中每个词的概念分布，即将词按语境映射到一个以概念为维度的向量上。在这一任务中,每个词的候选概念可从知识库中明确获取.例如，通过知识库Probase^[16]^，机器可获悉apple这个词有“水果”和“公司”这2个概念当apple出现在“apple ipad"这个短文本中,通过概念化可分析得出apple有较高的概率属于“公司”这个概念.最早的概念化方法由Song等人提出^[9]^。其模型使用知识库Probase,获取短文本中每个词与概念间的条件概率$p(concept | word)$和$p(word | concept)$,从而通过朴素贝叶斯的方法推出每个短文本的概念分布.这一单纯基于概率的模型无法处理由语义相关但概念不同的词组成的短文本(如“apple ipad").为解决无法识别语境的问题,Kim等人^[14]^对Song的模型做出了改进.新的模型使用LDA主题模型,分析整条短文本的主题分布,进而
计算$p(concept |word ,topic)$.

### 文献引用

[1] Linmei, H., Yang, T., Shi, C., Ji, H., & Li, X. (2019, November). Heterogeneous graph attention networks for semi-supervised short text classification. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* (pp. 4823-4832).

[2]王仲远, 程健鹏, 王海勋, 等. 短文本理解研究[J]. 计算机研究与发展, 2016, 53(2): 262-269.

[3]Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the American society for information science, 1990, 41(6): 391-407.

[4]Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence[J]. Behavior research methods, instruments, & computers, 1996, 28(2): 203-208.

[5]Turney P D, Pantel P. From frequency to meaning: Vector space models of semantics[J]. Journal of artificial intelligence research, 2010, 37: 141-188.

[6]Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.

[7]Mikolov T , Martin Karafiát, Burget L , et al. Recurrent neural network based language model[C]// INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010. DBLP, 2010.

[8]Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.

[11]Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 1999: 50-57.

[12]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.e

[13]Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611.

[9]Song Y, Wang H, Wang Z, et al. Short text conceptualization using a probabilistic knowledgebase[C]//Twenty-Second International Joint Conference on Artificial Intelligence. 2011.

[14]Kim D, Wang H, Oh A. Context-dependent conceptualization[C]//Twenty-Third International Joint Conference on Artificial Intelligence. 2013.

### 《基于深度学习的短文本分类研究综述-刘琴 袁家政 翁长虹》

#### 基于机器学习

* 支持向量机(Support Vector Machine, SVM)
* 朴素贝叶斯分类法
*  K-最近邻法
*  决策树法(Decision Tree, DT)
*  中心向量法 

#### 基于深度学习

* CNN+语义约束（Short text classification improved by learning multi-granularity topics-AAAI2011）

* CNN+语义聚类（Semantic clustering and convolutional neural network for short text categorization-ACL2015）
* CNN+RNN+顺序短文本分类（Sequential short-text classification with recurrent and convolutional neural networks-arxiv2016）
* DBN



### 标准划分

#### 传统短文本分类算法

基于传统机器学习的短文本分类方法主要是对文本分类进行预处理、特征提取、然后将处理后的文本向量化，最后通过常见的机器学习分类算法来对训练数据集建模

传统的短文本分类算法中，文本特征提取出的特征质量对文本分类精度有很大的影响

伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典方法，整个文本分类问题就拆分成了特征工程和分类器两部分。

* 特征工程

特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据-->信息”的过程，决定了结果的上限，而分类器是“信息-->知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。

文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统文本分类任务大部分工作也在此处。文本特征工程分为文本预处理、特征提取、文本表示三个部分，目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。

（1）文本预处理

文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。

具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的冠词助词代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。

（2）文本表示和特征提取

文本表示：

文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。

特征提取：

向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。

特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。

信息增益特征提取方法：该方法基于信息熵的原理，通过计算各个特征在不同类别样本中的分布情况，通过统计比较发现具有较高类别区分度的特征。信息增益能较好的描述不同特征对不同类别的区别能力，信息增益值越高则区别能力越强，在特征选择时保留较大IG 值，在分类时就更容易得到正确的分类结果，而且能降低特征的维度。

卡方检验特征选择方法：该方法主要基于相关性检验原理，通过计算某属性与某一类的相关性，来衡量特征候选项项的重要性，该方法考虑到了反相关的属性。在这里反相关的特征候选项同样存在有用信息，反相关的信息量对分类和聚类算法都有积极作用。

特征权重主要是经典的TF-IDF及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。

（3）特征扩展

由于短文本的特征少，固需要对文本向量进行特征扩展。当前对于短文本信息分类的研究主要聚焦于短文特征的扩展问题之上。特征扩展方法有基于WordNet、Wik ipedia或其他知识库等的特征扩展方法，然而这些方法来说一些的不足之处，对于短文本数据，尤其是即时数据来说，这些方法并不十分合适，因为这些知识库依靠的更多是人工参与编辑，信息相对滞后，扩展特征不够全面，缺乏全局性与一致性。另外一个更重要的原因是网络信息检索本身就会出现噪声问题。这些都会将错误因素累积，所以必须对检索结果进一步筛选。

为了解决这些问题，一些研究人员通过重新计算短文本与检索结果的相似度，验证并实现了多种方法，包括语义分析等。短文本计算相似度时有一定得困难，短文本包含的特征太少，若直接使用相同特征作为度量标准的话，很多短文本之间的相似度为零。为了解决这个问题，需要采用一种适合短文本相似度计算的方法，常见的相似度计算方法有：1．基于本体词典的规则化方法；2．基于知识库的规则化方法；3．基于搜索引擎的方法；4．基于统计的方法；5．句子层面语义相似度计算方法。

（4）基于语义的文本表示

传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础。

* 分类器

分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等。

### 深度学习文本分类方法

传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。

短文本分类是理解短文本的重要方法之一，适用于广泛的应用，包括情感分析（Wang et al.2014），对话系统（Lee和Dernoncourt 2016）以及用户理解（Hu et al.2009）。与段落或文档相比，短文本更加模糊，因为它们没有足够的上下文信息

与传统基于机器学习的短文本分类算法不同，基于深度学习的短文本分类算法则是通过例如CNN等深度学习模型来对数据进行训练，无需人工对数据进行特征提取，对文本分类精度影响更多的是数据量以及训练迭代次数。

一般流程为先使用深度学习的词向量技术，把文本数据转换为连续稠密向量，用来表示文本。再将得到的向量表示作为深度学习模型的特征输入，利用CNN/RNN等深度学习网络及其变体自动提取特征。